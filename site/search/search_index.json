{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG Networking Area \u00b6 Welcome to OSG Networking ! This is an entry point for those interested in Networking in OSG/WLCG or for those OSG/WLCG users experiencing network problems. It provides an overview of the networking goals, plans and various activities and subtopics underway regarding networking in the Open Science Grid (OSG) and World-wide LHC Computing Grid (WLCG) , operated as a joint project. This area started in June 2012 with initial focus on the network monitoring as monitoring is critical to provide needed visibility into existing networks and site connectivity. OSG is working to provide needed networking information and tools for users, sites and experiments/VOs. This documentation is divided into several sub-sections, each covering a specific area of activities. Network Monitoring in WLCG and OSG (perfSONAR) \u00b6 WLCG and OSG jointly operate a network of perfSONAR agents deployed world-wide, which provides an open platform that can be used to baseline network performance and debug any potential issues. The following subsections provide details on the motivation, deployment and operations of the perfSONARs in WLCG/OSG: Motivation - overview, core concepts, motivation Deployment Guide - deployment models and options, hardware requirements Installation and Administration Guide - installation, configuration and maintanance Frequently Asked Questions Network Troubleshooting \u00b6 Users with network issues should check the troubleshooting link below for initial guidance on how best to get their issue resolved. In addition, you can refer to the ESNet network performance guide for a detailed instructions on how to identify and isolate network performance issues using perfSONAR. Network Services \u00b6 OSG operates an advanced platform to collect, store, publish and analyse the network monitoring data it gathers from perfSONAR and other locations. All measurements are collected and available via streaming or through APIs. The following services are available: perfSONAR infrastructure monitoring - monitors state of perfSONAR network and reports on availability of core services OSG Distributed Network Datastore - distributed datastore based on ElasticSearch holding all the network measurements and providing an API to expose them via JSON is available at two locations (University of Chicago and University of Nebraska). OSG pSConfig Web Admin (PWA) - centralized configuration of the tests performed by the OSG/WLCG perfSONAR infrastructure . In case you'd like to start/manage particular mesh, please contact our support channels to get access. OSG Dashboards http://maddash.aglt2.org - set of dashboards showing an overview of the network state as seen by the perfSONAR infrastructure (NOTE: this instance is being deprecated and we plan to introduce dashboards that will replace MaDDash over the coming 2023-2024 year). WLCG Dashboards ) - set of dashboards showing WLCG and OSG network performance by combining multiple sources of data including perfSONAR, FTS, ESNet/LHCOPN traffic, etc. Network Analytics \u00b6 University of Chicago has set up an analytics platform using ElasticSearch and Kibana4 as well as Jupyter that can be used to access and analyse all the existing network measurements. Support and Feedback \u00b6 If you suspect a network problem and wish to follow up on it, we have a number of tools available. We have a ToolkitInfo page that can help you find resources to identify and explore problems. In general, networks problems are best resolved by opening a ticket with your site's network provider (see https://osg-htc.org/networking/network-troubleshooting/ ). If you want WLCG/OSG specific support, please open a ticket with the appropriate support unit: For OSG sites please open a ticket with GOC ; For WLCG sites please open a GGUS ticket to WLCG Network Throughput support unit. If you'd like to get help in setting up a WLCG/OSG perfSONAR instance please open a ticket with GOC or via GGUS to WLCG perfSONAR support. If you have problems or questions specific to perfSONAR, please email the perfSONAR user mailing list . For any other requests or to provide feedback, please open a ticket at GGUS and mention OSG networking. References \u00b6 ESNet network performance tuning and debugging https://fasterdata.es.net/ perfSONAR toolkit is part of the perfSONAR project. OSG/WLCG mesh configuration interface is available at https://psconfig.opensciencegrid.org OSG dashboard instance https://maddash.aglt2.org (NOTE: deprecated replacement) OSG perfSONAR infrastructure monitoring https://psetf.aglt2.org/etf/check_mk/ OSG Analytics platform https://atlas-kibana.mwt2.org/s/networking/app/kibana WLCG dashboards https://monit-grafana-open.cern.ch/d/MwuxgogIk/wlcg-site-network?var-bin=1h&orgId=16","title":"OSG Networking Area"},{"location":"#osg-networking-area","text":"Welcome to OSG Networking ! This is an entry point for those interested in Networking in OSG/WLCG or for those OSG/WLCG users experiencing network problems. It provides an overview of the networking goals, plans and various activities and subtopics underway regarding networking in the Open Science Grid (OSG) and World-wide LHC Computing Grid (WLCG) , operated as a joint project. This area started in June 2012 with initial focus on the network monitoring as monitoring is critical to provide needed visibility into existing networks and site connectivity. OSG is working to provide needed networking information and tools for users, sites and experiments/VOs. This documentation is divided into several sub-sections, each covering a specific area of activities.","title":"OSG Networking Area"},{"location":"#network-monitoring-in-wlcg-and-osg-perfsonar","text":"WLCG and OSG jointly operate a network of perfSONAR agents deployed world-wide, which provides an open platform that can be used to baseline network performance and debug any potential issues. The following subsections provide details on the motivation, deployment and operations of the perfSONARs in WLCG/OSG: Motivation - overview, core concepts, motivation Deployment Guide - deployment models and options, hardware requirements Installation and Administration Guide - installation, configuration and maintanance Frequently Asked Questions","title":"Network Monitoring in WLCG and OSG (perfSONAR)"},{"location":"#network-troubleshooting","text":"Users with network issues should check the troubleshooting link below for initial guidance on how best to get their issue resolved. In addition, you can refer to the ESNet network performance guide for a detailed instructions on how to identify and isolate network performance issues using perfSONAR.","title":"Network Troubleshooting"},{"location":"#network-services","text":"OSG operates an advanced platform to collect, store, publish and analyse the network monitoring data it gathers from perfSONAR and other locations. All measurements are collected and available via streaming or through APIs. The following services are available: perfSONAR infrastructure monitoring - monitors state of perfSONAR network and reports on availability of core services OSG Distributed Network Datastore - distributed datastore based on ElasticSearch holding all the network measurements and providing an API to expose them via JSON is available at two locations (University of Chicago and University of Nebraska). OSG pSConfig Web Admin (PWA) - centralized configuration of the tests performed by the OSG/WLCG perfSONAR infrastructure . In case you'd like to start/manage particular mesh, please contact our support channels to get access. OSG Dashboards http://maddash.aglt2.org - set of dashboards showing an overview of the network state as seen by the perfSONAR infrastructure (NOTE: this instance is being deprecated and we plan to introduce dashboards that will replace MaDDash over the coming 2023-2024 year). WLCG Dashboards ) - set of dashboards showing WLCG and OSG network performance by combining multiple sources of data including perfSONAR, FTS, ESNet/LHCOPN traffic, etc.","title":"Network Services"},{"location":"#network-analytics","text":"University of Chicago has set up an analytics platform using ElasticSearch and Kibana4 as well as Jupyter that can be used to access and analyse all the existing network measurements.","title":"Network Analytics"},{"location":"#support-and-feedback","text":"If you suspect a network problem and wish to follow up on it, we have a number of tools available. We have a ToolkitInfo page that can help you find resources to identify and explore problems. In general, networks problems are best resolved by opening a ticket with your site's network provider (see https://osg-htc.org/networking/network-troubleshooting/ ). If you want WLCG/OSG specific support, please open a ticket with the appropriate support unit: For OSG sites please open a ticket with GOC ; For WLCG sites please open a GGUS ticket to WLCG Network Throughput support unit. If you'd like to get help in setting up a WLCG/OSG perfSONAR instance please open a ticket with GOC or via GGUS to WLCG perfSONAR support. If you have problems or questions specific to perfSONAR, please email the perfSONAR user mailing list . For any other requests or to provide feedback, please open a ticket at GGUS and mention OSG networking.","title":"Support and Feedback"},{"location":"#references","text":"ESNet network performance tuning and debugging https://fasterdata.es.net/ perfSONAR toolkit is part of the perfSONAR project. OSG/WLCG mesh configuration interface is available at https://psconfig.opensciencegrid.org OSG dashboard instance https://maddash.aglt2.org (NOTE: deprecated replacement) OSG perfSONAR infrastructure monitoring https://psetf.aglt2.org/etf/check_mk/ OSG Analytics platform https://atlas-kibana.mwt2.org/s/networking/app/kibana WLCG dashboards https://monit-grafana-open.cern.ch/d/MwuxgogIk/wlcg-site-network?var-bin=1h&orgId=16","title":"References"},{"location":"BROKEN_LINKS_REPORT/","text":"Broken Links Report \u00b6 Generated: 2025-11-03T18:02:40.531421Z index.md \u00b6 Link text: *OSG Dashboards* \u2014 href: https://maddash.aglt2.org external check failed: <urlopen error [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1032)>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. network-troubleshooting\\osg-debugging-document.md \u00b6 Link text: rs@internet2.edu \u2014 href: mailto:rs@internet2.edu (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: trouble@es.net \u2014 href: mailto:trouble@es.net (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: rs@internet2.edu \u2014 href: mailto:rs@internet2.edu (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: trouble@es.net \u2014 href: mailto:trouble@es.net (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: noc@nlr.net \u2014 href: mailto:noc@nlr.net (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: goc@opensciencegrid.org \u2014 href: mailto:goc@opensciencegrid.org (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. perfsonar\\install-testpoint.md \u00b6 Link text: Red Hat Policy Routing \u2014 href: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/assembly_configuring-policy-based-routing_configuring-and-managing-networking (external check failed: HTTP Error 403: Forbidden) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. perfsonar\\psetf.md \u00b6 Link text: perfSONAR monitoring instance \u2014 href: https://psetf.aglt2.org/etf/check_mk/index.py?start_url=%2Fetf%2Fcheck_mk%2Fdashboard.py (external check failed: HTTP Error 401: Unauthorized) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: ETF \u2014 href: http://etf.cern.ch/docs/latest/ (external check failed: <urlopen error [Errno 11001] getaddrinfo failed>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status.","title":"Broken Links Report"},{"location":"BROKEN_LINKS_REPORT/#broken-links-report","text":"Generated: 2025-11-03T18:02:40.531421Z","title":"Broken Links Report"},{"location":"BROKEN_LINKS_REPORT/#indexmd","text":"Link text: *OSG Dashboards* \u2014 href: https://maddash.aglt2.org external check failed: <urlopen error [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1032)>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status.","title":"index.md"},{"location":"BROKEN_LINKS_REPORT/#network-troubleshootingosg-debugging-documentmd","text":"Link text: rs@internet2.edu \u2014 href: mailto:rs@internet2.edu (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: trouble@es.net \u2014 href: mailto:trouble@es.net (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: rs@internet2.edu \u2014 href: mailto:rs@internet2.edu (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: trouble@es.net \u2014 href: mailto:trouble@es.net (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: noc@nlr.net \u2014 href: mailto:noc@nlr.net (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: goc@opensciencegrid.org \u2014 href: mailto:goc@opensciencegrid.org (external check failed: <urlopen error unknown url type: mailto>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status.","title":"network-troubleshooting\\osg-debugging-document.md"},{"location":"BROKEN_LINKS_REPORT/#perfsonarinstall-testpointmd","text":"Link text: Red Hat Policy Routing \u2014 href: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/assembly_configuring-policy-based-routing_configuring-and-managing-networking (external check failed: HTTP Error 403: Forbidden) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status.","title":"perfsonar\\install-testpoint.md"},{"location":"BROKEN_LINKS_REPORT/#perfsonarpsetfmd","text":"Link text: perfSONAR monitoring instance \u2014 href: https://psetf.aglt2.org/etf/check_mk/index.py?start_url=%2Fetf%2Fcheck_mk%2Fdashboard.py (external check failed: HTTP Error 401: Unauthorized) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status. Link text: ETF \u2014 href: http://etf.cern.ch/docs/latest/ (external check failed: <urlopen error [Errno 11001] getaddrinfo failed>) \u2014 Check target path or update to correct URL. If external, run script with --check-externals to test HTTP status.","title":"perfsonar\\psetf.md"},{"location":"network-troubleshooting/","text":"Network and Network Troubleshooting Documentation \u00b6 This page will eventually include pointers to various network troubleshooting resources. In general, when users suspect a network problem the procedure is: Document the problem : Basically you need to describe the problem you encountered. Provide any relevant details like the exact command you used, any errors or warning you got and any problems you observed. This can help you better understand the problem and will allow you to easily \"hand-off\" troubleshooting to an expert Gather relevant data: Run tests (see Guide below) and capture the results. Contact your local network support (this is sometimes where users don't know where to go). Google or your campus web-pages should be able to help you find the right contact. Escalate , if the problem persists (See below) ESnet maintains a very useful page on network troubleshooting at https://fasterdata.es.net/performance-testing/troubleshooting/ We have a (older) draft version of the OSG network debugging document that describes things in much more detail. If you have comments, questions or suggestions, please contact Shawn McKee. If you want to learn more about perfSONAR and its various components, the Network Startup Resource Center maintains a list of training videos at https server learn.nsrc.org/perfsonar. Information on Contacting Network Support \u00b6 There are numerous regional and campus network operations centers that have their own ticketing systems for reporting problems. I encourage you to identify how to contact your local campus network support personnel. Suggestion: try using Google with \"YourUniversity network trouble ticket\" as the search terms (of course substitute your university name for YourUniversity ). Below are the links you can use to report problems to OSG or the major Research and Education networks. If you can't determine who your local contact should be or they are unable to help you resolve the issue, you should open a ticket with one of these entities: INTERNET2 : If you are located at a University in the United States you are likely connected to Internet2. You can find the details on opening a ticket with Internet2 at: https://noc.net.internet2.edu/internet2/help/index.html This is typically a good choice for support beyond your campus. ESNET : If your problem involves a national laboratory in the United States, a university connected to ESnet or a trans-Atlantic network problem, then you may want to contact ESnet: http://es.net/about/contact-us/ This page contains information on opening ticket and pointers to relevant tools and documentation. ESnet serves the Department of Energy national labs. OSG : If you have questions or unusual problems you think are OSG related, feel free to contact the OSG GOC. You can also open network related tickets and OSG can help to 'triage' your request and get it to the right people: https://support.opensciencegrid.org/support/home WLCG : WLCG has a dedicated support unit, which will assist in debugging the problem and triaging the issue. More details can be found at: https://twiki.cern.ch/twiki/bin/view/LCG/NetworkTransferMetrics#Network_Throughput_Support_Unit","title":"Network Troubleshooting"},{"location":"network-troubleshooting/#network-and-network-troubleshooting-documentation","text":"This page will eventually include pointers to various network troubleshooting resources. In general, when users suspect a network problem the procedure is: Document the problem : Basically you need to describe the problem you encountered. Provide any relevant details like the exact command you used, any errors or warning you got and any problems you observed. This can help you better understand the problem and will allow you to easily \"hand-off\" troubleshooting to an expert Gather relevant data: Run tests (see Guide below) and capture the results. Contact your local network support (this is sometimes where users don't know where to go). Google or your campus web-pages should be able to help you find the right contact. Escalate , if the problem persists (See below) ESnet maintains a very useful page on network troubleshooting at https://fasterdata.es.net/performance-testing/troubleshooting/ We have a (older) draft version of the OSG network debugging document that describes things in much more detail. If you have comments, questions or suggestions, please contact Shawn McKee. If you want to learn more about perfSONAR and its various components, the Network Startup Resource Center maintains a list of training videos at https server learn.nsrc.org/perfsonar.","title":"Network and Network Troubleshooting Documentation"},{"location":"network-troubleshooting/#information-on-contacting-network-support","text":"There are numerous regional and campus network operations centers that have their own ticketing systems for reporting problems. I encourage you to identify how to contact your local campus network support personnel. Suggestion: try using Google with \"YourUniversity network trouble ticket\" as the search terms (of course substitute your university name for YourUniversity ). Below are the links you can use to report problems to OSG or the major Research and Education networks. If you can't determine who your local contact should be or they are unable to help you resolve the issue, you should open a ticket with one of these entities: INTERNET2 : If you are located at a University in the United States you are likely connected to Internet2. You can find the details on opening a ticket with Internet2 at: https://noc.net.internet2.edu/internet2/help/index.html This is typically a good choice for support beyond your campus. ESNET : If your problem involves a national laboratory in the United States, a university connected to ESnet or a trans-Atlantic network problem, then you may want to contact ESnet: http://es.net/about/contact-us/ This page contains information on opening ticket and pointers to relevant tools and documentation. ESnet serves the Department of Energy national labs. OSG : If you have questions or unusual problems you think are OSG related, feel free to contact the OSG GOC. You can also open network related tickets and OSG can help to 'triage' your request and get it to the right people: https://support.opensciencegrid.org/support/home WLCG : WLCG has a dedicated support unit, which will assist in debugging the problem and triaging the issue. More details can be found at: https://twiki.cern.ch/twiki/bin/view/LCG/NetworkTransferMetrics#Network_Throughput_Support_Unit","title":"Information on Contacting Network Support"},{"location":"osg-network-analytics/","text":"OSG Network Analytics \u00b6 OSG Analytics platform http://atlas-kibana.mwt2.org/s/networking/app/kibana#/dashboard/Default?_g=() WLCG dashboards https://monit-grafana-open.cern.ch/d/000000523/home?orgId=16","title":"Network Analytics"},{"location":"osg-network-analytics/#osg-network-analytics","text":"OSG Analytics platform http://atlas-kibana.mwt2.org/s/networking/app/kibana#/dashboard/Default?_g=() WLCG dashboards https://monit-grafana-open.cern.ch/d/000000523/home?orgId=16","title":"OSG Network Analytics"},{"location":"osg-network-services/","text":"OSG Network Services \u00b6 Open Science Grid is providing a number of network-related services to support its members and collaborators in monitoring, understanding, measuring, diagnosing and managing the networks used to support their work. Here we will provide an overview of the OSG network services and where to find more information. Much of the work to create effective analytics is being done in the NSF funded SAND project which is exploring technologies and methodologies to analyze, summarize and visualize the various networking metrics we have available in the OSG networking data pipeline. As some of these tools and dashboards become production ready, we will add them to this documentation.","title":"Network Datastore and Streaming"},{"location":"osg-network-services/#osg-network-services","text":"Open Science Grid is providing a number of network-related services to support its members and collaborators in monitoring, understanding, measuring, diagnosing and managing the networks used to support their work. Here we will provide an overview of the OSG network services and where to find more information. Much of the work to create effective analytics is being done in the NSF funded SAND project which is exploring technologies and methodologies to analyze, summarize and visualize the various networking metrics we have available in the OSG networking data pipeline. As some of these tools and dashboards become production ready, we will add them to this documentation.","title":"OSG Network Services"},{"location":"perfsonar-in-osg/","text":"Overview of perfSONAR \u00b6 For those not familiar with perfSONAR , this page provides a quick overview of what it is and why we recommend its deployment at OSG and WLCG sites. OSG is working to support the scientific networking needs of it's constituents and collaborators. To do this, we are recommending all sites deploy perfSONAR so we can measure, monitor and diagnose the OSG (and WLCG) networks. Motivation \u00b6 Distributed scientific computing relies upon networks to interconnect resources and make them usable for scientific workflows. This dependency upon the network means that issues in our networks can significantly impact the behavior of all the various cyber-infrastructure components that rely upon it. Compounding the problem is that networks, by their nature, are distributed and typically involve many different \"owners\" and administrators. When a problem arises somewhere along a network path, it can be very difficult to identify and localize. This was the context for the formation of the perfSONAR collaboration . This collaboration is focused on developing and deploying the perfSONAR software suite in support of network monitoring across the global set of research and education (R&E) networks. The Open Science Grid ( OSG ) has chosen to base the core of its network monitoring framework on perfSONAR because of both the capabilities of the toolkit for measuring our networks and its global acceptance as the defacto network monitoring infrastructure of first choice. The https://www.perfsonar.net/about/what-is-perfsonar/ provides a succinct summary: perfSONAR is a network measurement toolkit designed to provide federated coverage of paths, and help to establish end-to-end usage expectations. There are 1000s of perfSONAR instances deployed world wide, many of which are available for open testing of key measures of network performance. This global infrastructure helps to identify and isolate problems as they happen, making the role of supporting network users easier for engineering teams, and increasing productivity when utilizing network resources. How can OSG/WLCG members and collaborators understand, maintain and effectively utilize the networks that form the basis of their distributed collaborations? Our answer starts by providing visibility into our networks by the deployment of perfSONAR . perfSONAR allows us to regularly and consistently measure a set of network metrics that we can use to understand how our networks are operating. When problems arise, the data, along with access to the perfSONAR tools, can be used to diagnose and localize problems. The presence of perfSONAR toolkit deployments across our sites and networks makes identifying and fixing network problems feasible. We strongly recommend that all OSG (and WLCG) sites deploy perfSONAR toolkit instances as described in our installation guide . Before installing you should consult the requirements along with the guidance on deployment models . Note Installing perfSONAR not only benefits users at a site but will enable network engineers and OSG staff to much more effectively support those sites if network issues are suspected. All OSG and WLCG sites should deploy two perfSONAR instances: one to measure latency/packet loss and one to measure bandwidth. It is possible to install both versions on a single host with at least two NICs by following the instructions at multiple NIC guidance page . Warning It is very important that the perfSONAR instances be located in the same subnet as the primary storage for the site. This is to ensure that we are measuring as much of the network path involved with data transfer as possible. The WLCG Network Throughput Working Group is responsible for monitoring the WLCG/OSG instances and for defining and maintaining the mesh-configurations that we use to control perfSONAR testing. Please contact us if you have questions or suggestions related to perfSONAR testing amongst WLCG sites. For anyone maintaining/using perfSONAR we suggest to join either/both of the following mailing lists: User's Mailing List The perfSONAR project maintains a mailing list for communication on matters of installation, configuration, bug reports, or general performance discussions: https://lists.internet2.edu/sympa/subscribe/perfsonar-user Announcement Mailing List The perfSONAR project also maintains a low volume mailing list used for announcements related to software updates and vulnerabilities: https://lists.internet2.edu/sympa/subscribe/perfsonar-announce Changes for perfSONAR 5.0 \u00b6 The first release of perfSONAR 5.0.0 was available on April 17, 2023, followed by a version supporting EL8/EL9 on June 21, 2023 ( version 5.0.3 ). All sites following our recommendation of having auto-updates enabled should have upgraded during within 1-2 days after the releases. Version 5 marks a transition for the OSG/WLCG perfSONAR deployment, enabling us to migrate to a new network data pipeline where perfSONAR hosts directly send their measurement data to our central Elasticsearch instance via an HTTP-Archiver and Logstash. Highlights include: Use of Opensearch for the measurement archive, replacing ESmond Support for new OSes, include EL8 and EL9. No longer supports ISO install option. Significant number of bug fixes and new features. We recommend deploying or updating to the latest version available (v5.2.3 as of October 2025). For a more complete list of changes, see the full release notes at https://www.perfsonar.net/docs_releasenotes.html","title":"Motivation"},{"location":"perfsonar-in-osg/#overview-of-perfsonar","text":"For those not familiar with perfSONAR , this page provides a quick overview of what it is and why we recommend its deployment at OSG and WLCG sites. OSG is working to support the scientific networking needs of it's constituents and collaborators. To do this, we are recommending all sites deploy perfSONAR so we can measure, monitor and diagnose the OSG (and WLCG) networks.","title":"Overview of perfSONAR"},{"location":"perfsonar-in-osg/#motivation","text":"Distributed scientific computing relies upon networks to interconnect resources and make them usable for scientific workflows. This dependency upon the network means that issues in our networks can significantly impact the behavior of all the various cyber-infrastructure components that rely upon it. Compounding the problem is that networks, by their nature, are distributed and typically involve many different \"owners\" and administrators. When a problem arises somewhere along a network path, it can be very difficult to identify and localize. This was the context for the formation of the perfSONAR collaboration . This collaboration is focused on developing and deploying the perfSONAR software suite in support of network monitoring across the global set of research and education (R&E) networks. The Open Science Grid ( OSG ) has chosen to base the core of its network monitoring framework on perfSONAR because of both the capabilities of the toolkit for measuring our networks and its global acceptance as the defacto network monitoring infrastructure of first choice. The https://www.perfsonar.net/about/what-is-perfsonar/ provides a succinct summary: perfSONAR is a network measurement toolkit designed to provide federated coverage of paths, and help to establish end-to-end usage expectations. There are 1000s of perfSONAR instances deployed world wide, many of which are available for open testing of key measures of network performance. This global infrastructure helps to identify and isolate problems as they happen, making the role of supporting network users easier for engineering teams, and increasing productivity when utilizing network resources. How can OSG/WLCG members and collaborators understand, maintain and effectively utilize the networks that form the basis of their distributed collaborations? Our answer starts by providing visibility into our networks by the deployment of perfSONAR . perfSONAR allows us to regularly and consistently measure a set of network metrics that we can use to understand how our networks are operating. When problems arise, the data, along with access to the perfSONAR tools, can be used to diagnose and localize problems. The presence of perfSONAR toolkit deployments across our sites and networks makes identifying and fixing network problems feasible. We strongly recommend that all OSG (and WLCG) sites deploy perfSONAR toolkit instances as described in our installation guide . Before installing you should consult the requirements along with the guidance on deployment models . Note Installing perfSONAR not only benefits users at a site but will enable network engineers and OSG staff to much more effectively support those sites if network issues are suspected. All OSG and WLCG sites should deploy two perfSONAR instances: one to measure latency/packet loss and one to measure bandwidth. It is possible to install both versions on a single host with at least two NICs by following the instructions at multiple NIC guidance page . Warning It is very important that the perfSONAR instances be located in the same subnet as the primary storage for the site. This is to ensure that we are measuring as much of the network path involved with data transfer as possible. The WLCG Network Throughput Working Group is responsible for monitoring the WLCG/OSG instances and for defining and maintaining the mesh-configurations that we use to control perfSONAR testing. Please contact us if you have questions or suggestions related to perfSONAR testing amongst WLCG sites. For anyone maintaining/using perfSONAR we suggest to join either/both of the following mailing lists: User's Mailing List The perfSONAR project maintains a mailing list for communication on matters of installation, configuration, bug reports, or general performance discussions: https://lists.internet2.edu/sympa/subscribe/perfsonar-user Announcement Mailing List The perfSONAR project also maintains a low volume mailing list used for announcements related to software updates and vulnerabilities: https://lists.internet2.edu/sympa/subscribe/perfsonar-announce","title":"Motivation"},{"location":"perfsonar-in-osg/#changes-for-perfsonar-50","text":"The first release of perfSONAR 5.0.0 was available on April 17, 2023, followed by a version supporting EL8/EL9 on June 21, 2023 ( version 5.0.3 ). All sites following our recommendation of having auto-updates enabled should have upgraded during within 1-2 days after the releases. Version 5 marks a transition for the OSG/WLCG perfSONAR deployment, enabling us to migrate to a new network data pipeline where perfSONAR hosts directly send their measurement data to our central Elasticsearch instance via an HTTP-Archiver and Logstash. Highlights include: Use of Opensearch for the measurement archive, replacing ESmond Support for new OSes, include EL8 and EL9. No longer supports ISO install option. Significant number of bug fixes and new features. We recommend deploying or updating to the latest version available (v5.2.3 as of October 2025). For a more complete list of changes, see the full release notes at https://www.perfsonar.net/docs_releasenotes.html","title":"Changes for perfSONAR 5.0"},{"location":"features/fail2ban/","text":"When to enable fail2ban: when exposed services accept external credentials (ssh, web admin). Example configuration and a sample jail are provided.","title":"fail2ban"},{"location":"features/nftables/","text":"Example rules and tips for debugging blocked tests.","title":"nftables"},{"location":"features/selinux/","text":"Short guidance: prefer permissive mode for initial deployment only; then create minimal allow rules and move to enforcing mode.","title":"SELinux"},{"location":"network-troubleshooting/osg-debugging-document/","text":"OSG Debugging Documentation \u00b6 Edited By: J. Zurawski \u2013 Internet2, S. McKee \u2013 University of Michigan February 4th 2013 Note ``` This document is old but still may have useful information. Many tools it references may no longer be supported or available. ``` Abstract \u00b6 Scientific progress relies on intricate systems of computers and the networks that connect them. Often it is the case that scientific data is gathered via a well defined process, information is digitized, stored, transmitted, and processed by members of large and distributed collaborations. The Open Science Grid advances science through the concept of distributed computing \u2013 the process for sharing resources through a unified software framework focused on the common tasks of data movement, processing, and analysis. Networks are an integral part of the distributed computing process. Similar to the computational and storage resources, it is crucial that all networking components, on the complete end-to-end path, are functional and free of physical and logical flaws. A rich set of measurement and monitoring tools exists to provide network operations staff and end users a window into the functionality of networks, despite the fact that these actors do not have direct control over the complete path their data may travel. This document discusses common measurement and monitoring tools available to the OSG community, and strategies to deploy, use, and interpret the results they produce. The end goal is to give end users more insight into network behavior, and assist local and remote networking staffs as they correct damaging performance problems that will impact the scientific process. Introduction \u00b6 The process of science is often complicated when viewed as a complete system. At the core of any project, there is a mechanism to observe or simulate some system, and produce meaningful results that will be interpreted and scrutinized between experimentation runs. The machinery that surrounds this process can be as benign as simple cameras, or as complex as the Large Hadron Collider and its associated experiments. Other common components include ways to digitize, store, process, and share the end result of experimentation \u2013 often done using computational systems. Computation falls into 3 broad classifications, all of which are required to implement the paradigm of scientific computing: Storage \u2013 Readable and writable physical medium used for temporary or long term residency of gathered data. Processing \u2013 Specially designed hardware and software that iterates over collected data sets looking for pre-defined triggers and results. Networking \u2013 Interconnecting hardware and software used to facilitate communication between storage and processing components both on a local, and fully distributed basis. When fully realized, even a small facility can contribute a great amount of resources to the overall goal of scientific advancement. In practice it may be the case that a lab consisting of a single researcher can pull data sets from a centralized location, perform carefully selected segments of an entire set of analysis that is required, and return any relevant results as they are discovered. When used in an inductive fashion, one can imagine the overall throughout that a VO such as the LHC project is able to attain through 100s of distributed facilities and 1000s of collaborating researchers. Complexity is present as we travel down the individual technology items in the above scenario. Often it is the case that ideal performance is hard to attain due to the intertwined nature of the mechanisms involved. For example, data must be written and read from physical medium. Often this step is slower due to the mechanical nature of the process, and struggles to keep up with faster technologies such as processing or transmission on network infrastructure. Equally, it may be the case that a flaw in the network infrastructure, such as a failing component, can introduce data loss that must be compensated for through retransmission. Retransmission implies additional work for storage and processing components that must waste resources to overcome a fixable, but often unnoticed, problem. Network performance monitoring is a relatively unseen, but still extremely necessary, practice. This statement is true due to the nature of network use through application software and communication protocols. Application developers wish to unburden the user with details about \"how\" data may be moved between facilities. Care is taken to design applications in such a way that the user is simply presented with options related to a source and destination only, and little or no insight into the path taken or the current conditions that may be present. The aforementioned situation where a failing component institutes data loss results in only one symptom to the end user: lower than expected throughput. Many users may not notice, or have become complacent, with low performance situations. Some may write this off as \"the network is slow\", or perhaps will not notice at all due to experiences with home connections that are often 2 orders of magnitude slower than what is possible in a typical academic environment. Software exists to monitor network performance in many different ways. For example it is possible take a measure of network throughput, and simulate the behavior of a file transfer application. It is also possible to observe network stability (e.g. jitter) over time to simulate video or audio transmission. These basic observations are powerful when used both in a local environment, as well as on an end-to-end basis. In either case, software must be deployed and available to the community on points of interest: specifically on the local and shared network infrastructure distributed around the world. perfSONAR is a software framework that simplifies network debugging activities by making it easier to deploy measurement tools, and facilities the sharing of results. It is currently deployed on many communal networking resources in the R&E community, including backbones, regional networks, campuses, and individual laboratories. Once perfSONAR is deployed, it becomes possible to troubleshoot situations that result in low throughout for the end user in a straightforward manner. It is important to note that when something like this occurs, it is not the sole responsibility of the end user to debug and solve a networking problem; rather it is their duty to report the problem and provide as much information as possible to local or remote network staffs so they may learn about the issue, and interpret the results so as to lead to quick and efficient problem resolution. Locating this staff may be challenging, but many organizations maintain a dedicated Network Operations Center (NOC) whose staff are ready to accept trouble reports and act in an appropriate manner. Section 8 details locations you may turn to for additional support. This document will expand upon these topics in the remaining sections, and conclude with information where additional resources beyond a simple introduction to these topics can be found. The Scientific Networking Process \u00b6 There is a rich ecosystem of components available for monitoring and managing the scientific networking process. This myriad of hardware and software must work together to complete the overall goal of interpreting gathered or simulated observations. Each component we will discuss has the ability to be installed, operated, and maintained in different ways. Individual brands or versions are not important, but the overall idea of each will be explored. Hosts \u00b6 Server or \"host\" hardware and software can be used in many different ways. Often it is the case that these components serve as computational resources for processing data, or provide access to underlying data stored on physical media. It may also be the case that software designed to \"glue\" components of a framework together (e.g. processing mail, authenticating users, providing mappings between names and addresses) is installed on a dedicated or shared host resource. Hosts must contain an operating system: software designed to control and maintain the underlying hardware such as storage media, network interface cards, processors, and other peripheral devices. Operating system hardware can vary in functionality; completely interactive systems such as those found on laptops can be more pleasing for a human user vs. that of a no frills batch processing system designed to only iterate over scientific data. The choice of operating system will vary from use case to use case. The footing provided by the hardware and operating system serves as the basis for the remainder of the components in this discussion. Protocols \u00b6 Protocols are software algorithms implemented on hosts and networking components, and are used to facilitate communication strategies. Protocols are constructed in a \"layered\" fashion, and are designed to handle certain aspects of the overall communication plan. For instance a protocol may be used to communicate between two network devices, and may involve the use of different patterns of electrical or optical signal. On top of this basic system of signals we may construct a different protocol that is focused on communication between end hosts, and is able to break up the notion of a user's files into small chunks so they can be sent reliably end-to-end. Protocols evolve with the underlying technology, and often can be tuned to specific use cases. One such protocol, Transmission Control Protocol (TCP), is widely used in applications that many users are familiar with such as web browsing, mail transfer, and file exchange. Early incarnations of TCP were designed for the networks of the 1980s; slower, less reliable, constructions than what is present in the networks of the 2010s. As such TCP must be instructed, via configuration on a host or network devices operating environment, that it should behave in a different and more efficient manner. With the protocol in place, we can now begin to discuss applications that are able to use the network to communicate in an automated fashion. Applications \u00b6 Applications are specific use cases, programmed as software, and made available to end users via a host's operating environment. There are numerous applications we use on a daily basis \u2013 web browsers to fetch remote content, word processing applications to type papers, mail and instant messaging clients to exchange information in near real time. Scientific applications normally focus on performing a single task (e.g. end-to-end data movement, visualization, data transformation, data analysis) on either a local or distributed basis. In the case of distribution, it becomes necessary to interact with the underlying network through the use of a protocol. File transfer is a specialized application that takes is interested in either sending a local file to a remote location, or retrieving remote data to bring locally. In either case the application must broker with a protocol, such as TCP, that is available on both ends of the transfer. Through a series of API calls information is segmented into transmission chunks and sent reliably though the communication medium. Most of this is handled transparently from the user's perspective, and as such they are not given much in the way of feedback beyond a pass or failure, and some notion of how long it took. Understanding more about the network can be enlightening exercise for users who are unaware of the complexity and span of components that are required for operation. This will be discussed in the remaining sections. Lab Local Area Network \u00b6 The first step in the networking tree is often the interconnections between components local to the user. This may consist of the storage and processing nodes in a single rack or data center used for scientific processing, connected via technology consistent with the tasks they are performing. Cluster nodes may use a high speed interconnect such as InfiniBand; servers may also just use typical Ethernet at 1Gbps or 10Gbps. In either case, there will be dedicated network equipment with the task of aggregating and controlling traffic flow to the local devices, and serving as an uplink to the next network in the chain (the campus). Monitoring and management of this local environment is a good idea, either through passive means such as using the SNMP system, or active tests that check the health of transfers on a local basis. Campus Local Area Network \u00b6 The first hop beyond a laboratory environment is a network maintained by campus/local support staff. It is often the case that this group is maintaining the infrastructure for the use of all end users, and as such will design and maintain things to preserve uniform functionality and performance. Campus networks are an even larger ecosystem of devices given the area they may cover. It may be the case that the network in the previous section is behind several devices before it has a clean path to the outside world. It may also be the case that traffic aggregation is extremely high, and congestion becomes a factor during certain parts of the day or times of the year. These nuances make local performance monitoring crucial for operational soundness. This group is also the first contact that should be exercised in the event of a network performance problem. While they may not be able to answer for the status of the entire path, they can escalate the problem to the regional or backbone support staff as needed. Regional Network \u00b6 A regional network provider aggregates the networks of numerous campuses in a state, country, or pre-defined region. Examples include provider for states of the US (e.g. KanREN), countries (SWITCH, the network of Switzerland), or collaboration between parties without a political boundary (the SOX regional network in the United States). Regional networks may cover a large geographical area, but often have less equipment than a campus. The role of a regional aggregator is to take connections (large and small) and condense them into long-haul links that will uplink to a backbone or exchange point as a next step. Regional operations teams have similar performance concerns to that of a campus network. The aggregation point of several networks can be a critical component, and one of the more likely places that a failing piece of equipment or congestion can impact downstream network users. Monitoring local and remote components (e.g. maintaining active testing between networks) critical. Regional support teams can be likely candidates for assistance on performance problems, but users are reminded to discuss options with their local staff first before engaging with these groups. R&E Backbone \u00b6 An R&E backbone consists of an aggregation of numerous regional providers. Capacity must reflect the number and expectations of this group of customers, and often is orders of magnitude higher than other networks that are downstream. As an aggregation point, normally spread over a very large geographical area, traffic flows will be numerous, of mixtures of size and duration, and be destined for diverse destinations domestically and internationally. Each Point of Presence (PoP) could have a large number of customers integrating, and thus increases the chance of an issue local to this device. As a service to customers, the R&E backbone should consider making test instances available to help bisect and debug challenging problems that may cross the domain. Backbone support teams are also well trained and have knowledge of performance monitoring. Some providers such as ESnet and Internet2 have dedicated staffs just to support the troubleshooting of network problems for customers. An end user is encouraged to seek out these resources, as well as those that are local, when debugging a problem impacting scientific work. Exchange Point \u00b6 An exchange point is normally a location where multiple backbone networks and international transit links (e.g. trans-oceanic links) combine and transit to other domains. An exchange point is a special case of an aggregation network like a regional in that policies may be different depending on the membership structure. International exchange points suffer from the aforementioned problems of traffic aggregation wherein congestion or equipment failure will have a severe impact on all traffic. Monitoring these devices is crucial, as in other use cases. Actor & Agent Definitions \u00b6 There are many actors involved in the process of network management and debugging. We will highlight three here, as they represent the most critical members of the support team that OSG has available when problems are discovered. End User \u00b6 The end user is understood to be the primary user and beneficiary of OSG software to process and operate on scientific data sets. The sophistication of this end user is assumed to be beginner to average in matters related to system and network administration. In general we assume they are knowledgeable enough to install and maintain OSG software, and connect devices to the networking infrastructure. This actor is assumed to be the primary user of the perfSONAR end user tools, packaged in the OSG VDT distribution. These tools are meant to be run from a system to upstream test machines provided by the campus, regional, or backbone network. Local Administrator \u00b6 The local administrator can be campus support staff responsible for the health of servers or network devices across the greater campus ecosystem. Their primary responsibility it to ensure uptime of the network for all users, as well as assist in debugging specific problems caused by performance impacting problems on a local basis. This actor may not be able to directly address problems on a regional, national, or international basis but can serve as a liaison with individuals within those stewardship organizations. Remote Administrator \u00b6 A remote administrator can be regional, backbone, or exchange point staff responsible for the health of remote networking resources. It is often the case that these individuals may not be aware of a specific use case between remote campuses, but could answer questions about the current health and status of the network they control. These individuals are assumed to be knowledgeable about performance tools, and can work with local staff as needed to make test points available to assist in debugging. Local Preparations \u00b6 A first step to any OSG software installation to support scientific activity is preparation of the local environment. Given the considerations denoted in the previous sections, we will discuss 3 specific preparation activities: End System Operating System and Protocol Tuning Network Architecture Adjustments Network Configuration Tuning Each of these steps is considered to be most relevant to the laboratory local environment, although some should be considered for the campus as well. It is assumed that the end user actor, with the help of local administrators, can make these changes. End System Tuning \u00b6 Computer systems are similar to automobiles in that its possible to \"tune\" certain internal aspects and achieve higher performance when using the network. The operating system and associated protocols like TCP make these changes rather simple to implement. In general there are several options worth considering: Network interface cards have an adjustable size for their packet queues Kernel buffers can be increased to support long distance transfers The TCP congestion control algorithm can be changed ESnet has made a web based resource available to assist with the task of host tuning, it can be found here: http://fasterdata.es.net/host-tuning/ Network Architecture \u00b6 Architectural decisions are often involved and will involve the input of local support staff. In general laboratory architecture should be robust in the following manner: Multiple uplinks to the campus network to provide capacity and resiliency A limited amount of \"fan in\" (e.g. number of connections) into a given access switch. It is recommended that as the fan in grows, multiple switches be employed to manage connectivity and congestion Elimination of firewalls from the path. Security can be implemented by host-based firewalls that restrict ports as well as Access Control Lists (ACLs) to white list sites that are communicated with. Firewall devices have been known to severely impact traffic for bandwidth intensive applications. Choice of device manufacturer that allows for out of band management and monitoring (e.g. SNMP) of devices Choice of device manufacturer that allows for per-interface tuning of memory buffers (vs. that of a shared memory region) Network architectural considerations are far too broad to be represented in a single document for the OSG, and the interested reader is encouraged to read the following resource provided by ESnet: http://fasterdata.es.net/science-dmz Network Configuration Tuning \u00b6 Much like end hosts, network devices have the ability to be \"tuned\" for specific use cases. This tuning normally centers on enabling or disabling certain features on a router or switch (e.g. policy maps) or adjusting the available memory available to account for a specific use case (e.g. less memory for a video application, more for a throughput intensive tool). As every manufacturer provides different interfaces to the underlying hardware, we cannot make specific recommendations in this document. The interested reader is encouraged to read this guide provided by ESnet: http://fasterdata.es.net/network-tuning/router-tuning/ Measurement Software \u00b6 The available span of network measurement software is vast. A simple web search will reveal 10s of names, some still active and others long dead. The R&E community began to standardize on available tools in the later part of the 2000s with an effort to unify measurement tools and infrastructure to support them: perfSONAR. perfSONAR is a framework to simplify end to end network debugging. It consists of a layer of middleware, designed to sit between the measurement tools and the visualization and analysis that is useful to human users. A key component of the perfSONAR concept is the pS Performance Toolkit; this completely enclosed operating system packages performance tools and easy to follow GUIs to enable configuration. perfSONAR focuses on several key metrics: Achievable Bandwidth One Way Latency Round Trip Latency Packet Loss, Duplication, Out of Orderness Interface Utilization, Errors, Discards Layer 3 Path Path MTU Many of these metrics are calculated through simple tests that can be run from the command line. The OSG VDT package contains 3 key measurement tools that will be used as we discuss networking debugging in Section 7: BWCTL \u2013 A tool for measuring end to end bandwidth availability NDT \u2013 A tool designed to diagnose several different aspects of a host and network OWAMP \u2013 A tool designed for measuring one way delays of packets, as well as loss, duplication, and out of orderness. These 3 command line tools, when installed on a compute or storage node, can be used to launch tests to perfSONAR servers located in the greater R&E networking world, e.g. on the campus, regional, backbone, or exchange point networks. Debugging Process \u00b6 The following sections will discuss the process to install, use, and interpret measurement tools in an OSG software environment. End users are encouraged to try these steps first, but also contact their local support staff at the earliest possible moment. Involving support staff will ensure that expert eyes are available to assist with problems, and funnel the requests for help to the proper area (e.g. GOC, other networks, etc.). Software Installation \u00b6 Client software can be installed in one of two ways, either though the OSG VDT or via RPMs from the perfSONAR web site. OSG Software \u00b6 [INSERT INSTRUCTIONS ON HOW TO INSTALL VDT HERE] perfSONAR-PS Software \u00b6 All perfSONAR software is available through an RPM (Red Hat Package Manager) repository to make for easy installation and updates. The following steps can be taken to install these tools: Import the Internet2 Signing Key The following command will import the key. sudo rpm --import http://software.internet2.edu/rpms/RPM-GPG-KEY-Internet2 Download RPM Software The latest version for CentOS 5 and 6 (both x86 and x86_64 architectures) can be found on the the following web site: http://software.internet2.edu Note that SL5 andSL6, RHEL 5 and RHEL 6 should work with these builds. Those using other operating systems are suggested to try source builds that can be found at the same location. Run Yum Update The following command will invoke updates to the yum system, and also prepare the newly installed perfSONAR repository: sudo yum update Search for Tools Yum can be searched in the following manner: sudo yum search TOOLNAME Install Tools Yum can install tools in a similar fashion, the following command will install the client libraries: sudo yum install owamp-client bwctl-client ndt-client Note that some other tools may be pulled in automatically. Note that iperf and nuttcp are required for BWCTL to work. Tool Selection \u00b6 Debugging network problems involves running several tools, and gathering results both an end-to-end basis as well as to points in the middle. Initial tool selection can depend on a couple of factors: What servers are available on the other end, as well as in the middle What use case is attempting to be debugged How sophisticated is the user running the tools In general we recommend that users try \" all \" available tools, but in a structured and complete fashion before moving on to new tests. In general the following recommendation can be made regarding tool selection: Perform NDT client tests to the closest server possible. Additional tests to other points in the middle as needed. Perform end-to-end BWCTL tests to establish a baseline bandwidth. Perform a bisected BWCTL test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem) Perform end-to-end OWAMP tests to establish baseline latency and loss. Perform a bisected OWAMP test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem) The following are some examples of how to use the tools from the command line: NDT NDT uses a command line client called web100clt . There are many options available, but in general you must supply a server name, and some debugging flags to get additional output. Here is a simple invocation: [user@host ~]$ web100clt -n ndt.chic.net.internet2.edu Testing network path for configuration and performance problems -- Using IPv6 address Checking for Middleboxes . . . . . . . . . . . . . . . . . . Done checking for firewalls . . . . . . . . . . . . . . . . . . . Done running 10s outbound test (client to server) . . . . . 92.16 Mb/s running 10s inbound test (server to client) . . . . . . 90.63 Mb/s The slowest link in the end-to-end path is a 100 Mbps Full duplex Fast Ethernet subnet Information: Other network traffic is congesting the link Information [S2C]: Packet queuing detected: 15.06% (local buffers) Server &#39;ndt.chic.net.internet2.edu&#39; is not behind a firewall. [Connection to the ephemeral port was successful] Client is probably behind a firewall. [Connection to the ephemeral port failed] Information: Network Middlebox is modifying MSS variable (changed to 1440) Server IP addresses are preserved End-to-End Client IP addresses are preserved End-to-End To get additional data, try adding the -ll flag, it will produce a more in depth analysis. NDT is useful as the first step of debugging to gather information about the end host, as well as the basic network configuration. BWCTL BWCTL is invoked from the command line with a number of options. Of these the following are important: - - -f - Sets the format, supply either a byte based metric (K, M, G) or a bit based metric (k, m, g). - \u2013t \u2013 Sets the length of the test in seconds - \u2013i \u2013 Specifies the reporting interval (e.g. how often instantaneous bandwidth results are available) in seconds - \u2013c \u2013 Specifics the host that will receive the flow of data, e.g. the \"catcher\" - \u2013s \u2013 Specifics the host that will send the flow of data, e.g. the \"sender\" An example of invoking BWCTL can be seen below. In this example we are sending data from the host we are on to another located in Kansas City MO, on the Internet2 network: [user@host ~]$ bwctl -f m -t 10 -i 1 -c nms-rthr.kans.net.internet2.edu bwctl: Using tool: iperf bwctl: 93 seconds until test results available RECEIVER START bwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.210 -s -f m -m -p 5011 -t 10 -i 1 bwctl: start\\_tool: 3568979157.239050 ------------------------------------------------------------ Server listening on TCP port 5011 Binding to local address 64.57.16.210 TCP window size: 0.08 MByte (default) ------------------------------------------------------------ [14] local 64.57.16.210 port 5011 connected with 64.57.17.18 port 5011 [14] 0.0- 1.0 sec 105 MBytes 879 Mbits/sec [14] 1.0- 2.0 sec 118 MBytes 990 Mbits/sec [14] 2.0- 3.0 sec 118 MBytes 990 Mbits/sec [14] 3.0- 4.0 sec 118 MBytes 990 Mbits/sec [14] 4.0- 5.0 sec 118 MBytes 990 Mbits/sec [14] 5.0- 6.0 sec 118 MBytes 990 Mbits/sec [14] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec [14] 7.0- 8.0 sec 118 MBytes 990 Mbits/sec [14] 8.0- 9.0 sec 118 MBytes 990 Mbits/sec [14] 9.0-10.0 sec 118 MBytes 990 Mbits/sec [14] 0.0-10.1 sec 1178 MBytes 979 Mbits/sec [14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface) bwctl: stop\\_exec: 3568979172.016198 RECEIVER END This test reveals that over the course of 10 seconds we achieved an average bandwidth of 979Mbps and sent a total of 1178MB of data. We can reverse the direction of this test in the next example: [user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu bwctl: Using tool: iperf bwctl: 75 seconds until test results available RECEIVER START bwctl: exec\\_line: /usr/bin/iperf -B 64.57.17.18 -s -f m -m -p 5011 -t 10 -i 1 bwctl: start\\_tool: 3568979241.960327 ------------------------------------------------------------ Server listening on TCP port 5011 Binding to local address 64.57.17.18 TCP window size: 16.0 MByte (default) ------------------------------------------------------------ [14] local 64.57.17.18 port 5011 connected with 64.57.16.210 port 5011 [ID] Interval Transfer Bandwidth [14] 0.0- 1.0 sec 111 MBytes 929 Mbits/sec [14] 1.0- 2.0 sec 118 MBytes 990 Mbits/sec [14] 2.0- 3.0 sec 118 MBytes 990 Mbits/sec [14] 3.0- 4.0 sec 118 MBytes 990 Mbits/sec [14] 4.0- 5.0 sec 118 MBytes 990 Mbits/sec [14] 5.0- 6.0 sec 118 MBytes 990 Mbits/sec [14] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec [14] 7.0- 8.0 sec 118 MBytes 990 Mbits/sec [14] 8.0- 9.0 sec 118 MBytes 990 Mbits/sec [14] 9.0-10.0 sec 118 MBytes 990 Mbits/sec [14] 0.0-10.2 sec 1193 MBytes 984 Mbits/sec [14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface) bwctl: stop\\_exec: 3568979256.889493 RECEIVER END A similar result is seen in that we achieve near 1Gbps bandwidth (e.g. the hosts are only connected at 1Gbps). BWCTL can (and should) be used to check available bandwidth between servers. Start first on the long path (e.g. end-to-end) then test to resources in the middle. Note that BWCTL supports a 3 mode operation, wherein you can provide options for both the '-c' and '-s' and perform tests between these two hosts without being physically logged into either: [user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu -c nms-rthr1.hous.net.internet2.edu bwctl: Using tool: iperf bwctl: 82 seconds until test results available RECEIVER START bwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.130 -s -f m -m -p 5001 -t 10 -i 1 bwctl: start\\_tool: 3568979772.344387 ------------------------------------------------------------ Server listening on TCP port 5001 Binding to local address 64.57.16.130 TCP window size: 0.08 MByte (default) ------------------------------------------------------------ [14] local 64.57.16.130 port 5001 connected with 64.57.16.210 port 5001 [ID] Interval Transfer Bandwidth [14] 0.0- 1.0 sec 103 MBytes 861 Mbits/sec [14] 1.0- 2.0 sec 118 MBytes 990 Mbits/sec [14] 2.0- 3.0 sec 118 MBytes 990 Mbits/sec [14] 3.0- 4.0 sec 118 MBytes 990 Mbits/sec [14] 4.0- 5.0 sec 118 MBytes 990 Mbits/sec [14] 5.0- 6.0 sec 118 MBytes 990 Mbits/sec [14] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec [14] 7.0- 8.0 sec 118 MBytes 990 Mbits/sec [14] 8.0- 9.0 sec 118 MBytes 990 Mbits/sec [14] 9.0-10.0 sec 118 MBytes 990 Mbits/sec [14] 0.0-10.2 sec 1183 MBytes 977 Mbits/sec [14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface) bwctl: stop\\_exec: 3568979785.230833 RECEIVER END BWCTL requires a stable NTP clock to work properly, be sure that NTP is configured before using this tool. OWAMP OWAMP is a tool that measures latency, loss, out of orderness, and duplication of packets between a source and a destination. Note that this test measures each direction independently versus that of the traditional round trip tool ping . Below is an example of a test: [user@host ~]$ owping owamp.wash.net.internet2.edu Approximately 12.6 seconds until results available --- owping statistics from [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:60455 to [owamp.wash.net.internet2.edu]:47148 --- SID: 00160034d4ba4cad4e8c0485546b4ebf first: 2013-02-04T15:05:18.240 last: 2013-02-04T15:05:27.254 100 sent, 0 lost (0.000%), 0 duplicates one-way delay min/median/max = 2.02/2.1/2.06 ms, (err=0.218 ms) one-way jitter = 0 ms (P95-P50) Hops = 2 (consistently) no reordering --- owping statistics from [owamp.wash.net.internet2.edu]:47149 to [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:33562 --- SID: 00170098d4ba4cad8eb45282697d2cc2 first: 2013-02-04T15:05:18.259 last: 2013-02-04T15:05:27.175 100 sent, 0 lost (0.000%), 0 duplicates one-way delay min/median/max = 3.19/3.3/3.27 ms, (err=0.218 ms) one-way jitter = 0 ms (P95-P50) Hops = 2 (consistently) no reordering The results clearly state each direction of operation, and any problems that were found. As in the BWCTL case the tool is highly reliant on stable NTP numbers, so be sure your server is synchronized against an NTP server. OWAMP is a lightweight test and can be used to show minor amounts of packet loss between hosts. Perform the test on the full end-to-end path, and then bisect the path by testing to points in the middle. Often low throughput observed via BWTL will show up as packet loss in OWAMP. End-to-end Testing \u00b6 The concept of end-to-end testing is required as a first step in debugging network problems. Via the OSG tools it is possible to use \"client\" tools as discussed in Section 7.2 to gauge the total end-to-end path. These client tools can be pointed at a pS Performance Toolkit instance installed on the remote end, or via stand-alone daemon applications started on OSG systems. In either case, a daemon and client will be needed. The following procedure should be followed: Notify local networking staff at each end that are noticing problems, and would like to investigate them. Note that you can run tests end-to-end, and share them when you are complete. Identify Servers on both ends (e.g. standalone pS Performance Toolkit instances or starting daemons on OSG servers) Identify clients on both ends, normally the compute or storage nodes. Avoid using machines that are not involved in the OSG software process such as laptops. Perform end-to-end testing with: NDT BWCTL OWAMP Perform several tests and always record the results. It's a good idea to run at different times during the day, and note when you ran the tests Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate. After end-to-end testing, and examining the results with local and GOC based professionals, it may be time to embark on a larger debugging exercise with partial path decomposition. Partial Path Decomposition \u00b6 As we saw in Section 7.4, it is necessary to use all tools in a structured and scripted manner. Deciding to divide the path is no different. The following steps should be followed: Using a tool like traceroute or tracepath, find the paths between you and the remote host. If possible validate the path for the reverse direction as well. It may be possible that these are different. For one of the networks on the path, usually one in the direct middle (often a backbone network like Internet2, ESnet, or NLR), find a testing host. If these are not posted on public we pages, send an email to the support teams (e.g. rs@internet2.edu [BROKEN-LINK: mailto:rs@internet2.edu], or trouble@es.net [BROKEN-LINK: mailto:trouble@es.net]). Perform end-to-middle testing from the source and desgination with: NDT BWCTL OWAMP Perform several tests and always record the results. It's a good idea to run at different times during the day, and note when you ran the tests Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate. If the tests show one 'side' as being better than the other, you can repeat this process by further bisecting the path on the side with the problem. Interpreting Results \u00b6 Interpretation of results can be tricky due to the nature of protocols on the network, including TCP. In general the only symptom that is given off to a problem with TCP is \"low throughput\". The following are some tips on interpreting results: NDT will denote if your host does not have the proper amount of tuning. If it doesn't, please considering following the host tuning steps discussed in Section 5.1 NDT will give the first indication of network problems as well, and may indicate the presence of packet loss, link bottlenecks, or congestion. Since NDT is based on heuristics these results can turn out to be false positives, but are often worthy of following up on. In the event of congestion, ask local networking staff to see if there are any heavily utilized links. If packet loss is an issue, ask to see if any interface errors or discards are present. BWCTL, when used in TCP mode, is only useful at nothing high or low throughput. This is normally good from a capability standpoint, but it cannot tell you anything else about a serious problem OWAMP is useful for detecting loss. The theory is that if you notice the loss of small UDP packets produced by OWAMP, the same behavior will be seen in the form of low throughput from a tool like BWCTL. OWAMP can also be used to show asymmetric routing (with the aide of tools like traceroute) or if queuing and congestion are becoming a factor in one direction vs. another. Bisecting a path, and being patient, are normally the only ways to narrow down problems. In addition to these adages, please consider asking your local staff for assistance when you first notice a problem. If they are unable to help, consult the resources listed in Section 8. Additional Help \u00b6 The following locations can be consulted for more help in debugging network problems: Internet2 Research Support Center \u2013 rs@internet2.edu [BROKEN-LINK: mailto:rs@internet2.edu] ESnet Trouble Reporting \u2013 trouble@es.net [BROKEN-LINK: mailto:trouble@es.net] NLR NOC - noc@nlr.net [BROKEN-LINK: mailto:noc@nlr.net] OSG GOC - goc@opensciencegrid.org [BROKEN-LINK: mailto:goc@opensciencegrid.org] Acknowledgements \u00b6 The authors would like to acknowledge and thank the OSG community for their support and feedback into network performance problems and tools that would be useful for end users. The perfSONAR-PS community has been invaluable, and the authors would like to thank them for their generous contributions of software, expertise, and time. References \u00b6 TBD","title":"OSG Network Debugging Document"},{"location":"network-troubleshooting/osg-debugging-document/#osg-debugging-documentation","text":"Edited By: J. Zurawski \u2013 Internet2, S. McKee \u2013 University of Michigan February 4th 2013 Note ``` This document is old but still may have useful information. Many tools it references may no longer be supported or available. ```","title":"OSG Debugging Documentation"},{"location":"network-troubleshooting/osg-debugging-document/#abstract","text":"Scientific progress relies on intricate systems of computers and the networks that connect them. Often it is the case that scientific data is gathered via a well defined process, information is digitized, stored, transmitted, and processed by members of large and distributed collaborations. The Open Science Grid advances science through the concept of distributed computing \u2013 the process for sharing resources through a unified software framework focused on the common tasks of data movement, processing, and analysis. Networks are an integral part of the distributed computing process. Similar to the computational and storage resources, it is crucial that all networking components, on the complete end-to-end path, are functional and free of physical and logical flaws. A rich set of measurement and monitoring tools exists to provide network operations staff and end users a window into the functionality of networks, despite the fact that these actors do not have direct control over the complete path their data may travel. This document discusses common measurement and monitoring tools available to the OSG community, and strategies to deploy, use, and interpret the results they produce. The end goal is to give end users more insight into network behavior, and assist local and remote networking staffs as they correct damaging performance problems that will impact the scientific process.","title":"Abstract"},{"location":"network-troubleshooting/osg-debugging-document/#introduction","text":"The process of science is often complicated when viewed as a complete system. At the core of any project, there is a mechanism to observe or simulate some system, and produce meaningful results that will be interpreted and scrutinized between experimentation runs. The machinery that surrounds this process can be as benign as simple cameras, or as complex as the Large Hadron Collider and its associated experiments. Other common components include ways to digitize, store, process, and share the end result of experimentation \u2013 often done using computational systems. Computation falls into 3 broad classifications, all of which are required to implement the paradigm of scientific computing: Storage \u2013 Readable and writable physical medium used for temporary or long term residency of gathered data. Processing \u2013 Specially designed hardware and software that iterates over collected data sets looking for pre-defined triggers and results. Networking \u2013 Interconnecting hardware and software used to facilitate communication between storage and processing components both on a local, and fully distributed basis. When fully realized, even a small facility can contribute a great amount of resources to the overall goal of scientific advancement. In practice it may be the case that a lab consisting of a single researcher can pull data sets from a centralized location, perform carefully selected segments of an entire set of analysis that is required, and return any relevant results as they are discovered. When used in an inductive fashion, one can imagine the overall throughout that a VO such as the LHC project is able to attain through 100s of distributed facilities and 1000s of collaborating researchers. Complexity is present as we travel down the individual technology items in the above scenario. Often it is the case that ideal performance is hard to attain due to the intertwined nature of the mechanisms involved. For example, data must be written and read from physical medium. Often this step is slower due to the mechanical nature of the process, and struggles to keep up with faster technologies such as processing or transmission on network infrastructure. Equally, it may be the case that a flaw in the network infrastructure, such as a failing component, can introduce data loss that must be compensated for through retransmission. Retransmission implies additional work for storage and processing components that must waste resources to overcome a fixable, but often unnoticed, problem. Network performance monitoring is a relatively unseen, but still extremely necessary, practice. This statement is true due to the nature of network use through application software and communication protocols. Application developers wish to unburden the user with details about \"how\" data may be moved between facilities. Care is taken to design applications in such a way that the user is simply presented with options related to a source and destination only, and little or no insight into the path taken or the current conditions that may be present. The aforementioned situation where a failing component institutes data loss results in only one symptom to the end user: lower than expected throughput. Many users may not notice, or have become complacent, with low performance situations. Some may write this off as \"the network is slow\", or perhaps will not notice at all due to experiences with home connections that are often 2 orders of magnitude slower than what is possible in a typical academic environment. Software exists to monitor network performance in many different ways. For example it is possible take a measure of network throughput, and simulate the behavior of a file transfer application. It is also possible to observe network stability (e.g. jitter) over time to simulate video or audio transmission. These basic observations are powerful when used both in a local environment, as well as on an end-to-end basis. In either case, software must be deployed and available to the community on points of interest: specifically on the local and shared network infrastructure distributed around the world. perfSONAR is a software framework that simplifies network debugging activities by making it easier to deploy measurement tools, and facilities the sharing of results. It is currently deployed on many communal networking resources in the R&E community, including backbones, regional networks, campuses, and individual laboratories. Once perfSONAR is deployed, it becomes possible to troubleshoot situations that result in low throughout for the end user in a straightforward manner. It is important to note that when something like this occurs, it is not the sole responsibility of the end user to debug and solve a networking problem; rather it is their duty to report the problem and provide as much information as possible to local or remote network staffs so they may learn about the issue, and interpret the results so as to lead to quick and efficient problem resolution. Locating this staff may be challenging, but many organizations maintain a dedicated Network Operations Center (NOC) whose staff are ready to accept trouble reports and act in an appropriate manner. Section 8 details locations you may turn to for additional support. This document will expand upon these topics in the remaining sections, and conclude with information where additional resources beyond a simple introduction to these topics can be found.","title":"Introduction"},{"location":"network-troubleshooting/osg-debugging-document/#the-scientific-networking-process","text":"There is a rich ecosystem of components available for monitoring and managing the scientific networking process. This myriad of hardware and software must work together to complete the overall goal of interpreting gathered or simulated observations. Each component we will discuss has the ability to be installed, operated, and maintained in different ways. Individual brands or versions are not important, but the overall idea of each will be explored.","title":"The Scientific Networking Process"},{"location":"network-troubleshooting/osg-debugging-document/#hosts","text":"Server or \"host\" hardware and software can be used in many different ways. Often it is the case that these components serve as computational resources for processing data, or provide access to underlying data stored on physical media. It may also be the case that software designed to \"glue\" components of a framework together (e.g. processing mail, authenticating users, providing mappings between names and addresses) is installed on a dedicated or shared host resource. Hosts must contain an operating system: software designed to control and maintain the underlying hardware such as storage media, network interface cards, processors, and other peripheral devices. Operating system hardware can vary in functionality; completely interactive systems such as those found on laptops can be more pleasing for a human user vs. that of a no frills batch processing system designed to only iterate over scientific data. The choice of operating system will vary from use case to use case. The footing provided by the hardware and operating system serves as the basis for the remainder of the components in this discussion.","title":"Hosts"},{"location":"network-troubleshooting/osg-debugging-document/#protocols","text":"Protocols are software algorithms implemented on hosts and networking components, and are used to facilitate communication strategies. Protocols are constructed in a \"layered\" fashion, and are designed to handle certain aspects of the overall communication plan. For instance a protocol may be used to communicate between two network devices, and may involve the use of different patterns of electrical or optical signal. On top of this basic system of signals we may construct a different protocol that is focused on communication between end hosts, and is able to break up the notion of a user's files into small chunks so they can be sent reliably end-to-end. Protocols evolve with the underlying technology, and often can be tuned to specific use cases. One such protocol, Transmission Control Protocol (TCP), is widely used in applications that many users are familiar with such as web browsing, mail transfer, and file exchange. Early incarnations of TCP were designed for the networks of the 1980s; slower, less reliable, constructions than what is present in the networks of the 2010s. As such TCP must be instructed, via configuration on a host or network devices operating environment, that it should behave in a different and more efficient manner. With the protocol in place, we can now begin to discuss applications that are able to use the network to communicate in an automated fashion.","title":"Protocols"},{"location":"network-troubleshooting/osg-debugging-document/#applications","text":"Applications are specific use cases, programmed as software, and made available to end users via a host's operating environment. There are numerous applications we use on a daily basis \u2013 web browsers to fetch remote content, word processing applications to type papers, mail and instant messaging clients to exchange information in near real time. Scientific applications normally focus on performing a single task (e.g. end-to-end data movement, visualization, data transformation, data analysis) on either a local or distributed basis. In the case of distribution, it becomes necessary to interact with the underlying network through the use of a protocol. File transfer is a specialized application that takes is interested in either sending a local file to a remote location, or retrieving remote data to bring locally. In either case the application must broker with a protocol, such as TCP, that is available on both ends of the transfer. Through a series of API calls information is segmented into transmission chunks and sent reliably though the communication medium. Most of this is handled transparently from the user's perspective, and as such they are not given much in the way of feedback beyond a pass or failure, and some notion of how long it took. Understanding more about the network can be enlightening exercise for users who are unaware of the complexity and span of components that are required for operation. This will be discussed in the remaining sections.","title":"Applications"},{"location":"network-troubleshooting/osg-debugging-document/#lab-local-area-network","text":"The first step in the networking tree is often the interconnections between components local to the user. This may consist of the storage and processing nodes in a single rack or data center used for scientific processing, connected via technology consistent with the tasks they are performing. Cluster nodes may use a high speed interconnect such as InfiniBand; servers may also just use typical Ethernet at 1Gbps or 10Gbps. In either case, there will be dedicated network equipment with the task of aggregating and controlling traffic flow to the local devices, and serving as an uplink to the next network in the chain (the campus). Monitoring and management of this local environment is a good idea, either through passive means such as using the SNMP system, or active tests that check the health of transfers on a local basis.","title":"Lab Local Area Network"},{"location":"network-troubleshooting/osg-debugging-document/#campus-local-area-network","text":"The first hop beyond a laboratory environment is a network maintained by campus/local support staff. It is often the case that this group is maintaining the infrastructure for the use of all end users, and as such will design and maintain things to preserve uniform functionality and performance. Campus networks are an even larger ecosystem of devices given the area they may cover. It may be the case that the network in the previous section is behind several devices before it has a clean path to the outside world. It may also be the case that traffic aggregation is extremely high, and congestion becomes a factor during certain parts of the day or times of the year. These nuances make local performance monitoring crucial for operational soundness. This group is also the first contact that should be exercised in the event of a network performance problem. While they may not be able to answer for the status of the entire path, they can escalate the problem to the regional or backbone support staff as needed.","title":"Campus Local Area Network"},{"location":"network-troubleshooting/osg-debugging-document/#regional-network","text":"A regional network provider aggregates the networks of numerous campuses in a state, country, or pre-defined region. Examples include provider for states of the US (e.g. KanREN), countries (SWITCH, the network of Switzerland), or collaboration between parties without a political boundary (the SOX regional network in the United States). Regional networks may cover a large geographical area, but often have less equipment than a campus. The role of a regional aggregator is to take connections (large and small) and condense them into long-haul links that will uplink to a backbone or exchange point as a next step. Regional operations teams have similar performance concerns to that of a campus network. The aggregation point of several networks can be a critical component, and one of the more likely places that a failing piece of equipment or congestion can impact downstream network users. Monitoring local and remote components (e.g. maintaining active testing between networks) critical. Regional support teams can be likely candidates for assistance on performance problems, but users are reminded to discuss options with their local staff first before engaging with these groups.","title":"Regional Network"},{"location":"network-troubleshooting/osg-debugging-document/#re-backbone","text":"An R&E backbone consists of an aggregation of numerous regional providers. Capacity must reflect the number and expectations of this group of customers, and often is orders of magnitude higher than other networks that are downstream. As an aggregation point, normally spread over a very large geographical area, traffic flows will be numerous, of mixtures of size and duration, and be destined for diverse destinations domestically and internationally. Each Point of Presence (PoP) could have a large number of customers integrating, and thus increases the chance of an issue local to this device. As a service to customers, the R&E backbone should consider making test instances available to help bisect and debug challenging problems that may cross the domain. Backbone support teams are also well trained and have knowledge of performance monitoring. Some providers such as ESnet and Internet2 have dedicated staffs just to support the troubleshooting of network problems for customers. An end user is encouraged to seek out these resources, as well as those that are local, when debugging a problem impacting scientific work.","title":"R&amp;E Backbone"},{"location":"network-troubleshooting/osg-debugging-document/#exchange-point","text":"An exchange point is normally a location where multiple backbone networks and international transit links (e.g. trans-oceanic links) combine and transit to other domains. An exchange point is a special case of an aggregation network like a regional in that policies may be different depending on the membership structure. International exchange points suffer from the aforementioned problems of traffic aggregation wherein congestion or equipment failure will have a severe impact on all traffic. Monitoring these devices is crucial, as in other use cases.","title":"Exchange Point"},{"location":"network-troubleshooting/osg-debugging-document/#actor-agent-definitions","text":"There are many actors involved in the process of network management and debugging. We will highlight three here, as they represent the most critical members of the support team that OSG has available when problems are discovered.","title":"Actor &amp; Agent Definitions"},{"location":"network-troubleshooting/osg-debugging-document/#end-user","text":"The end user is understood to be the primary user and beneficiary of OSG software to process and operate on scientific data sets. The sophistication of this end user is assumed to be beginner to average in matters related to system and network administration. In general we assume they are knowledgeable enough to install and maintain OSG software, and connect devices to the networking infrastructure. This actor is assumed to be the primary user of the perfSONAR end user tools, packaged in the OSG VDT distribution. These tools are meant to be run from a system to upstream test machines provided by the campus, regional, or backbone network.","title":"End User"},{"location":"network-troubleshooting/osg-debugging-document/#local-administrator","text":"The local administrator can be campus support staff responsible for the health of servers or network devices across the greater campus ecosystem. Their primary responsibility it to ensure uptime of the network for all users, as well as assist in debugging specific problems caused by performance impacting problems on a local basis. This actor may not be able to directly address problems on a regional, national, or international basis but can serve as a liaison with individuals within those stewardship organizations.","title":"Local Administrator"},{"location":"network-troubleshooting/osg-debugging-document/#remote-administrator","text":"A remote administrator can be regional, backbone, or exchange point staff responsible for the health of remote networking resources. It is often the case that these individuals may not be aware of a specific use case between remote campuses, but could answer questions about the current health and status of the network they control. These individuals are assumed to be knowledgeable about performance tools, and can work with local staff as needed to make test points available to assist in debugging.","title":"Remote Administrator"},{"location":"network-troubleshooting/osg-debugging-document/#local-preparations","text":"A first step to any OSG software installation to support scientific activity is preparation of the local environment. Given the considerations denoted in the previous sections, we will discuss 3 specific preparation activities: End System Operating System and Protocol Tuning Network Architecture Adjustments Network Configuration Tuning Each of these steps is considered to be most relevant to the laboratory local environment, although some should be considered for the campus as well. It is assumed that the end user actor, with the help of local administrators, can make these changes.","title":"Local Preparations"},{"location":"network-troubleshooting/osg-debugging-document/#end-system-tuning","text":"Computer systems are similar to automobiles in that its possible to \"tune\" certain internal aspects and achieve higher performance when using the network. The operating system and associated protocols like TCP make these changes rather simple to implement. In general there are several options worth considering: Network interface cards have an adjustable size for their packet queues Kernel buffers can be increased to support long distance transfers The TCP congestion control algorithm can be changed ESnet has made a web based resource available to assist with the task of host tuning, it can be found here: http://fasterdata.es.net/host-tuning/","title":"End System Tuning"},{"location":"network-troubleshooting/osg-debugging-document/#network-architecture","text":"Architectural decisions are often involved and will involve the input of local support staff. In general laboratory architecture should be robust in the following manner: Multiple uplinks to the campus network to provide capacity and resiliency A limited amount of \"fan in\" (e.g. number of connections) into a given access switch. It is recommended that as the fan in grows, multiple switches be employed to manage connectivity and congestion Elimination of firewalls from the path. Security can be implemented by host-based firewalls that restrict ports as well as Access Control Lists (ACLs) to white list sites that are communicated with. Firewall devices have been known to severely impact traffic for bandwidth intensive applications. Choice of device manufacturer that allows for out of band management and monitoring (e.g. SNMP) of devices Choice of device manufacturer that allows for per-interface tuning of memory buffers (vs. that of a shared memory region) Network architectural considerations are far too broad to be represented in a single document for the OSG, and the interested reader is encouraged to read the following resource provided by ESnet: http://fasterdata.es.net/science-dmz","title":"Network Architecture"},{"location":"network-troubleshooting/osg-debugging-document/#network-configuration-tuning","text":"Much like end hosts, network devices have the ability to be \"tuned\" for specific use cases. This tuning normally centers on enabling or disabling certain features on a router or switch (e.g. policy maps) or adjusting the available memory available to account for a specific use case (e.g. less memory for a video application, more for a throughput intensive tool). As every manufacturer provides different interfaces to the underlying hardware, we cannot make specific recommendations in this document. The interested reader is encouraged to read this guide provided by ESnet: http://fasterdata.es.net/network-tuning/router-tuning/","title":"Network Configuration Tuning"},{"location":"network-troubleshooting/osg-debugging-document/#measurement-software","text":"The available span of network measurement software is vast. A simple web search will reveal 10s of names, some still active and others long dead. The R&E community began to standardize on available tools in the later part of the 2000s with an effort to unify measurement tools and infrastructure to support them: perfSONAR. perfSONAR is a framework to simplify end to end network debugging. It consists of a layer of middleware, designed to sit between the measurement tools and the visualization and analysis that is useful to human users. A key component of the perfSONAR concept is the pS Performance Toolkit; this completely enclosed operating system packages performance tools and easy to follow GUIs to enable configuration. perfSONAR focuses on several key metrics: Achievable Bandwidth One Way Latency Round Trip Latency Packet Loss, Duplication, Out of Orderness Interface Utilization, Errors, Discards Layer 3 Path Path MTU Many of these metrics are calculated through simple tests that can be run from the command line. The OSG VDT package contains 3 key measurement tools that will be used as we discuss networking debugging in Section 7: BWCTL \u2013 A tool for measuring end to end bandwidth availability NDT \u2013 A tool designed to diagnose several different aspects of a host and network OWAMP \u2013 A tool designed for measuring one way delays of packets, as well as loss, duplication, and out of orderness. These 3 command line tools, when installed on a compute or storage node, can be used to launch tests to perfSONAR servers located in the greater R&E networking world, e.g. on the campus, regional, backbone, or exchange point networks.","title":"Measurement Software"},{"location":"network-troubleshooting/osg-debugging-document/#debugging-process","text":"The following sections will discuss the process to install, use, and interpret measurement tools in an OSG software environment. End users are encouraged to try these steps first, but also contact their local support staff at the earliest possible moment. Involving support staff will ensure that expert eyes are available to assist with problems, and funnel the requests for help to the proper area (e.g. GOC, other networks, etc.).","title":"Debugging Process"},{"location":"network-troubleshooting/osg-debugging-document/#software-installation","text":"Client software can be installed in one of two ways, either though the OSG VDT or via RPMs from the perfSONAR web site.","title":"Software Installation"},{"location":"network-troubleshooting/osg-debugging-document/#osg-software","text":"[INSERT INSTRUCTIONS ON HOW TO INSTALL VDT HERE]","title":"OSG Software"},{"location":"network-troubleshooting/osg-debugging-document/#perfsonar-ps-software","text":"All perfSONAR software is available through an RPM (Red Hat Package Manager) repository to make for easy installation and updates. The following steps can be taken to install these tools: Import the Internet2 Signing Key The following command will import the key. sudo rpm --import http://software.internet2.edu/rpms/RPM-GPG-KEY-Internet2 Download RPM Software The latest version for CentOS 5 and 6 (both x86 and x86_64 architectures) can be found on the the following web site: http://software.internet2.edu Note that SL5 andSL6, RHEL 5 and RHEL 6 should work with these builds. Those using other operating systems are suggested to try source builds that can be found at the same location. Run Yum Update The following command will invoke updates to the yum system, and also prepare the newly installed perfSONAR repository: sudo yum update Search for Tools Yum can be searched in the following manner: sudo yum search TOOLNAME Install Tools Yum can install tools in a similar fashion, the following command will install the client libraries: sudo yum install owamp-client bwctl-client ndt-client Note that some other tools may be pulled in automatically. Note that iperf and nuttcp are required for BWCTL to work.","title":"perfSONAR-PS Software"},{"location":"network-troubleshooting/osg-debugging-document/#tool-selection","text":"Debugging network problems involves running several tools, and gathering results both an end-to-end basis as well as to points in the middle. Initial tool selection can depend on a couple of factors: What servers are available on the other end, as well as in the middle What use case is attempting to be debugged How sophisticated is the user running the tools In general we recommend that users try \" all \" available tools, but in a structured and complete fashion before moving on to new tests. In general the following recommendation can be made regarding tool selection: Perform NDT client tests to the closest server possible. Additional tests to other points in the middle as needed. Perform end-to-end BWCTL tests to establish a baseline bandwidth. Perform a bisected BWCTL test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem) Perform end-to-end OWAMP tests to establish baseline latency and loss. Perform a bisected OWAMP test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem) The following are some examples of how to use the tools from the command line: NDT NDT uses a command line client called web100clt . There are many options available, but in general you must supply a server name, and some debugging flags to get additional output. Here is a simple invocation: [user@host ~]$ web100clt -n ndt.chic.net.internet2.edu Testing network path for configuration and performance problems -- Using IPv6 address Checking for Middleboxes . . . . . . . . . . . . . . . . . . Done checking for firewalls . . . . . . . . . . . . . . . . . . . Done running 10s outbound test (client to server) . . . . . 92.16 Mb/s running 10s inbound test (server to client) . . . . . . 90.63 Mb/s The slowest link in the end-to-end path is a 100 Mbps Full duplex Fast Ethernet subnet Information: Other network traffic is congesting the link Information [S2C]: Packet queuing detected: 15.06% (local buffers) Server &#39;ndt.chic.net.internet2.edu&#39; is not behind a firewall. [Connection to the ephemeral port was successful] Client is probably behind a firewall. [Connection to the ephemeral port failed] Information: Network Middlebox is modifying MSS variable (changed to 1440) Server IP addresses are preserved End-to-End Client IP addresses are preserved End-to-End To get additional data, try adding the -ll flag, it will produce a more in depth analysis. NDT is useful as the first step of debugging to gather information about the end host, as well as the basic network configuration. BWCTL BWCTL is invoked from the command line with a number of options. Of these the following are important: - - -f - Sets the format, supply either a byte based metric (K, M, G) or a bit based metric (k, m, g). - \u2013t \u2013 Sets the length of the test in seconds - \u2013i \u2013 Specifies the reporting interval (e.g. how often instantaneous bandwidth results are available) in seconds - \u2013c \u2013 Specifics the host that will receive the flow of data, e.g. the \"catcher\" - \u2013s \u2013 Specifics the host that will send the flow of data, e.g. the \"sender\" An example of invoking BWCTL can be seen below. In this example we are sending data from the host we are on to another located in Kansas City MO, on the Internet2 network: [user@host ~]$ bwctl -f m -t 10 -i 1 -c nms-rthr.kans.net.internet2.edu bwctl: Using tool: iperf bwctl: 93 seconds until test results available RECEIVER START bwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.210 -s -f m -m -p 5011 -t 10 -i 1 bwctl: start\\_tool: 3568979157.239050 ------------------------------------------------------------ Server listening on TCP port 5011 Binding to local address 64.57.16.210 TCP window size: 0.08 MByte (default) ------------------------------------------------------------ [14] local 64.57.16.210 port 5011 connected with 64.57.17.18 port 5011 [14] 0.0- 1.0 sec 105 MBytes 879 Mbits/sec [14] 1.0- 2.0 sec 118 MBytes 990 Mbits/sec [14] 2.0- 3.0 sec 118 MBytes 990 Mbits/sec [14] 3.0- 4.0 sec 118 MBytes 990 Mbits/sec [14] 4.0- 5.0 sec 118 MBytes 990 Mbits/sec [14] 5.0- 6.0 sec 118 MBytes 990 Mbits/sec [14] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec [14] 7.0- 8.0 sec 118 MBytes 990 Mbits/sec [14] 8.0- 9.0 sec 118 MBytes 990 Mbits/sec [14] 9.0-10.0 sec 118 MBytes 990 Mbits/sec [14] 0.0-10.1 sec 1178 MBytes 979 Mbits/sec [14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface) bwctl: stop\\_exec: 3568979172.016198 RECEIVER END This test reveals that over the course of 10 seconds we achieved an average bandwidth of 979Mbps and sent a total of 1178MB of data. We can reverse the direction of this test in the next example: [user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu bwctl: Using tool: iperf bwctl: 75 seconds until test results available RECEIVER START bwctl: exec\\_line: /usr/bin/iperf -B 64.57.17.18 -s -f m -m -p 5011 -t 10 -i 1 bwctl: start\\_tool: 3568979241.960327 ------------------------------------------------------------ Server listening on TCP port 5011 Binding to local address 64.57.17.18 TCP window size: 16.0 MByte (default) ------------------------------------------------------------ [14] local 64.57.17.18 port 5011 connected with 64.57.16.210 port 5011 [ID] Interval Transfer Bandwidth [14] 0.0- 1.0 sec 111 MBytes 929 Mbits/sec [14] 1.0- 2.0 sec 118 MBytes 990 Mbits/sec [14] 2.0- 3.0 sec 118 MBytes 990 Mbits/sec [14] 3.0- 4.0 sec 118 MBytes 990 Mbits/sec [14] 4.0- 5.0 sec 118 MBytes 990 Mbits/sec [14] 5.0- 6.0 sec 118 MBytes 990 Mbits/sec [14] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec [14] 7.0- 8.0 sec 118 MBytes 990 Mbits/sec [14] 8.0- 9.0 sec 118 MBytes 990 Mbits/sec [14] 9.0-10.0 sec 118 MBytes 990 Mbits/sec [14] 0.0-10.2 sec 1193 MBytes 984 Mbits/sec [14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface) bwctl: stop\\_exec: 3568979256.889493 RECEIVER END A similar result is seen in that we achieve near 1Gbps bandwidth (e.g. the hosts are only connected at 1Gbps). BWCTL can (and should) be used to check available bandwidth between servers. Start first on the long path (e.g. end-to-end) then test to resources in the middle. Note that BWCTL supports a 3 mode operation, wherein you can provide options for both the '-c' and '-s' and perform tests between these two hosts without being physically logged into either: [user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu -c nms-rthr1.hous.net.internet2.edu bwctl: Using tool: iperf bwctl: 82 seconds until test results available RECEIVER START bwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.130 -s -f m -m -p 5001 -t 10 -i 1 bwctl: start\\_tool: 3568979772.344387 ------------------------------------------------------------ Server listening on TCP port 5001 Binding to local address 64.57.16.130 TCP window size: 0.08 MByte (default) ------------------------------------------------------------ [14] local 64.57.16.130 port 5001 connected with 64.57.16.210 port 5001 [ID] Interval Transfer Bandwidth [14] 0.0- 1.0 sec 103 MBytes 861 Mbits/sec [14] 1.0- 2.0 sec 118 MBytes 990 Mbits/sec [14] 2.0- 3.0 sec 118 MBytes 990 Mbits/sec [14] 3.0- 4.0 sec 118 MBytes 990 Mbits/sec [14] 4.0- 5.0 sec 118 MBytes 990 Mbits/sec [14] 5.0- 6.0 sec 118 MBytes 990 Mbits/sec [14] 6.0- 7.0 sec 118 MBytes 990 Mbits/sec [14] 7.0- 8.0 sec 118 MBytes 990 Mbits/sec [14] 8.0- 9.0 sec 118 MBytes 990 Mbits/sec [14] 9.0-10.0 sec 118 MBytes 990 Mbits/sec [14] 0.0-10.2 sec 1183 MBytes 977 Mbits/sec [14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface) bwctl: stop\\_exec: 3568979785.230833 RECEIVER END BWCTL requires a stable NTP clock to work properly, be sure that NTP is configured before using this tool. OWAMP OWAMP is a tool that measures latency, loss, out of orderness, and duplication of packets between a source and a destination. Note that this test measures each direction independently versus that of the traditional round trip tool ping . Below is an example of a test: [user@host ~]$ owping owamp.wash.net.internet2.edu Approximately 12.6 seconds until results available --- owping statistics from [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:60455 to [owamp.wash.net.internet2.edu]:47148 --- SID: 00160034d4ba4cad4e8c0485546b4ebf first: 2013-02-04T15:05:18.240 last: 2013-02-04T15:05:27.254 100 sent, 0 lost (0.000%), 0 duplicates one-way delay min/median/max = 2.02/2.1/2.06 ms, (err=0.218 ms) one-way jitter = 0 ms (P95-P50) Hops = 2 (consistently) no reordering --- owping statistics from [owamp.wash.net.internet2.edu]:47149 to [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:33562 --- SID: 00170098d4ba4cad8eb45282697d2cc2 first: 2013-02-04T15:05:18.259 last: 2013-02-04T15:05:27.175 100 sent, 0 lost (0.000%), 0 duplicates one-way delay min/median/max = 3.19/3.3/3.27 ms, (err=0.218 ms) one-way jitter = 0 ms (P95-P50) Hops = 2 (consistently) no reordering The results clearly state each direction of operation, and any problems that were found. As in the BWCTL case the tool is highly reliant on stable NTP numbers, so be sure your server is synchronized against an NTP server. OWAMP is a lightweight test and can be used to show minor amounts of packet loss between hosts. Perform the test on the full end-to-end path, and then bisect the path by testing to points in the middle. Often low throughput observed via BWTL will show up as packet loss in OWAMP.","title":"Tool Selection"},{"location":"network-troubleshooting/osg-debugging-document/#end-to-end-testing","text":"The concept of end-to-end testing is required as a first step in debugging network problems. Via the OSG tools it is possible to use \"client\" tools as discussed in Section 7.2 to gauge the total end-to-end path. These client tools can be pointed at a pS Performance Toolkit instance installed on the remote end, or via stand-alone daemon applications started on OSG systems. In either case, a daemon and client will be needed. The following procedure should be followed: Notify local networking staff at each end that are noticing problems, and would like to investigate them. Note that you can run tests end-to-end, and share them when you are complete. Identify Servers on both ends (e.g. standalone pS Performance Toolkit instances or starting daemons on OSG servers) Identify clients on both ends, normally the compute or storage nodes. Avoid using machines that are not involved in the OSG software process such as laptops. Perform end-to-end testing with: NDT BWCTL OWAMP Perform several tests and always record the results. It's a good idea to run at different times during the day, and note when you ran the tests Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate. After end-to-end testing, and examining the results with local and GOC based professionals, it may be time to embark on a larger debugging exercise with partial path decomposition.","title":"End-to-end Testing"},{"location":"network-troubleshooting/osg-debugging-document/#partial-path-decomposition","text":"As we saw in Section 7.4, it is necessary to use all tools in a structured and scripted manner. Deciding to divide the path is no different. The following steps should be followed: Using a tool like traceroute or tracepath, find the paths between you and the remote host. If possible validate the path for the reverse direction as well. It may be possible that these are different. For one of the networks on the path, usually one in the direct middle (often a backbone network like Internet2, ESnet, or NLR), find a testing host. If these are not posted on public we pages, send an email to the support teams (e.g. rs@internet2.edu [BROKEN-LINK: mailto:rs@internet2.edu], or trouble@es.net [BROKEN-LINK: mailto:trouble@es.net]). Perform end-to-middle testing from the source and desgination with: NDT BWCTL OWAMP Perform several tests and always record the results. It's a good idea to run at different times during the day, and note when you ran the tests Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate. If the tests show one 'side' as being better than the other, you can repeat this process by further bisecting the path on the side with the problem.","title":"Partial Path Decomposition"},{"location":"network-troubleshooting/osg-debugging-document/#interpreting-results","text":"Interpretation of results can be tricky due to the nature of protocols on the network, including TCP. In general the only symptom that is given off to a problem with TCP is \"low throughput\". The following are some tips on interpreting results: NDT will denote if your host does not have the proper amount of tuning. If it doesn't, please considering following the host tuning steps discussed in Section 5.1 NDT will give the first indication of network problems as well, and may indicate the presence of packet loss, link bottlenecks, or congestion. Since NDT is based on heuristics these results can turn out to be false positives, but are often worthy of following up on. In the event of congestion, ask local networking staff to see if there are any heavily utilized links. If packet loss is an issue, ask to see if any interface errors or discards are present. BWCTL, when used in TCP mode, is only useful at nothing high or low throughput. This is normally good from a capability standpoint, but it cannot tell you anything else about a serious problem OWAMP is useful for detecting loss. The theory is that if you notice the loss of small UDP packets produced by OWAMP, the same behavior will be seen in the form of low throughput from a tool like BWCTL. OWAMP can also be used to show asymmetric routing (with the aide of tools like traceroute) or if queuing and congestion are becoming a factor in one direction vs. another. Bisecting a path, and being patient, are normally the only ways to narrow down problems. In addition to these adages, please consider asking your local staff for assistance when you first notice a problem. If they are unable to help, consult the resources listed in Section 8.","title":"Interpreting Results"},{"location":"network-troubleshooting/osg-debugging-document/#additional-help","text":"The following locations can be consulted for more help in debugging network problems: Internet2 Research Support Center \u2013 rs@internet2.edu [BROKEN-LINK: mailto:rs@internet2.edu] ESnet Trouble Reporting \u2013 trouble@es.net [BROKEN-LINK: mailto:trouble@es.net] NLR NOC - noc@nlr.net [BROKEN-LINK: mailto:noc@nlr.net] OSG GOC - goc@opensciencegrid.org [BROKEN-LINK: mailto:goc@opensciencegrid.org]","title":"Additional Help"},{"location":"network-troubleshooting/osg-debugging-document/#acknowledgements","text":"The authors would like to acknowledge and thank the OSG community for their support and feedback into network performance problems and tools that would be useful for end users. The perfSONAR-PS community has been invaluable, and the authors would like to thank them for their generous contributions of software, expertise, and time.","title":"Acknowledgements"},{"location":"network-troubleshooting/osg-debugging-document/#references","text":"TBD","title":"References"},{"location":"perfsonar/deployment-models/","text":"perfSONAR Deployment Options \u00b6 The primary motivation for perfSONAR deployment is to test isolation, i.e. only one end-to-end test should run on a host at a time. This ensures that the test results are not impacted by the other tests. Otherwise it is much more difficult to interpret test results, which may vary due to host effects rather then network effects. Taking this into account it means that perfSONAR measurement tools are much more accurate running on a dedicated hardware and while it may be useful to run them on other hosts such as Data Transfer Nodes the current recommendation is to have specific measurement machine. In addition, as bandwidth testing could impact latency testing, we recommend to deploy two different nodes, each focused on specific set of tests. The following deployment options are currently available: Bare metal - preffered option in one of two possible configurations: * Two bare metal servers, one for latency node, one for bandwidth node * One bare metal server running both latency and bandwidth node together provided that there are two NICs available, please refer to [dual NIC](#multiple-nic-network-interface-card-guidance) section for more details on this. Virtual Machine - if bare metal is not available then it is also possible to run perfSONAR on a VM, however there are a set of additional requirements to fulfill: * Full-node VM is strongly preferred, having 2 VMs (latency/bandwidth node) on a single bare metal. Mixing perfSONAR VM(s) with others might have an impact on the measurements and is therefore not recommended. * VM needs to be configured to have SR-IOV to NIC(s) as well as pinned CPUs to ensure bandwidth tests are not impacted (by hypervisor switching CPUs during the test) * Succesfull full speed local bandwidth test is highly recommended prior to putting the VM into production Container - perfSONAR has supported containers from version 4.1 (Q1 2018) and is documented at https://docs.perfsonar.net/install_docker.html but is not typically used in the same way as a full toolkit installation. ``` * Docker perfSONAR test instance can however still be used by sites that run multiple perfSONAR instances on site for their internal testing as this deployment model allows to flexibly deploy a testpoint which can send results to a local measurement archive running on the perfSONAR toolkit node. ``` perfSONAR Toolkit vs Testpoint \u00b6 The perfSONAR team has documented the types of installations supported at https://docs.perfsonar.net/install_options.html . With the release of version 5, OSG/WLCG sites have a new option: instead of installing the full Toolkit sites can choose to install the Testpoint bundle. Pros * Simpler deployment when a local web interface is not needed and a central measurement archive is available. * Less resource intensive for both memory and I/O capacity. Cons ``` * Measurements are not stored locally * No web interface to use for configuration or adding local tests * Unable to show results in MaDDash ``` While sites are free to choose whatever deployment method they want, we would like to strongly recommend the use of perfSONAR's containerized testpoint. This method was chosen as a \"best practice\" recommendation because of the reduced resource constraints, less components and easier management. perfSONAR Hardware Requirements \u00b6 There are two different nodes participating in the network testing, latency node and bandwidth node, while both are running on the exact same perfSONAR toolkit, they have very different requirements. Bandwidth node measures available (or peak) throughput with low test frequency and will thus require NIC with high capacity (1/10/40/100G are supported) as well as enough memory and CPU to support high bandwidth testing. Our recommendation is to match bandwidth node NIC speed with the one installed on the storage nodes as this would provide us with the best match when there are issues to investigate. In case you'd like to deploy high speed (100G) bandwidth node, please consult ESNet tuning guide and 100G tuning presentation . Latency node on the other hand runs low bandwidth, but high frequency tests, sending a continuous stream of packets to measure delay and corresponding packet loss, packet reordering, etc. This means that while it doesn't require high capacity NIC, 1G is usually sufficient, it can impose significant load on the IO to disk as well as CPU as many tests run in parallel and need to continuously store its results into local measurement archive. The minimum hardware requirements to run perfSONAR toolkit are documented here . For WLCG/OSG deployment and taking into account the amount of testing that we perform, we recommend at least the following for perfSONAR 5.0+: NIC for bandwidth node matching the capacity of the site storage nodes(10/25/40/100G), 1G NIC for latency node (for higher NIC capacities, 40/100G, please check ESNet tuning guide ) High clock speede CPU (3.0 Ghz+), fwere cores OK, with at least 32GB+ of RAM (8GB+ if using a Testpoint install) NVMe or SSD disk (128GB should be sufficient) if using full Toolkit install with Opensearch. Multiple NIC (Network Interface Card) Guidance \u00b6 Many sites would prefer not to have to deploy two servers for cost, space and power reasons. Since perfSONAR 3.5+ there is a way to install both latency and bandwidth measurement services on a single node, as long as it has at least two NICs (one per 'flavor' of measurement) and sufficient processing power and memory. There are few additional steps required in order to configure the node with multiple network cards: Please setup source routing as described in the official documentation . You'll need to register two hostnames in OIM / GOCDB (and have two reverse DNS entries) as you would normally for two separate nodes. Instead of configuring just one auto-URL in for the remote URL, please add both, so you'll end up having something like this: psconfig remote add \"https://psconfig.opensciencegrid.org/pub/auto/<FQDN_latency>\" psconfig remote add \"https://psconfig.opensciencegrid.org/pub/auto/<FQDN_throughput>\" ...","title":"Deployment"},{"location":"perfsonar/deployment-models/#perfsonar-deployment-options","text":"The primary motivation for perfSONAR deployment is to test isolation, i.e. only one end-to-end test should run on a host at a time. This ensures that the test results are not impacted by the other tests. Otherwise it is much more difficult to interpret test results, which may vary due to host effects rather then network effects. Taking this into account it means that perfSONAR measurement tools are much more accurate running on a dedicated hardware and while it may be useful to run them on other hosts such as Data Transfer Nodes the current recommendation is to have specific measurement machine. In addition, as bandwidth testing could impact latency testing, we recommend to deploy two different nodes, each focused on specific set of tests. The following deployment options are currently available: Bare metal - preffered option in one of two possible configurations: * Two bare metal servers, one for latency node, one for bandwidth node * One bare metal server running both latency and bandwidth node together provided that there are two NICs available, please refer to [dual NIC](#multiple-nic-network-interface-card-guidance) section for more details on this. Virtual Machine - if bare metal is not available then it is also possible to run perfSONAR on a VM, however there are a set of additional requirements to fulfill: * Full-node VM is strongly preferred, having 2 VMs (latency/bandwidth node) on a single bare metal. Mixing perfSONAR VM(s) with others might have an impact on the measurements and is therefore not recommended. * VM needs to be configured to have SR-IOV to NIC(s) as well as pinned CPUs to ensure bandwidth tests are not impacted (by hypervisor switching CPUs during the test) * Succesfull full speed local bandwidth test is highly recommended prior to putting the VM into production Container - perfSONAR has supported containers from version 4.1 (Q1 2018) and is documented at https://docs.perfsonar.net/install_docker.html but is not typically used in the same way as a full toolkit installation. ``` * Docker perfSONAR test instance can however still be used by sites that run multiple perfSONAR instances on site for their internal testing as this deployment model allows to flexibly deploy a testpoint which can send results to a local measurement archive running on the perfSONAR toolkit node. ```","title":"perfSONAR Deployment Options"},{"location":"perfsonar/deployment-models/#perfsonar-toolkit-vs-testpoint","text":"The perfSONAR team has documented the types of installations supported at https://docs.perfsonar.net/install_options.html . With the release of version 5, OSG/WLCG sites have a new option: instead of installing the full Toolkit sites can choose to install the Testpoint bundle. Pros * Simpler deployment when a local web interface is not needed and a central measurement archive is available. * Less resource intensive for both memory and I/O capacity. Cons ``` * Measurements are not stored locally * No web interface to use for configuration or adding local tests * Unable to show results in MaDDash ``` While sites are free to choose whatever deployment method they want, we would like to strongly recommend the use of perfSONAR's containerized testpoint. This method was chosen as a \"best practice\" recommendation because of the reduced resource constraints, less components and easier management.","title":"perfSONAR Toolkit vs Testpoint"},{"location":"perfsonar/deployment-models/#perfsonar-hardware-requirements","text":"There are two different nodes participating in the network testing, latency node and bandwidth node, while both are running on the exact same perfSONAR toolkit, they have very different requirements. Bandwidth node measures available (or peak) throughput with low test frequency and will thus require NIC with high capacity (1/10/40/100G are supported) as well as enough memory and CPU to support high bandwidth testing. Our recommendation is to match bandwidth node NIC speed with the one installed on the storage nodes as this would provide us with the best match when there are issues to investigate. In case you'd like to deploy high speed (100G) bandwidth node, please consult ESNet tuning guide and 100G tuning presentation . Latency node on the other hand runs low bandwidth, but high frequency tests, sending a continuous stream of packets to measure delay and corresponding packet loss, packet reordering, etc. This means that while it doesn't require high capacity NIC, 1G is usually sufficient, it can impose significant load on the IO to disk as well as CPU as many tests run in parallel and need to continuously store its results into local measurement archive. The minimum hardware requirements to run perfSONAR toolkit are documented here . For WLCG/OSG deployment and taking into account the amount of testing that we perform, we recommend at least the following for perfSONAR 5.0+: NIC for bandwidth node matching the capacity of the site storage nodes(10/25/40/100G), 1G NIC for latency node (for higher NIC capacities, 40/100G, please check ESNet tuning guide ) High clock speede CPU (3.0 Ghz+), fwere cores OK, with at least 32GB+ of RAM (8GB+ if using a Testpoint install) NVMe or SSD disk (128GB should be sufficient) if using full Toolkit install with Opensearch.","title":"perfSONAR Hardware Requirements"},{"location":"perfsonar/deployment-models/#multiple-nic-network-interface-card-guidance","text":"Many sites would prefer not to have to deploy two servers for cost, space and power reasons. Since perfSONAR 3.5+ there is a way to install both latency and bandwidth measurement services on a single node, as long as it has at least two NICs (one per 'flavor' of measurement) and sufficient processing power and memory. There are few additional steps required in order to configure the node with multiple network cards: Please setup source routing as described in the official documentation . You'll need to register two hostnames in OIM / GOCDB (and have two reverse DNS entries) as you would normally for two separate nodes. Instead of configuring just one auto-URL in for the remote URL, please add both, so you'll end up having something like this: psconfig remote add \"https://psconfig.opensciencegrid.org/pub/auto/<FQDN_latency>\" psconfig remote add \"https://psconfig.opensciencegrid.org/pub/auto/<FQDN_throughput>\" ...","title":"Multiple NIC (Network Interface Card) Guidance"},{"location":"perfsonar/faq/","text":"# Frequently Asked Questions Here we will provide details on troubleshooting perfSONAR installations for OSG and WLCG as well as some additional configuration options and a FAQ. A good overview of existing tools provided by perfSONAR toolkit and examples how to use them to identify and isolate network problems can be found at https://fasterdata.es.net/performance-testing/troubleshooting/network-troubleshooting-quick-reference-guide/ We are maintaining a Network Troubleshooting page to guide users in identifying and following up on network problems. Installing a certificate \u00b6 What is the recommended way to install a certificate on my perfSONAR host? We recommend using Lets Encrypt (see https://letsencrypt.org). There is a tutorial that users may find helpful at Secure Apache with Lets Encrypt . ``` A quick set of steps is shown here: 0. Install certbot with yum, dnf, or snap: **yum install certbot python3-certbot-apache** 1. Certbot needs port 80/443 so stop anything blocking it or using it: **systemctl stop firewalld systemctl stop httpd** 2. Test with dry-run: **certbot certonly --standalone --preferred-challenges http --dry-run** 3. Get the certificate: **certbot certonly --standalone --preferred-challenges http** 4. Restart httpd: **systemctl start firewalld systemctl start httpd** 5. Note certificate is installed under /etc/letsencrypt/ 6. Tell http where your certificate is: Edit /etc/httpd/conf.d/ssl.conf 1. Set **SSLCertificateFile** to /etc/letsencrypt/live/FQDN/cert.pem 2. Set **SSLCertificateKeyFile** to /etc/letsencrypt/live/FQDN/privkey.pem 3. Set **SSLCertificateChainFile** to /etc/letsencrypt/live/FQDN/fullchain.pem 7. Renew your certficate: **certbot renew --dry-run certbot renew** 8. Make a donation :) ``` Thanks to Raul Lopes for these details! Network Troubleshooting \u00b6 I suspect there is a network performance issue impacting my site For OSG sites, please open a ticket with GOC. Otherwise please open a GGUS ticket (or assign an existing) one to WLCG Network Throughput support unit. Service Registration \u00b6 I got an email after registering with lots of information in it...what do I do? This is part of the process. If you are a new site you will need to attend the next OSG operations meeting. If you are an existing site and have just registered perfSONAR instances you don't have to do anything but feel free to attend the next operations meeting if you have questions or concerns. Once I registered, new tickets were opened concerning perfSONAR...What do I do? This is standard operating procedure and the tickets are to ensure that OSG operations properly gets your new perfSONAR instances registered. You don't have to do anything and the tickets will be closed by OSG operations staff. Infrastructure Monitoring (check_mk metrics) \u00b6 perfSONAR services: versions metric is failing. This metrics checks if your sonar is at the most recent version. Please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running an older version (3.3-3.5), please update and reconfigure as soon as possible following Installation Guide perfSONAR configuration: contacts or location metrics are failing Please check if you have added the administrative information as detailed here perfSONAR services: bwctl/owamp/pscheduler metrics are failing This means that we're unable to connect to controller ports of the respective services, please ensure you have correct firewall settings (especially white listed subnets allowed) as described in the Installation Guide . This can also indicate failures of service daemons, please check http://docs.perfsonar.net/FAQ.html for additional details. perfSONAR services: esmond metric is failing This means that your measurement archive is not accessible or failing, there can be many possible causes (disk full, httpd not running or inaccessible, etc.), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support. perfSONAR json summary is failing ``` - This means the toolkit's homepage is inaccessible, which is required to check many additional services, so in turn all the other metrics will likely be in unknown or critical state. Please check for usual causes (disk full, httpd not running or blocked), we need to be able to access your homepage via HTTP or HTTPS ``` perfSONAR configuration: meshes metric is failing This indicates that you're missing the recommended mesh configuration. Please follow mesh configuration as detailed in the installation guide . Also, please REMOVE any old mesh configuration, this metric will also fail in case you have both the new mesh config and the old mesh URLs perfSONAR services: ntp is failing This indicates that NTP service is not running correctly on your toolkit instance, please note that NTP is critical service . Some things to check include your perfSONAR NTP configuration . If NTP is correctly configured, it is possible you could have a firewall issue: port 123 UDP must be open . There is NTP debugging information available on Google (e.g., https://support.ntp.org/bin/view/Support/TroubleshootingNTP ). If you still have problems, please open a support ticket (see below). perfSONAR services: regular testing/pscheduler is failing This indicates that pscheduler is not working correctly. As this is the core daemon please contact WLCG perfSONAR support unit for help. There are many tests failing for given sonar, where should I start Please update and reconfigure your sonar following Installation Guide . Please ensure firewall doesn't block access from the whitelisted subnets that are required for the infrastructure monitoring to work. Where can I get support on managing WLCG perfSONAR ? You can open ticket in GGUS to WLCG perfSONAR support unit or contact directly wlcg-perfsonar-support (at cern.ch) perfSONAR esmond freshness Latency/Bandwidth Direct is failing or gives warning This metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured tests. This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it. perfSONAR services ndt/npad is failing Both metrics check if you have disabled NDT and NPAD. As both NDT and NPAD have been dropped starting with 4.0, this metrics should stay green in most of the cases. perfSONAR hardware check is failing Please consult the minimum and recommended hardware requirements .","title":"Frequently Asked Questions"},{"location":"perfsonar/faq/#installing-a-certificate","text":"What is the recommended way to install a certificate on my perfSONAR host? We recommend using Lets Encrypt (see https://letsencrypt.org). There is a tutorial that users may find helpful at Secure Apache with Lets Encrypt . ``` A quick set of steps is shown here: 0. Install certbot with yum, dnf, or snap: **yum install certbot python3-certbot-apache** 1. Certbot needs port 80/443 so stop anything blocking it or using it: **systemctl stop firewalld systemctl stop httpd** 2. Test with dry-run: **certbot certonly --standalone --preferred-challenges http --dry-run** 3. Get the certificate: **certbot certonly --standalone --preferred-challenges http** 4. Restart httpd: **systemctl start firewalld systemctl start httpd** 5. Note certificate is installed under /etc/letsencrypt/ 6. Tell http where your certificate is: Edit /etc/httpd/conf.d/ssl.conf 1. Set **SSLCertificateFile** to /etc/letsencrypt/live/FQDN/cert.pem 2. Set **SSLCertificateKeyFile** to /etc/letsencrypt/live/FQDN/privkey.pem 3. Set **SSLCertificateChainFile** to /etc/letsencrypt/live/FQDN/fullchain.pem 7. Renew your certficate: **certbot renew --dry-run certbot renew** 8. Make a donation :) ``` Thanks to Raul Lopes for these details!","title":"Installing a certificate"},{"location":"perfsonar/faq/#network-troubleshooting","text":"I suspect there is a network performance issue impacting my site For OSG sites, please open a ticket with GOC. Otherwise please open a GGUS ticket (or assign an existing) one to WLCG Network Throughput support unit.","title":"Network Troubleshooting"},{"location":"perfsonar/faq/#service-registration","text":"I got an email after registering with lots of information in it...what do I do? This is part of the process. If you are a new site you will need to attend the next OSG operations meeting. If you are an existing site and have just registered perfSONAR instances you don't have to do anything but feel free to attend the next operations meeting if you have questions or concerns. Once I registered, new tickets were opened concerning perfSONAR...What do I do? This is standard operating procedure and the tickets are to ensure that OSG operations properly gets your new perfSONAR instances registered. You don't have to do anything and the tickets will be closed by OSG operations staff.","title":"Service Registration"},{"location":"perfsonar/faq/#infrastructure-monitoring-check_mk-metrics","text":"perfSONAR services: versions metric is failing. This metrics checks if your sonar is at the most recent version. Please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running an older version (3.3-3.5), please update and reconfigure as soon as possible following Installation Guide perfSONAR configuration: contacts or location metrics are failing Please check if you have added the administrative information as detailed here perfSONAR services: bwctl/owamp/pscheduler metrics are failing This means that we're unable to connect to controller ports of the respective services, please ensure you have correct firewall settings (especially white listed subnets allowed) as described in the Installation Guide . This can also indicate failures of service daemons, please check http://docs.perfsonar.net/FAQ.html for additional details. perfSONAR services: esmond metric is failing This means that your measurement archive is not accessible or failing, there can be many possible causes (disk full, httpd not running or inaccessible, etc.), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support. perfSONAR json summary is failing ``` - This means the toolkit's homepage is inaccessible, which is required to check many additional services, so in turn all the other metrics will likely be in unknown or critical state. Please check for usual causes (disk full, httpd not running or blocked), we need to be able to access your homepage via HTTP or HTTPS ``` perfSONAR configuration: meshes metric is failing This indicates that you're missing the recommended mesh configuration. Please follow mesh configuration as detailed in the installation guide . Also, please REMOVE any old mesh configuration, this metric will also fail in case you have both the new mesh config and the old mesh URLs perfSONAR services: ntp is failing This indicates that NTP service is not running correctly on your toolkit instance, please note that NTP is critical service . Some things to check include your perfSONAR NTP configuration . If NTP is correctly configured, it is possible you could have a firewall issue: port 123 UDP must be open . There is NTP debugging information available on Google (e.g., https://support.ntp.org/bin/view/Support/TroubleshootingNTP ). If you still have problems, please open a support ticket (see below). perfSONAR services: regular testing/pscheduler is failing This indicates that pscheduler is not working correctly. As this is the core daemon please contact WLCG perfSONAR support unit for help. There are many tests failing for given sonar, where should I start Please update and reconfigure your sonar following Installation Guide . Please ensure firewall doesn't block access from the whitelisted subnets that are required for the infrastructure monitoring to work. Where can I get support on managing WLCG perfSONAR ? You can open ticket in GGUS to WLCG perfSONAR support unit or contact directly wlcg-perfsonar-support (at cern.ch) perfSONAR esmond freshness Latency/Bandwidth Direct is failing or gives warning This metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured tests. This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it. perfSONAR services ndt/npad is failing Both metrics check if you have disabled NDT and NPAD. As both NDT and NPAD have been dropped starting with 4.0, this metrics should stay green in most of the cases. perfSONAR hardware check is failing Please consult the minimum and recommended hardware requirements .","title":"Infrastructure Monitoring (check_mk metrics)"},{"location":"perfsonar/install-testpoint/","text":"1. Prerequisites \u00b6 Ensure the host is up to date: \u00b6 sudo dnf update -y Install required packages: \u00b6 sudo dnf install -y git podman docker-compose nftables iproute Note : podman is the default container engine on EL9. If you wish to use Docker instead, install it appropriately. 2. Deploy the perfSONAR Testpoint Container \u00b6 Clone the repository: \u00b6 git clone https://github.com/perfsonar/perfsonar-testpoint-docker.git cd perfsonar-testpoint-docker Prepare configuration storage: \u00b6 sudo mkdir -p /opt/testpoint/ sudo cp -r compose/psconfig /opt/testpoint/ Edit the compose file as needed: \u00b6 Edit docker-compose.systemd.yml if you need to customize e.g., resource limits or volumes. Pull and Launch the Container: \u00b6 sudo podman-compose -f docker-compose.systemd.yml pull sudo podman-compose -f docker-compose.systemd.yml up -d Or, if using Docker: sudo docker compose -f docker-compose.systemd.yml pull sudo docker compose -f docker-compose.systemd.yml up -d 3. Configure Policy-Based Routing for Multi-Homed NICs \u00b6 Suppose: eth0 is for latency tests, IP \\= 192.168.10.10/24, GW \\= 192.168.10.1 eth1 is for throughput tests, IP \\= 10.20.30.10/24, GW \\= 10.20.30.1 a) Add custom routing tables \u00b6 Edit /etc/iproute2/rt_tables and add: 200 eth0table 201 eth1table b) Add routes and rules (replace IPs as appropriate): \u00b6 # Add rules for eth0 (latency) sudo ip rule add from 192.168.10.10/32 table eth0table sudo ip route add 192.168.10.0/24 dev eth0 scope link table eth0table sudo ip route add default via 192.168.10.1 dev eth0 table eth0table # Add rules for eth1 (throughput) sudo ip rule add from 10.20.30.10/32 table eth1table sudo ip route add 10.20.30.0/24 dev eth1 scope link table eth1table sudo ip route add default via 10.20.30.1 dev eth1 table eth1table c) Make persistent \u00b6 For persistent configuration, add these rules and routes to a script (e.g., ./perfsonar-policy-routing.sh in your working directory) and call it from /etc/rc.local (be sure /etc/rc.d/rc.local is executable and enabled), or use NetworkManager\u2019s connection profile route-rules and routes fields for the relevant interfaces. Example systemd unit: # /etc/systemd/system/perfsonar-policy-routing.service [Unit] Description\\=PerfSONAR Policy Routing After\\=network.target [Service] Type\\=oneshot ExecStart\\=/path/to/your/working/dir/perfsonar-policy-routing.sh [Install] WantedBy\\=multi-user.target Enable it: sudo systemctl enable --now perfsonar-policy-routing 4. Example NFTables Firewall Rules \u00b6 Below is a sample NFTables rule set that Allows required perfSONAR measurement ports (especially for testpoint: traceroute, iperf3, OWAMP, etc.) Restricts SSH access to trusted subnets/hosts Accepts ICMP/ICMPv6 and related/permitted connections /etc/nftables.conf: flush ruleset table inet perfsonar { ``` set allowed_protocols { type inet_proto elements \\= { icmp, icmpv6 } } set allowed_interfaces { type ifname elements \\= { \"lo\" } } set allowed_tcp_dports { type inet_service elements \\= { 22, 443, 861, 862, 9090, 123, 5201, 5001, 5000, 5101 } } set allowed_udp_ports { type inet_service elements \\= { 123, 5201, 5001, 5000, 5101 } } chain allow { ct state established,related accept ct state invalid drop meta l4proto @allowed_protocols accept iifname @allowed_interfaces accept tcp dport @allowed_tcp_dports ct state new accept udp dport @allowed_udp_ports ct state new accept # traceroute and test ranges udp dport 33434-33634 ct state new accept udp dport 18760-19960 ct state new accept udp dport 8760-9960 ct state new accept tcp dport 5890-5900 ct state new accept # SSH controls (add your trusted IPs/subnets) tcp dport 22 ip saddr 192.168.10.0/24 accept tcp dport 22 ip saddr 10.20.30.0/24 accept } chain input { type filter hook input priority 0; policy drop; jump allow reject with icmpx admin-prohibited } ``` } Apply and persist: sudo nft -f /etc/nftables.conf sudo systemctl enable --now nftables 5. Optional: perfSONAR Testpoint Container Networking \u00b6 If you want the container to use a specific NIC, adjust the docker-compose.systemd.yml to use --network host, or configure the container\u2019s network accordingly. By default, host mode is recommended for testpoint deployments to avoid NAT and ensure direct packet timing. 6. Confirm Operation \u00b6 Check containers: sudo podman ps # or sudo docker ps * Check logs: sudo podman logs perfsonar-testpoint * * Test connectivity between testpoints. 7. (Optional) Configure perfSONAR Remotes \u00b6 To register your testpoint with a central config: sudo podman exec -it perfsonar-testpoint psconfig remote list sudo podman exec -it perfsonar-testpoint psconfig remote --configure-archives add http://psconfig.opensciencegrid.org/pub/auto/psb02-gva.cern.ch 8. References & Further Reading \u00b6 perfSONAR testpoint Docker GitHub perfSONAR Documentation Red Hat Policy Routing [BROKEN-LINK: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/assembly_configuring-policy-based-routing_configuring-and-managing-networking] NFTables Wiki","title":"Install testpoint"},{"location":"perfsonar/install-testpoint/#1-prerequisites","text":"","title":"1. Prerequisites"},{"location":"perfsonar/install-testpoint/#ensure-the-host-is-up-to-date","text":"sudo dnf update -y","title":"Ensure the host is up to date:"},{"location":"perfsonar/install-testpoint/#install-required-packages","text":"sudo dnf install -y git podman docker-compose nftables iproute Note : podman is the default container engine on EL9. If you wish to use Docker instead, install it appropriately.","title":"Install required packages:"},{"location":"perfsonar/install-testpoint/#2-deploy-the-perfsonar-testpoint-container","text":"","title":"2. Deploy the perfSONAR Testpoint Container"},{"location":"perfsonar/install-testpoint/#clone-the-repository","text":"git clone https://github.com/perfsonar/perfsonar-testpoint-docker.git cd perfsonar-testpoint-docker","title":"Clone the repository:"},{"location":"perfsonar/install-testpoint/#prepare-configuration-storage","text":"sudo mkdir -p /opt/testpoint/ sudo cp -r compose/psconfig /opt/testpoint/","title":"Prepare configuration storage:"},{"location":"perfsonar/install-testpoint/#edit-the-compose-file-as-needed","text":"Edit docker-compose.systemd.yml if you need to customize e.g., resource limits or volumes.","title":"Edit the compose file as needed:"},{"location":"perfsonar/install-testpoint/#pull-and-launch-the-container","text":"sudo podman-compose -f docker-compose.systemd.yml pull sudo podman-compose -f docker-compose.systemd.yml up -d Or, if using Docker: sudo docker compose -f docker-compose.systemd.yml pull sudo docker compose -f docker-compose.systemd.yml up -d","title":"Pull and Launch the Container:"},{"location":"perfsonar/install-testpoint/#3-configure-policy-based-routing-for-multi-homed-nics","text":"Suppose: eth0 is for latency tests, IP \\= 192.168.10.10/24, GW \\= 192.168.10.1 eth1 is for throughput tests, IP \\= 10.20.30.10/24, GW \\= 10.20.30.1","title":"3. Configure Policy-Based Routing for Multi-Homed NICs"},{"location":"perfsonar/install-testpoint/#a-add-custom-routing-tables","text":"Edit /etc/iproute2/rt_tables and add: 200 eth0table 201 eth1table","title":"a) Add custom routing tables"},{"location":"perfsonar/install-testpoint/#b-add-routes-and-rules-replace-ips-as-appropriate","text":"# Add rules for eth0 (latency) sudo ip rule add from 192.168.10.10/32 table eth0table sudo ip route add 192.168.10.0/24 dev eth0 scope link table eth0table sudo ip route add default via 192.168.10.1 dev eth0 table eth0table # Add rules for eth1 (throughput) sudo ip rule add from 10.20.30.10/32 table eth1table sudo ip route add 10.20.30.0/24 dev eth1 scope link table eth1table sudo ip route add default via 10.20.30.1 dev eth1 table eth1table","title":"b) Add routes and rules (replace IPs as appropriate):"},{"location":"perfsonar/install-testpoint/#c-make-persistent","text":"For persistent configuration, add these rules and routes to a script (e.g., ./perfsonar-policy-routing.sh in your working directory) and call it from /etc/rc.local (be sure /etc/rc.d/rc.local is executable and enabled), or use NetworkManager\u2019s connection profile route-rules and routes fields for the relevant interfaces. Example systemd unit: # /etc/systemd/system/perfsonar-policy-routing.service [Unit] Description\\=PerfSONAR Policy Routing After\\=network.target [Service] Type\\=oneshot ExecStart\\=/path/to/your/working/dir/perfsonar-policy-routing.sh [Install] WantedBy\\=multi-user.target Enable it: sudo systemctl enable --now perfsonar-policy-routing","title":"c) Make persistent"},{"location":"perfsonar/install-testpoint/#4-example-nftables-firewall-rules","text":"Below is a sample NFTables rule set that Allows required perfSONAR measurement ports (especially for testpoint: traceroute, iperf3, OWAMP, etc.) Restricts SSH access to trusted subnets/hosts Accepts ICMP/ICMPv6 and related/permitted connections /etc/nftables.conf: flush ruleset table inet perfsonar { ``` set allowed_protocols { type inet_proto elements \\= { icmp, icmpv6 } } set allowed_interfaces { type ifname elements \\= { \"lo\" } } set allowed_tcp_dports { type inet_service elements \\= { 22, 443, 861, 862, 9090, 123, 5201, 5001, 5000, 5101 } } set allowed_udp_ports { type inet_service elements \\= { 123, 5201, 5001, 5000, 5101 } } chain allow { ct state established,related accept ct state invalid drop meta l4proto @allowed_protocols accept iifname @allowed_interfaces accept tcp dport @allowed_tcp_dports ct state new accept udp dport @allowed_udp_ports ct state new accept # traceroute and test ranges udp dport 33434-33634 ct state new accept udp dport 18760-19960 ct state new accept udp dport 8760-9960 ct state new accept tcp dport 5890-5900 ct state new accept # SSH controls (add your trusted IPs/subnets) tcp dport 22 ip saddr 192.168.10.0/24 accept tcp dport 22 ip saddr 10.20.30.0/24 accept } chain input { type filter hook input priority 0; policy drop; jump allow reject with icmpx admin-prohibited } ``` } Apply and persist: sudo nft -f /etc/nftables.conf sudo systemctl enable --now nftables","title":"4. Example NFTables Firewall Rules"},{"location":"perfsonar/install-testpoint/#5-optional-perfsonar-testpoint-container-networking","text":"If you want the container to use a specific NIC, adjust the docker-compose.systemd.yml to use --network host, or configure the container\u2019s network accordingly. By default, host mode is recommended for testpoint deployments to avoid NAT and ensure direct packet timing.","title":"5. Optional: perfSONAR Testpoint Container Networking"},{"location":"perfsonar/install-testpoint/#6-confirm-operation","text":"Check containers: sudo podman ps # or sudo docker ps * Check logs: sudo podman logs perfsonar-testpoint * * Test connectivity between testpoints.","title":"6. Confirm Operation"},{"location":"perfsonar/install-testpoint/#7-optional-configure-perfsonar-remotes","text":"To register your testpoint with a central config: sudo podman exec -it perfsonar-testpoint psconfig remote list sudo podman exec -it perfsonar-testpoint psconfig remote --configure-archives add http://psconfig.opensciencegrid.org/pub/auto/psb02-gva.cern.ch","title":"7. (Optional) Configure perfSONAR Remotes"},{"location":"perfsonar/install-testpoint/#8-references-further-reading","text":"perfSONAR testpoint Docker GitHub perfSONAR Documentation Red Hat Policy Routing [BROKEN-LINK: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/assembly_configuring-policy-based-routing_configuring-and-managing-networking] NFTables Wiki","title":"8. References &amp; Further Reading"},{"location":"perfsonar/installation/","text":"perfSONAR Installation Guide \u00b6 Note ``` This page has older instructions for non-containerized perfSONAR deployments which are, as of October 2025, no longer the recommended best practice. ``` This page documents installing/upgrading perfSONAR for OSG and WLCG sites. In case this is the first time you're trying to install and integrate your perfSONAR into WLCG or OSG, please consult our overview and possible deployment options before installing. For troubleshooting an existing installation please consult official Troubleshooting Guide , FAQ as well as WLCG/OSG specific FAQ . For any questions or help with WLCG perfSONAR setup, please contact GGUS WLCG perfSONAR support unit or OSG GOC . We strongly recommend anyone maintaining/using perfSONAR to join perfsonar-user and perfsonar-announce mailing lists. Installation or Upgrade \u00b6 Prior to installing please consult the release notes ) for the latest available release. In case you have already an instance running and wish to re-install/update it then please follow our recommendations: Upgrades: We recommend reinstalling using an EL9 (RHEL,Rocky,Alma) OS for all sites already running a registered instance or planning new installation. The primary reason for this recommendation is to provide a long-term supported OS and to benefit from a 5.x kernel. perfSONAR team provides support for Debian9 and Ubuntu as well, but we recommend to use EL9 to have a common, well understood deployment. Local measurement archive backup is not needed as OSG/WLCG stores all measurements centrally. In case you plan to deploy a single bare metal node with multiple NICs, please consult Multiple NIC Guidance First, install your chosen EL9 operating system on your host after saving you local configuration if you are \"updating\". The following options are then recommended to install perfSONAR for OSG/WLCG: Installation method Link Toolkit bundle installation Toolkit Installation Quick Start Testpoint bundle installation Follow quick start above but do 'dnf install perfsonar-testpoint' instead of toolkit You can see more details about EL supported installs at https://docs.perfsonar.net/install_el.html Note ``` In all cases, we strongly recommend to keep auto-updates enabled as this is the default settings starting from perfSONAR 4+. With yum auto-updates in place there is a possibility that updated packages can \"break\" your perfSONAR install but this is viewed an acceptable risk in order to have security updates quickly applied on perfSONAR instances. ``` The following additional steps are needed to configure the toolkit to be used in OSG/WLCG in addition to the steps described in the official guide: Please register your nodes in GOCDB/OIM. For OSG sites, follow the details in OSG Topology below. For non-OSG sites, follow the details in GOCDB Please ensure you have added or updated your administrative information You will need to configure your instance(s) to use the OSG/WLCG mesh-configuration. Please follow the steps below: `` * **For toolkit versions 5.0 and higher**, please run from the command line psconfig remote add https://psconfig.opensciencegrid.org/pub/auto/ . Replace with the fully qualified domain name of your host, e.g., psum01.aglt2.org . To verify the configuration is correct, you can run psconfig remote list`, which should show the URL configured, e.g. ``` json === pScheduler Agent === [ { \"url\" : \"https://psconfig.opensciencegrid.org/pub/auto/psum01.aglt2.org\" \"configure-archives\" : true } ] Please remove any old/stale URLs using psconfig remote delete <URL> If this is a new instance or you have changed the node's FQDN, you will need to notify wlcg-perfsonar-support 'at' cern.ch to add/update the hostname in one or more test meshes, which will then auto-configure the tests. Please indicate if you have preferences for which meshes your node should be included in (USATLAS, USCMS, ATLAS, CMS, LHCb, Alice, BelleII, etc.). You could also add any additional local tests via web interface (see Configuring regular tests for details). Please check which tests are auto-added via central meshes before adding any custom tests to avoid duplication. Note ``` Until your host is added (on https://psconfig.opensciencegrid.org ) to one or more meshes by a mesh-config administrator, the automesh configuration above won't be returning any tests (See registration information above). ``` We strongly recommend configuring perfSONAR in dual-stack mode (both IPv4 and IPv6). In case your site has IPv6 support, the only necessary step is to get both A and AAAA records for your perfSONAR DNS names (as well as ensuring the reverse DNS is in place). Adding communities is optional, but if you do, we recommend putting in WLCG as well as your VO: ATLAS , CMS , etc. This just helps others from the community lookup your instances in the public lookup service. As noted in the documentation you can select from already registered communities as appropriate. Please check that both local and campus firewall has the necessary port openings . Local iptables are configured automatically, but there are ways how to tune the existing set, please see the official firewall guide for details. Once installation is finished, please reboot the node. For any further questions, please consult official Troubleshooting Guide , FAQ as well as WLCG/OSG specific FAQ or contact directly WLCG or OSG perfSONAR support units. Maintenance \u00b6 Provided that you have enabled auto-updates, the only thing that remains is to follow up on any kernel security issues and either patch the node as soon as possible or reboot once the patched kernel is released. In case you'd like to manually update the node please follow the official guide . Using automated configuration tools (such as Chef, Puppet, etc) for managing perfSONAR are not officially supported, but there are some community driven projects that could be helpful, such as HEP-Puppet . As perfSONAR manages most of its configuration automatically via packages and there is very little initial configuration needed, we suggest to keep automated configuration to the minimum necessary to avoid unncessary interventions after auto-updates. Security Considerations \u00b6 The perfSONAR toolkit is reviewed both internally and externally for security flaws and the official documentation provides a lot of information on what security software is available and what firewall ports need to be opened, please see Manage Security for details. The toolkit's purpose is to allow us to measure and diagnose network problems and we therefore need to be cautious about blocking needed functionality by site or host firewalls. An overview of perfSONAR security is available at https://www.perfsonar.net/deployment_security.html Warning ``` As of perfSONAR 4.0+ ALL perfSONAR instances need to have port 443 accessible to all the other perfSONAR instances. Allowing access to port 443 is required because it's now used as a controller port for scheduling tests (via pScheduler). If sites are unable to reach your instance on port 443, tests may not run and results may not be available. Starting from perfSONAR 4.0, HTTPS/443 is now by default configured on all perfSONAR instances, i.e. local iptables as well as httpd configuration comes out of the box and requires no extra steps, therefore opening is only needed if you have central/campus firewall. ``` For sites that are concerned about having port 443 open, there is a possiblity to get a list of hosts to/from which the tests will be initiated. However as this list is dynamic, implementing the corresponding firewall rules would need to be done both locally and on the central/campus firewall in a way that would ensure dynamic updates. It's important to emphasize that port 443 provides access to the perfSONAR web interface as well, which is very useful to users and network administrators to debug network issues. Warning ``` In case you have central/campus firewall , please check the required port openings in the perfSONAR security documentation . ``` Enabling SNMP plugins \u00b6 Starting from release 4.0.2, perfSONAR toolkit allows to configure passive SNMP traffic from the local routers to be captured and stored in the local measurement archive. This is currently a beta feature that needs further testing and we're looking for volunteers willing to test, please let us know in case you would be interested. Register perfSONAR Service in GOCDB \u00b6 This section describes how to register the perfSONAR service in GOCDB. In order to register you perfSONAR services in GOCDB, you should access the proper section of GOC for adding a Service Endpoint https://goc.egi.eu/portal/index.php?Page_Type=New_Service_Endpoint You might not be able to access the page if you are not properly registered in GOC, so a snapshot can be found below. In filling the information please follow those simple guidelines: There are two service types for perfSONAR: net.perfSONAR.Bandwidth and net.perfSONAR.Latency. This is because we suggest t install two perfSONAR boxes at the site (one for latency tests and one for bandwidth tests) and therefore two distinct service endpoints should be published with two distinct service types. If the site can not afford sufficient hardware for the proposed setup, it can install a unique perfSONAR box, but still should publish both services types (with the same host in the \"host name\" field of the form). For each form (i.e. for each service type) fill at least the important informations: * Hosting Site (drop-down menu, mandatory) * Service Type (drop-down menu, mandatory) * Host Name (free text, mandatory) * Host IP (free text, optional) * Description: (free text, optional) This field has a default value of your site name. It is used to \"Label\" your host in our MaDDash GUI. If you want to use this field please use something as short as possible uniquely identifying this instance. Check \"N\" when asked \"Is it a beta service\" Check \"Y\" when asked \"Is this service in production\" Check \"Y\" when asked \"Is this service monitored\" GOCDB screen shot for creating a Service Endpoint: Register perfSONAR in OSG Topology \u00b6 Each OSG site should have two perfSONAR instances (one for Latency and one for Bandwidth) installed to enable network monitoring. These instances should be located as \"close\" (in a network-sense) as possible to the site's storage. If a logical site is comprised of more than one physical site, each physical site should be instrumented with perfSONAR instances. To add hosts to OSG Topology, please follow the instructions at https://osg-htc.org/docs/common/registration/ If you have problems or questions please consult our FAQ or alternatively open a ticket with GOC.","title":"Installation and Maintenance"},{"location":"perfsonar/installation/#perfsonar-installation-guide","text":"Note ``` This page has older instructions for non-containerized perfSONAR deployments which are, as of October 2025, no longer the recommended best practice. ``` This page documents installing/upgrading perfSONAR for OSG and WLCG sites. In case this is the first time you're trying to install and integrate your perfSONAR into WLCG or OSG, please consult our overview and possible deployment options before installing. For troubleshooting an existing installation please consult official Troubleshooting Guide , FAQ as well as WLCG/OSG specific FAQ . For any questions or help with WLCG perfSONAR setup, please contact GGUS WLCG perfSONAR support unit or OSG GOC . We strongly recommend anyone maintaining/using perfSONAR to join perfsonar-user and perfsonar-announce mailing lists.","title":"perfSONAR Installation Guide"},{"location":"perfsonar/installation/#installation-or-upgrade","text":"Prior to installing please consult the release notes ) for the latest available release. In case you have already an instance running and wish to re-install/update it then please follow our recommendations: Upgrades: We recommend reinstalling using an EL9 (RHEL,Rocky,Alma) OS for all sites already running a registered instance or planning new installation. The primary reason for this recommendation is to provide a long-term supported OS and to benefit from a 5.x kernel. perfSONAR team provides support for Debian9 and Ubuntu as well, but we recommend to use EL9 to have a common, well understood deployment. Local measurement archive backup is not needed as OSG/WLCG stores all measurements centrally. In case you plan to deploy a single bare metal node with multiple NICs, please consult Multiple NIC Guidance First, install your chosen EL9 operating system on your host after saving you local configuration if you are \"updating\". The following options are then recommended to install perfSONAR for OSG/WLCG: Installation method Link Toolkit bundle installation Toolkit Installation Quick Start Testpoint bundle installation Follow quick start above but do 'dnf install perfsonar-testpoint' instead of toolkit You can see more details about EL supported installs at https://docs.perfsonar.net/install_el.html Note ``` In all cases, we strongly recommend to keep auto-updates enabled as this is the default settings starting from perfSONAR 4+. With yum auto-updates in place there is a possibility that updated packages can \"break\" your perfSONAR install but this is viewed an acceptable risk in order to have security updates quickly applied on perfSONAR instances. ``` The following additional steps are needed to configure the toolkit to be used in OSG/WLCG in addition to the steps described in the official guide: Please register your nodes in GOCDB/OIM. For OSG sites, follow the details in OSG Topology below. For non-OSG sites, follow the details in GOCDB Please ensure you have added or updated your administrative information You will need to configure your instance(s) to use the OSG/WLCG mesh-configuration. Please follow the steps below: `` * **For toolkit versions 5.0 and higher**, please run from the command line psconfig remote add https://psconfig.opensciencegrid.org/pub/auto/ . Replace with the fully qualified domain name of your host, e.g., psum01.aglt2.org . To verify the configuration is correct, you can run psconfig remote list`, which should show the URL configured, e.g. ``` json === pScheduler Agent === [ { \"url\" : \"https://psconfig.opensciencegrid.org/pub/auto/psum01.aglt2.org\" \"configure-archives\" : true } ] Please remove any old/stale URLs using psconfig remote delete <URL> If this is a new instance or you have changed the node's FQDN, you will need to notify wlcg-perfsonar-support 'at' cern.ch to add/update the hostname in one or more test meshes, which will then auto-configure the tests. Please indicate if you have preferences for which meshes your node should be included in (USATLAS, USCMS, ATLAS, CMS, LHCb, Alice, BelleII, etc.). You could also add any additional local tests via web interface (see Configuring regular tests for details). Please check which tests are auto-added via central meshes before adding any custom tests to avoid duplication. Note ``` Until your host is added (on https://psconfig.opensciencegrid.org ) to one or more meshes by a mesh-config administrator, the automesh configuration above won't be returning any tests (See registration information above). ``` We strongly recommend configuring perfSONAR in dual-stack mode (both IPv4 and IPv6). In case your site has IPv6 support, the only necessary step is to get both A and AAAA records for your perfSONAR DNS names (as well as ensuring the reverse DNS is in place). Adding communities is optional, but if you do, we recommend putting in WLCG as well as your VO: ATLAS , CMS , etc. This just helps others from the community lookup your instances in the public lookup service. As noted in the documentation you can select from already registered communities as appropriate. Please check that both local and campus firewall has the necessary port openings . Local iptables are configured automatically, but there are ways how to tune the existing set, please see the official firewall guide for details. Once installation is finished, please reboot the node. For any further questions, please consult official Troubleshooting Guide , FAQ as well as WLCG/OSG specific FAQ or contact directly WLCG or OSG perfSONAR support units.","title":"Installation or Upgrade"},{"location":"perfsonar/installation/#maintenance","text":"Provided that you have enabled auto-updates, the only thing that remains is to follow up on any kernel security issues and either patch the node as soon as possible or reboot once the patched kernel is released. In case you'd like to manually update the node please follow the official guide . Using automated configuration tools (such as Chef, Puppet, etc) for managing perfSONAR are not officially supported, but there are some community driven projects that could be helpful, such as HEP-Puppet . As perfSONAR manages most of its configuration automatically via packages and there is very little initial configuration needed, we suggest to keep automated configuration to the minimum necessary to avoid unncessary interventions after auto-updates.","title":"Maintenance"},{"location":"perfsonar/installation/#security-considerations","text":"The perfSONAR toolkit is reviewed both internally and externally for security flaws and the official documentation provides a lot of information on what security software is available and what firewall ports need to be opened, please see Manage Security for details. The toolkit's purpose is to allow us to measure and diagnose network problems and we therefore need to be cautious about blocking needed functionality by site or host firewalls. An overview of perfSONAR security is available at https://www.perfsonar.net/deployment_security.html Warning ``` As of perfSONAR 4.0+ ALL perfSONAR instances need to have port 443 accessible to all the other perfSONAR instances. Allowing access to port 443 is required because it's now used as a controller port for scheduling tests (via pScheduler). If sites are unable to reach your instance on port 443, tests may not run and results may not be available. Starting from perfSONAR 4.0, HTTPS/443 is now by default configured on all perfSONAR instances, i.e. local iptables as well as httpd configuration comes out of the box and requires no extra steps, therefore opening is only needed if you have central/campus firewall. ``` For sites that are concerned about having port 443 open, there is a possiblity to get a list of hosts to/from which the tests will be initiated. However as this list is dynamic, implementing the corresponding firewall rules would need to be done both locally and on the central/campus firewall in a way that would ensure dynamic updates. It's important to emphasize that port 443 provides access to the perfSONAR web interface as well, which is very useful to users and network administrators to debug network issues. Warning ``` In case you have central/campus firewall , please check the required port openings in the perfSONAR security documentation . ```","title":"Security Considerations"},{"location":"perfsonar/installation/#enabling-snmp-plugins","text":"Starting from release 4.0.2, perfSONAR toolkit allows to configure passive SNMP traffic from the local routers to be captured and stored in the local measurement archive. This is currently a beta feature that needs further testing and we're looking for volunteers willing to test, please let us know in case you would be interested.","title":"Enabling SNMP plugins"},{"location":"perfsonar/installation/#register-perfsonar-service-in-gocdb","text":"This section describes how to register the perfSONAR service in GOCDB. In order to register you perfSONAR services in GOCDB, you should access the proper section of GOC for adding a Service Endpoint https://goc.egi.eu/portal/index.php?Page_Type=New_Service_Endpoint You might not be able to access the page if you are not properly registered in GOC, so a snapshot can be found below. In filling the information please follow those simple guidelines: There are two service types for perfSONAR: net.perfSONAR.Bandwidth and net.perfSONAR.Latency. This is because we suggest t install two perfSONAR boxes at the site (one for latency tests and one for bandwidth tests) and therefore two distinct service endpoints should be published with two distinct service types. If the site can not afford sufficient hardware for the proposed setup, it can install a unique perfSONAR box, but still should publish both services types (with the same host in the \"host name\" field of the form). For each form (i.e. for each service type) fill at least the important informations: * Hosting Site (drop-down menu, mandatory) * Service Type (drop-down menu, mandatory) * Host Name (free text, mandatory) * Host IP (free text, optional) * Description: (free text, optional) This field has a default value of your site name. It is used to \"Label\" your host in our MaDDash GUI. If you want to use this field please use something as short as possible uniquely identifying this instance. Check \"N\" when asked \"Is it a beta service\" Check \"Y\" when asked \"Is this service in production\" Check \"Y\" when asked \"Is this service monitored\" GOCDB screen shot for creating a Service Endpoint:","title":"Register perfSONAR Service in GOCDB"},{"location":"perfsonar/installation/#register-perfsonar-in-osg-topology","text":"Each OSG site should have two perfSONAR instances (one for Latency and one for Bandwidth) installed to enable network monitoring. These instances should be located as \"close\" (in a network-sense) as possible to the site's storage. If a logical site is comprised of more than one physical site, each physical site should be instrumented with perfSONAR instances. To add hosts to OSG Topology, please follow the instructions at https://osg-htc.org/docs/common/registration/ If you have problems or questions please consult our FAQ or alternatively open a ticket with GOC.","title":"Register perfSONAR in OSG Topology"},{"location":"perfsonar/multiple-nic-guidance/","text":"Multiple NIC (Network Interface Card) Guidance \u00b6 The OSG and WLCG recommendation is to deploy two flavors of perfSONAR measurement nodes: 1) a latency instance which continuously measures packet delay and lost and 2) a bandwidth instance measuring the achievable bandwidth. That implies sites must purchase, deploy and maintain two systems. Why can't we just run both latency and bandwidth services on a single instance? The problem is that running both latency and bandwidth services on a single node / single NIC may cause interence between the various measurements and introduce \"false-positive\" indications of network problems. Many sites would prefer not to have to deploy two servers for cost, space and power reasons. Fortunately the perfSONAR developers have provided a way to install both latency and bandwidth measurements services on a single node, as long as it has at least two NICs (one per 'flavor' of measurement) and sufficient processing and memory. See manage-dual-xface for details on configuring this.","title":"Multiple nic guidance"},{"location":"perfsonar/multiple-nic-guidance/#multiple-nic-network-interface-card-guidance","text":"The OSG and WLCG recommendation is to deploy two flavors of perfSONAR measurement nodes: 1) a latency instance which continuously measures packet delay and lost and 2) a bandwidth instance measuring the achievable bandwidth. That implies sites must purchase, deploy and maintain two systems. Why can't we just run both latency and bandwidth services on a single instance? The problem is that running both latency and bandwidth services on a single node / single NIC may cause interence between the various measurements and introduce \"false-positive\" indications of network problems. Many sites would prefer not to have to deploy two servers for cost, space and power reasons. Fortunately the perfSONAR developers have provided a way to install both latency and bandwidth measurements services on a single node, as long as it has at least two NICs (one per 'flavor' of measurement) and sufficient processing and memory. See manage-dual-xface for details on configuring this.","title":"Multiple NIC (Network Interface Card) Guidance"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/","text":"perfSONAR use of Policy Based Routing and rp_filter on EL9 \u00b6 In a multihome setup on EL9, sysctl rp_filter and policy-based routing (PBR) address two different layers of network traffic handling and can conflict with each other. PBR determines the outbound path for traffic based on criteria, while rp_filter is a security feature that validates the source address of inbound traffic. If not configured properly, strict rp_filter can block legitimate traffic in a PBR setup. sysctl rp_filter overview \u00b6 The Reverse Path Filtering ( rp_filter ) kernel parameter is a security measure designed to prevent IP spoofing, often associated with Denial of Service (DoS) attacks. When enabled, it checks the source IP address of an incoming packet to ensure the return path for a response would exit through the same interface that received the packet. It can be configured with three values: 0 : Disabled. No source validation is performed. This is required for asymmetric routing setups. 1 : Strict mode. The kernel performs a reverse-path lookup. If the best return path for the packet's source address is not the interface on which the packet was received, the packet is dropped. 2 : Loose mode. The kernel only validates that the source IP is routable via any interface, not necessarily the one it arrived on. Policy-based routing (PBR) overview \u00b6 PBR is a technique for overriding the standard Linux routing behavior, which is typically based solely on the destination IP address. It allows administrators to route traffic based on other criteria, such as the source IP address, application, protocol, or firewall marks ( fwmark ). A PBR setup involves these key steps: Create custom routing tables. Additional tables beyond the main routing table are defined, often in /etc/iproute2/rt_tables . Define rules. The ip rule command is used to add rules that select a specific routing table. Rules are matched against packets based on criteria like source address or firewall mark. Add routes to custom tables. Use the ip route command to add routes to the new tables. The conflict and solution \u00b6 The conflict arises when using strict rp_filter (value 1) in a multihomed network configured with PBR. A server is configured with PBR to send traffic from a specific service (e.g., source IP A1) out through network interface eth0 . The server's PBR rules also dictate that another service (e.g., source IP A2) sends traffic out through interface eth1 . An external client sends a packet to the service on source IP A2. The server's PBR rules correctly select the route on eth1 to send the response. However, if strict rp_filter is enabled, the kernel checks the incoming packet's source address and performs a reverse-path lookup. This lookup would likely find the best return path is through eth0 (the primary default gateway) rather than the eth1 interface where the packet arrived. The kernel drops the packet because of the rp_filter check, even though the PBR configuration would have correctly handled the outbound response. Solution for EL9 multihome: \u00b6 To enable asymmetric routing with PBR, you must relax the rp_filter setting, as strict mode will cause legitimate packets to be dropped. The best practice is to use a value of 2 (loose mode) or 0 (disabled). The loose mode is safer as it still provides some protection against spoofing, while 0 completely disables the check. You can configure this persistently by editing a file in /etc/sysctl.d/ , for example /etc/sysctl.d/99-network.conf : # Disable strict reverse path filtering for asymmetric routing net.ipv4.conf.all.rp_filter=2 After saving the file, apply the changes with sysctl -p . Summary of differences \u00b6 Feature sysctl rp_filter Policy-Based Routing (PBR) Purpose Security mechanism to prevent IP spoofing by checking inbound packet source addresses. Advanced routing method to control outbound traffic path based on policies. Traffic direction Checks incoming traffic. Explicitly directs outgoing traffic. Mechanism A single kernel parameter with three states ( 0 , 1 , 2 ) that applies globally or per-interface. Uses multiple routing tables and rules ( ip rule ) to select the appropriate table for a given packet. Compatibility Strict mode ( 1 ) conflicts with multihome setups involving asymmetric routing. Is designed for multihome setups but requires loosening or disabling rp_filter to function correctly","title":"perfSONAR use of Policy Based Routing and rp_filter on EL9"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#perfsonar-use-of-policy-based-routing-and-rp_filter-on-el9","text":"In a multihome setup on EL9, sysctl rp_filter and policy-based routing (PBR) address two different layers of network traffic handling and can conflict with each other. PBR determines the outbound path for traffic based on criteria, while rp_filter is a security feature that validates the source address of inbound traffic. If not configured properly, strict rp_filter can block legitimate traffic in a PBR setup.","title":"perfSONAR use of Policy Based Routing and rp_filter on EL9"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#sysctl-rp_filter-overview","text":"The Reverse Path Filtering ( rp_filter ) kernel parameter is a security measure designed to prevent IP spoofing, often associated with Denial of Service (DoS) attacks. When enabled, it checks the source IP address of an incoming packet to ensure the return path for a response would exit through the same interface that received the packet. It can be configured with three values: 0 : Disabled. No source validation is performed. This is required for asymmetric routing setups. 1 : Strict mode. The kernel performs a reverse-path lookup. If the best return path for the packet's source address is not the interface on which the packet was received, the packet is dropped. 2 : Loose mode. The kernel only validates that the source IP is routable via any interface, not necessarily the one it arrived on.","title":"sysctl rp_filter overview"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#policy-based-routing-pbr-overview","text":"PBR is a technique for overriding the standard Linux routing behavior, which is typically based solely on the destination IP address. It allows administrators to route traffic based on other criteria, such as the source IP address, application, protocol, or firewall marks ( fwmark ). A PBR setup involves these key steps: Create custom routing tables. Additional tables beyond the main routing table are defined, often in /etc/iproute2/rt_tables . Define rules. The ip rule command is used to add rules that select a specific routing table. Rules are matched against packets based on criteria like source address or firewall mark. Add routes to custom tables. Use the ip route command to add routes to the new tables.","title":"Policy-based routing (PBR) overview"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#the-conflict-and-solution","text":"The conflict arises when using strict rp_filter (value 1) in a multihomed network configured with PBR. A server is configured with PBR to send traffic from a specific service (e.g., source IP A1) out through network interface eth0 . The server's PBR rules also dictate that another service (e.g., source IP A2) sends traffic out through interface eth1 . An external client sends a packet to the service on source IP A2. The server's PBR rules correctly select the route on eth1 to send the response. However, if strict rp_filter is enabled, the kernel checks the incoming packet's source address and performs a reverse-path lookup. This lookup would likely find the best return path is through eth0 (the primary default gateway) rather than the eth1 interface where the packet arrived. The kernel drops the packet because of the rp_filter check, even though the PBR configuration would have correctly handled the outbound response.","title":"The conflict and solution"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#solution-for-el9-multihome","text":"To enable asymmetric routing with PBR, you must relax the rp_filter setting, as strict mode will cause legitimate packets to be dropped. The best practice is to use a value of 2 (loose mode) or 0 (disabled). The loose mode is safer as it still provides some protection against spoofing, while 0 completely disables the check. You can configure this persistently by editing a file in /etc/sysctl.d/ , for example /etc/sysctl.d/99-network.conf : # Disable strict reverse path filtering for asymmetric routing net.ipv4.conf.all.rp_filter=2 After saving the file, apply the changes with sysctl -p .","title":"Solution for EL9 multihome:"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#summary-of-differences","text":"Feature sysctl rp_filter Policy-Based Routing (PBR) Purpose Security mechanism to prevent IP spoofing by checking inbound packet source addresses. Advanced routing method to control outbound traffic path based on policies. Traffic direction Checks incoming traffic. Explicitly directs outgoing traffic. Mechanism A single kernel parameter with three states ( 0 , 1 , 2 ) that applies globally or per-interface. Uses multiple routing tables and rules ( ip rule ) to select the appropriate table for a given packet. Compatibility Strict mode ( 1 ) conflicts with multihome setups involving asymmetric routing. Is designed for multihome setups but requires loosening or disabling rp_filter to function correctly","title":"Summary of differences"},{"location":"perfsonar/perfSONAR-sysctl-info/","text":"Key sysctl settings for perfSONAR \u00b6 For optimal performance, the perfSONAR Toolkit applies system tuning settings by default upon installation by using the perfsonar-toolkit-sysctl package. These settings are based on the ESnet \"fasterdata\" knowledge base for high-performance test and measurement hosts and are sufficient for most use cases. However, you can manually verify or adjust the settings in /etc/sysctl.conf for specific needs. TCP buffer sizing \u00b6 These settings increase the maximum TCP buffer sizes to support high throughput, especially on high-speed (e.g., 10 Gbps and faster) networks over long distances. net.core.rmem_max : The maximum receive socket buffer size in bytes. net.core.wmem_max : The maximum send socket buffer size in bytes. net.ipv4.tcp_rmem : The minimum, default, and maximum receive buffer sizes for TCP. net.ipv4.tcp_wmem : The minimum, default, and maximum send buffer sizes for TCP. Example settings for a 10G or 40G host with up to 100ms round-trip time (RTT): net.core.rmem_max \\= 67108864 net.core.wmem_max \\= 67108864 net.ipv4.tcp_rmem \\= 4096 87380 33554432 net.ipv4.tcp_wmem \\= 4096 65536 33554432 For 100G or higher speeds, you may need to increase these values even further. Queue management and packet processing \u00b6 These settings help prevent packet loss and improve network efficiency. net.core.netdev_max_backlog : Increases the maximum length of the input packet queue for the network device. A higher value can prevent dropped packets during bursts of traffic. net.core.default_qdisc : Sets the default queuing discipline. Fair Queuing (fq) or Fair Queuing with CoDel (fq_codel) is recommended for its \"fairness\" in distributing bandwidth and kernel-level packet pacing. Multipath and routing \u00b6 If you are running perfSONAR on a host with multiple network interfaces on the same subnet, specific sysctl settings are needed to prevent Address Resolution Protocol (ARP) conflicts. net.ipv4.conf.all.arp_ignore=1 net.ipv4.conf.all.arp_announce=2 net.ipv4.conf.default.arp_filter=1 net.ipv4.conf.all.arp_filter=1 TCP optimizations \u00b6 For specific use cases, you might adjust other TCP settings: net.ipv4.tcp_timestamps : While often suggested for improving CPU utilization, be aware of the implications. The fasterdata guide suggests disabling it for older kernels, but modern kernels handle timestamps more efficiently. How to apply the settings \u00b6 Edit the configuration file: Add the desired settings to /etc/sysctl.conf or a new file in /etc/sysctl.d/ (e.g., /etc/sysctl.d/99-perfsonar.conf ). Load the settings: Run sysctl -p to load the changes from the configuration file. Verify the settings: Use sysctl [parameter] to confirm the new values are in effect. Automatic tuning with the perfSONAR Toolkit For installations using the perfSONAR Toolkit, the included perfsonar-toolkit-sysctl package handles most tuning automatically. If you change an interface speed, you can re-run the tuning script to apply appropriate settings: /usr/lib/perfsonar/scripts/configure_sysctl","title":"Key sysctl settings for perfSONAR"},{"location":"perfsonar/perfSONAR-sysctl-info/#key-sysctl-settings-for-perfsonar","text":"For optimal performance, the perfSONAR Toolkit applies system tuning settings by default upon installation by using the perfsonar-toolkit-sysctl package. These settings are based on the ESnet \"fasterdata\" knowledge base for high-performance test and measurement hosts and are sufficient for most use cases. However, you can manually verify or adjust the settings in /etc/sysctl.conf for specific needs.","title":"Key sysctl settings for perfSONAR"},{"location":"perfsonar/perfSONAR-sysctl-info/#tcp-buffer-sizing","text":"These settings increase the maximum TCP buffer sizes to support high throughput, especially on high-speed (e.g., 10 Gbps and faster) networks over long distances. net.core.rmem_max : The maximum receive socket buffer size in bytes. net.core.wmem_max : The maximum send socket buffer size in bytes. net.ipv4.tcp_rmem : The minimum, default, and maximum receive buffer sizes for TCP. net.ipv4.tcp_wmem : The minimum, default, and maximum send buffer sizes for TCP. Example settings for a 10G or 40G host with up to 100ms round-trip time (RTT): net.core.rmem_max \\= 67108864 net.core.wmem_max \\= 67108864 net.ipv4.tcp_rmem \\= 4096 87380 33554432 net.ipv4.tcp_wmem \\= 4096 65536 33554432 For 100G or higher speeds, you may need to increase these values even further.","title":"TCP buffer sizing"},{"location":"perfsonar/perfSONAR-sysctl-info/#queue-management-and-packet-processing","text":"These settings help prevent packet loss and improve network efficiency. net.core.netdev_max_backlog : Increases the maximum length of the input packet queue for the network device. A higher value can prevent dropped packets during bursts of traffic. net.core.default_qdisc : Sets the default queuing discipline. Fair Queuing (fq) or Fair Queuing with CoDel (fq_codel) is recommended for its \"fairness\" in distributing bandwidth and kernel-level packet pacing.","title":"Queue management and packet processing"},{"location":"perfsonar/perfSONAR-sysctl-info/#multipath-and-routing","text":"If you are running perfSONAR on a host with multiple network interfaces on the same subnet, specific sysctl settings are needed to prevent Address Resolution Protocol (ARP) conflicts. net.ipv4.conf.all.arp_ignore=1 net.ipv4.conf.all.arp_announce=2 net.ipv4.conf.default.arp_filter=1 net.ipv4.conf.all.arp_filter=1","title":"Multipath and routing"},{"location":"perfsonar/perfSONAR-sysctl-info/#tcp-optimizations","text":"For specific use cases, you might adjust other TCP settings: net.ipv4.tcp_timestamps : While often suggested for improving CPU utilization, be aware of the implications. The fasterdata guide suggests disabling it for older kernels, but modern kernels handle timestamps more efficiently.","title":"TCP optimizations"},{"location":"perfsonar/perfSONAR-sysctl-info/#how-to-apply-the-settings","text":"Edit the configuration file: Add the desired settings to /etc/sysctl.conf or a new file in /etc/sysctl.d/ (e.g., /etc/sysctl.d/99-perfsonar.conf ). Load the settings: Run sysctl -p to load the changes from the configuration file. Verify the settings: Use sysctl [parameter] to confirm the new values are in effect. Automatic tuning with the perfSONAR Toolkit For installations using the perfSONAR Toolkit, the included perfsonar-toolkit-sysctl package handles most tuning automatically. If you change an interface speed, you can re-run the tuning script to apply appropriate settings: /usr/lib/perfsonar/scripts/configure_sysctl","title":"How to apply the settings"},{"location":"perfsonar/psetf/","text":"Infrastructure Monitoring \u00b6 WLCG/OSG is operating more than 200 perfSONAR agents world-wide. A typical perfSONAR deployment has many services that need to function correctly for the the system to work. As we scale-up to many perfSONAR deployments across many sites it can be difficult to verify everything is working correctly. perfSONAR monitoring instance [BROKEN-LINK: https://psetf.opensciencegrid.org/etf/check_mk/index.py?start_url=%2Fetf%2Fcheck_mk%2Fdashboard.py] actively monitors the state of the infrastructure for both remote perfSONAR installation as well as central services. The instance is based on ETF [BROKEN-LINK: http://etf.cern.ch/docs/latest/], which is an open source measurement middleware for functional/availability testing of the resources. In order to access the page you'll need to have x509 grid certificate loaded in the browser. A sample initial dashboard is shown below: You can use quicksearch in the left pane to search for hostnames, domains or tests. The tests performed can be divided into four categories: Configuration tests ( perfSONAR configuration: ) tests if the contact, organisation and meshes were set following our installation guide . Service tests ( perfSONAR services: ) check if different perfSONAR toolkit services are up and running correctly as well as if ports are reachable from OSG subnets. Hardware test ( perfSONAR hardware ) checks if the node conforms to the minimal hardware requirements (see Requirements for details) Freshness tests ( perfSONAR freshness ) is a high level test that checks what tests are available in the local measurement archive and compares this with the tests configured. There can be many different reasons why certain tests are stale, such as disfunctional remote perfSONAR nodes, network connectivity issues as well as local issues with measurement archive or scheduling, therefore this test is informative and never reaches critical state. A special kind of freshness tests are OSG datastore freshness tests, which account for what fraction of tests results are stored centrally as compared to local measurement archive. It mainly reflects on the efficiency of the central OSG collector and doesn't provide any information on the on the local services. This is sample snapshost showing all metrics for particular perfSONAR instance (latency node in this case): For any issues/questions concerning the monitoring pages and tests, please consult the FAQ Central services are also monitored with the same tool and their status can be seen by following Business Intelligence/All Aggregations on the left pane. It shows the aggregated status of both production and pre-production services including mesh configuration interface, central datastore and infrastructure monitoring.","title":"Infrastructure Monitoring"},{"location":"perfsonar/psetf/#infrastructure-monitoring","text":"WLCG/OSG is operating more than 200 perfSONAR agents world-wide. A typical perfSONAR deployment has many services that need to function correctly for the the system to work. As we scale-up to many perfSONAR deployments across many sites it can be difficult to verify everything is working correctly. perfSONAR monitoring instance [BROKEN-LINK: https://psetf.opensciencegrid.org/etf/check_mk/index.py?start_url=%2Fetf%2Fcheck_mk%2Fdashboard.py] actively monitors the state of the infrastructure for both remote perfSONAR installation as well as central services. The instance is based on ETF [BROKEN-LINK: http://etf.cern.ch/docs/latest/], which is an open source measurement middleware for functional/availability testing of the resources. In order to access the page you'll need to have x509 grid certificate loaded in the browser. A sample initial dashboard is shown below: You can use quicksearch in the left pane to search for hostnames, domains or tests. The tests performed can be divided into four categories: Configuration tests ( perfSONAR configuration: ) tests if the contact, organisation and meshes were set following our installation guide . Service tests ( perfSONAR services: ) check if different perfSONAR toolkit services are up and running correctly as well as if ports are reachable from OSG subnets. Hardware test ( perfSONAR hardware ) checks if the node conforms to the minimal hardware requirements (see Requirements for details) Freshness tests ( perfSONAR freshness ) is a high level test that checks what tests are available in the local measurement archive and compares this with the tests configured. There can be many different reasons why certain tests are stale, such as disfunctional remote perfSONAR nodes, network connectivity issues as well as local issues with measurement archive or scheduling, therefore this test is informative and never reaches critical state. A special kind of freshness tests are OSG datastore freshness tests, which account for what fraction of tests results are stored centrally as compared to local measurement archive. It mainly reflects on the efficiency of the central OSG collector and doesn't provide any information on the on the local services. This is sample snapshost showing all metrics for particular perfSONAR instance (latency node in this case): For any issues/questions concerning the monitoring pages and tests, please consult the FAQ Central services are also monitored with the same tool and their status can be seen by following Business Intelligence/All Aggregations on the left pane. It shows the aggregated status of both production and pre-production services including mesh configuration interface, central datastore and infrastructure monitoring.","title":"Infrastructure Monitoring"},{"location":"perfsonar/register-ps-in-gocdb/","text":"Register perfSONAR Service in GOCDB \u00b6 This section describes how to register the perfSONAR service in GOCDB. \u00b6 In order to register you perfSONAR services in GOCDB, you should access the proper section of GOC for adding a Service Endpoint https://goc.egi.eu/portal/index.php?Page_Type=New_Service_Endpoint You might not be able to access the page if you are not properly registered in GOC, so a snapshot can be found below. In filling the information please follow those simple guidelines: There are two service types for perfSONAR: net.perfSONAR.Bandwidth and net.perfSONAR.Latency. This is because we suggest t install two perfSONAR boxes at the site (one for latency tests and one for bandwidth tests) and therefore two distinct service endpoints should be published with two distinct service types. If the site can not afford sufficient hardware for the proposed setup, it can install a unique perfSONAR box, but still should publish both services types (with the same host in the \"host name\" field of the form). For each form (i.e. for each service type) fill at least the important informations: ``` - Hosting Site (drop-down menu, mandatory) - Service Type (drop-down menu, mandatory) - Host Name (free text, mandatory) - Host IP (free text, optional) - Description: (free text, optional) This field has a default value of your site name. It is used to \"Label\" your host in our MaDDash GUI. If you want to use this field please use something as short as possible uniquely identifying this instance. - Check \"N\" when asked \"Is it a beta service\" - Check \"Y\" when asked \"Is this service in production\" - Check \"Y\" when asked \"Is this service monitored\" ``` GOCDB screen shot for creating a Service Endpoint: -- Main.ShawnMcKee - 21 Oct 2014","title":"Register ps in gocdb"},{"location":"perfsonar/register-ps-in-gocdb/#register-perfsonar-service-in-gocdb","text":"","title":"Register perfSONAR Service in GOCDB"},{"location":"perfsonar/register-ps-in-gocdb/#this-section-describes-how-to-register-the-perfsonar-service-in-gocdb","text":"In order to register you perfSONAR services in GOCDB, you should access the proper section of GOC for adding a Service Endpoint https://goc.egi.eu/portal/index.php?Page_Type=New_Service_Endpoint You might not be able to access the page if you are not properly registered in GOC, so a snapshot can be found below. In filling the information please follow those simple guidelines: There are two service types for perfSONAR: net.perfSONAR.Bandwidth and net.perfSONAR.Latency. This is because we suggest t install two perfSONAR boxes at the site (one for latency tests and one for bandwidth tests) and therefore two distinct service endpoints should be published with two distinct service types. If the site can not afford sufficient hardware for the proposed setup, it can install a unique perfSONAR box, but still should publish both services types (with the same host in the \"host name\" field of the form). For each form (i.e. for each service type) fill at least the important informations: ``` - Hosting Site (drop-down menu, mandatory) - Service Type (drop-down menu, mandatory) - Host Name (free text, mandatory) - Host IP (free text, optional) - Description: (free text, optional) This field has a default value of your site name. It is used to \"Label\" your host in our MaDDash GUI. If you want to use this field please use something as short as possible uniquely identifying this instance. - Check \"N\" when asked \"Is it a beta service\" - Check \"Y\" when asked \"Is this service in production\" - Check \"Y\" when asked \"Is this service monitored\" ``` GOCDB screen shot for creating a Service Endpoint: -- Main.ShawnMcKee - 21 Oct 2014","title":"This section describes how to register the perfSONAR service in GOCDB."},{"location":"perfsonar/tools_scripts/","text":"perfSONAR multi-NIC NetworkManager configuration \u00b6 This directory contains perfSONAR-pbr-nm.sh , a Bash script to configure static IPv4/IPv6 addressing and per-NIC source-based routing via NetworkManager (nmcli). Quick overview \u00b6 Script: perfSONAR-pbr-nm.sh Config file: /etc/perfSONAR-multi-nic-config.conf Log file: /var/log/perfSONAR-multi-nic-config.log Requirements \u00b6 Must be run as root. The script now enforces running as root early in execution and will exit if run as a non-privileged user. Run it with sudo or from a root shell. NetworkManager ( nmcli ) is required. The script checks for the presence of nmcli and will abort if it is not installed. Install NetworkManager via your distribution's package manager before running. Dependencies and package install hints \u00b6 The scripts in this directory call a number of external commands. Install these packages (or their distro equivalents) before using the tools below. Essential packages bash (Bash 4.3+) coreutils (cp/mv/rm/mkdir/chmod/chown) iproute2 (provides ip and ip route ) NetworkManager (provides nmcli ) rsync (recommended for safe backups; scripts fall back to cp ) curl openssl Optional / feature packages nftables (provides nft ) \u2014 required for perfSONAR-install-nftables.sh fail2ban (provides fail2ban-client ) \u2014 optional; only used if present SELinux user tools (provides getenforce , setenforce , restorecon ) \u2014 used by SELinux-related operations A container engine: podman or docker \u2014 required for the lsregistration updater/extractor when operating against the running testpoint container podman-compose or docker-compose \u2014 useful for running the testpoint compose bundle locally Note: the check-deps.sh helper accepts podman-compose as an alternative provider to docker-compose and will report the dependency as satisfied if either binary is present. Example install commands Note: package names vary slightly across distributions. Adapt as needed. Fedora / RHEL / CentOS (dnf): sudo dnf install -y bash coreutils iproute NetworkManager rsync curl openssl nftables podman podman-compose docker-compose fail2ban policycoreutils Debian / Ubuntu (apt): sudo apt-get update sudo apt-get install -y bash coreutils iproute2 network-manager rsync curl openssl nftables podman podman-compose docker.io docker-compose fail2ban policycoreutils If you intend to use the lsregistration container helpers, ensure either podman or docker is installed and that the service can list and access containers (e.g., podman ps or docker ps works as root). If rsync is not available the scripts will attempt a cp -a fallback, but installing rsync provides safer, more robust backups. Safety first \u00b6 This script will REMOVE ALL existing NetworkManager connections when run. Always test in a VM or console-attached host and use --dry-run to preview changes. The script creates a timestamped backup of existing connections before modifying anything. Compatibility and fallbacks \u00b6 The script prefers to configure routing and policy rules via NetworkManager ( nmcli ). However, nmcli support for advanced routes entries and routing-rules varies across versions and distributions. If nmcli cannot apply a given route or routing-rule, the script will attempt a compatibility fallback using the ip route and ip rule commands directly. Because the script now requires root, it no longer invokes sudo internally (the caller should run it with root privileges). This makes behavior deterministic in automation and avoids interactive sudo prompts. How to run (dry-run / debug) \u00b6 Preview what the script would do without changing the system: sudo bash perfSONAR-pbr-nm.sh --dry-run --debug Generate an example or auto-detected config (preview, dry-run only): sudo bash perfSONAR-pbr-nm.sh --generate-config-debug Write the auto-detected config to /etc (does not apply changes): sudo bash perfSONAR-pbr-nm.sh --generate-config-auto Run for real (be careful): sudo bash perfSONAR-pbr-nm.sh # or non-interactive sudo bash perfSONAR-pbr-nm.sh --yes Gateway requirement, inference, and generator warnings \u00b6 Any NIC with an IPv4 address must have a corresponding IPv4 gateway; likewise for IPv6. Conservative gateway inference: if a NIC has an address/prefix but no gateway, the tool will try to reuse a gateway from another NIC on the SAME subnet. IPv4: subnets are checked in bash; one unambiguous match is required. IPv6: requires python3 ( ipaddress module) to verify the gateway is in the same prefix; link-local gateways (fe80::/10) are not reused; one unambiguous match is required. If multiple gateways match, no guess is made; a warning is logged and validation will require you to set it explicitly. This inference runs in two places: 1) During auto-generation ( --generate-config-auto or --generate-config-debug ) so the written config can be immediately useful. 2) During normal execution after loading the config but before validation, so missing gateways may be filled automatically. Example: generated config with inferred gateways NIC_NAMES =( \"eth0\" \"eth1\" ) NIC_IPV4_ADDRS =( \"192.0.2.10\" \"192.0.2.20\" ) NIC_IPV4_PREFIXES =( \"/24\" \"/24\" ) NIC_IPV4_GWS =( \"192.0.2.1\" # guessed from eth0 \"192.0.2.1\" # guessed (reused gateway) ) NIC_IPV6_ADDRS =( \"2001:db8::10\" \"2001:db8::20\" ) NIC_IPV6_PREFIXES =( \"/64\" \"/64\" ) NIC_IPV6_GWS =( \"2001:db8::1\" # guessed from eth0 \"2001:db8::1\" # guessed (reused gateway) ) When gateways are inferred, a NOTE section is added near the bottom of the generated file listing each guess. The script will also print a NOTICE to the console/log. Review and edit the guessed values if needed before applying changes. If gateways remain missing after inference, the generator writes a WARNING block listing the affected NICs and the script will refuse to proceed until you set the gateways. Backups and safety \u00b6 Before applying changes, the script creates a timestamped backup of existing NetworkManager connections. It prefers rsync when available and falls back to cp -a . If the backup fails, the script aborts without removing existing configurations. Tests \u00b6 A small set of unit-style tests is provided under tests/ . These are designed to exercise pure validation and sanitization helpers without modifying system configuration. They source the script (functions only) and run checks in a non-destructive way. Run the tests: cd docs/perfsonar ./tests/run_tests.sh Notes \u00b6 The script requires Bash (uses local -n namerefs). Run tests on a system with Bash 4.3+. For more extensive validation, run shellcheck -x perfSONAR-pbr-nm.sh and address any issues reported. Contact \u00b6 Shawn McKee (script author) \u2014 smckee@umich.edu","title":"Scripts and tools"},{"location":"perfsonar/tools_scripts/#perfsonar-multi-nic-networkmanager-configuration","text":"This directory contains perfSONAR-pbr-nm.sh , a Bash script to configure static IPv4/IPv6 addressing and per-NIC source-based routing via NetworkManager (nmcli).","title":"perfSONAR multi-NIC NetworkManager configuration"},{"location":"perfsonar/tools_scripts/#quick-overview","text":"Script: perfSONAR-pbr-nm.sh Config file: /etc/perfSONAR-multi-nic-config.conf Log file: /var/log/perfSONAR-multi-nic-config.log","title":"Quick overview"},{"location":"perfsonar/tools_scripts/#requirements","text":"Must be run as root. The script now enforces running as root early in execution and will exit if run as a non-privileged user. Run it with sudo or from a root shell. NetworkManager ( nmcli ) is required. The script checks for the presence of nmcli and will abort if it is not installed. Install NetworkManager via your distribution's package manager before running.","title":"Requirements"},{"location":"perfsonar/tools_scripts/#dependencies-and-package-install-hints","text":"The scripts in this directory call a number of external commands. Install these packages (or their distro equivalents) before using the tools below. Essential packages bash (Bash 4.3+) coreutils (cp/mv/rm/mkdir/chmod/chown) iproute2 (provides ip and ip route ) NetworkManager (provides nmcli ) rsync (recommended for safe backups; scripts fall back to cp ) curl openssl Optional / feature packages nftables (provides nft ) \u2014 required for perfSONAR-install-nftables.sh fail2ban (provides fail2ban-client ) \u2014 optional; only used if present SELinux user tools (provides getenforce , setenforce , restorecon ) \u2014 used by SELinux-related operations A container engine: podman or docker \u2014 required for the lsregistration updater/extractor when operating against the running testpoint container podman-compose or docker-compose \u2014 useful for running the testpoint compose bundle locally Note: the check-deps.sh helper accepts podman-compose as an alternative provider to docker-compose and will report the dependency as satisfied if either binary is present. Example install commands Note: package names vary slightly across distributions. Adapt as needed. Fedora / RHEL / CentOS (dnf): sudo dnf install -y bash coreutils iproute NetworkManager rsync curl openssl nftables podman podman-compose docker-compose fail2ban policycoreutils Debian / Ubuntu (apt): sudo apt-get update sudo apt-get install -y bash coreutils iproute2 network-manager rsync curl openssl nftables podman podman-compose docker.io docker-compose fail2ban policycoreutils If you intend to use the lsregistration container helpers, ensure either podman or docker is installed and that the service can list and access containers (e.g., podman ps or docker ps works as root). If rsync is not available the scripts will attempt a cp -a fallback, but installing rsync provides safer, more robust backups.","title":"Dependencies and package install hints"},{"location":"perfsonar/tools_scripts/#safety-first","text":"This script will REMOVE ALL existing NetworkManager connections when run. Always test in a VM or console-attached host and use --dry-run to preview changes. The script creates a timestamped backup of existing connections before modifying anything.","title":"Safety first"},{"location":"perfsonar/tools_scripts/#compatibility-and-fallbacks","text":"The script prefers to configure routing and policy rules via NetworkManager ( nmcli ). However, nmcli support for advanced routes entries and routing-rules varies across versions and distributions. If nmcli cannot apply a given route or routing-rule, the script will attempt a compatibility fallback using the ip route and ip rule commands directly. Because the script now requires root, it no longer invokes sudo internally (the caller should run it with root privileges). This makes behavior deterministic in automation and avoids interactive sudo prompts.","title":"Compatibility and fallbacks"},{"location":"perfsonar/tools_scripts/#how-to-run-dry-run-debug","text":"Preview what the script would do without changing the system: sudo bash perfSONAR-pbr-nm.sh --dry-run --debug Generate an example or auto-detected config (preview, dry-run only): sudo bash perfSONAR-pbr-nm.sh --generate-config-debug Write the auto-detected config to /etc (does not apply changes): sudo bash perfSONAR-pbr-nm.sh --generate-config-auto Run for real (be careful): sudo bash perfSONAR-pbr-nm.sh # or non-interactive sudo bash perfSONAR-pbr-nm.sh --yes","title":"How to run (dry-run / debug)"},{"location":"perfsonar/tools_scripts/#gateway-requirement-inference-and-generator-warnings","text":"Any NIC with an IPv4 address must have a corresponding IPv4 gateway; likewise for IPv6. Conservative gateway inference: if a NIC has an address/prefix but no gateway, the tool will try to reuse a gateway from another NIC on the SAME subnet. IPv4: subnets are checked in bash; one unambiguous match is required. IPv6: requires python3 ( ipaddress module) to verify the gateway is in the same prefix; link-local gateways (fe80::/10) are not reused; one unambiguous match is required. If multiple gateways match, no guess is made; a warning is logged and validation will require you to set it explicitly. This inference runs in two places: 1) During auto-generation ( --generate-config-auto or --generate-config-debug ) so the written config can be immediately useful. 2) During normal execution after loading the config but before validation, so missing gateways may be filled automatically. Example: generated config with inferred gateways NIC_NAMES =( \"eth0\" \"eth1\" ) NIC_IPV4_ADDRS =( \"192.0.2.10\" \"192.0.2.20\" ) NIC_IPV4_PREFIXES =( \"/24\" \"/24\" ) NIC_IPV4_GWS =( \"192.0.2.1\" # guessed from eth0 \"192.0.2.1\" # guessed (reused gateway) ) NIC_IPV6_ADDRS =( \"2001:db8::10\" \"2001:db8::20\" ) NIC_IPV6_PREFIXES =( \"/64\" \"/64\" ) NIC_IPV6_GWS =( \"2001:db8::1\" # guessed from eth0 \"2001:db8::1\" # guessed (reused gateway) ) When gateways are inferred, a NOTE section is added near the bottom of the generated file listing each guess. The script will also print a NOTICE to the console/log. Review and edit the guessed values if needed before applying changes. If gateways remain missing after inference, the generator writes a WARNING block listing the affected NICs and the script will refuse to proceed until you set the gateways.","title":"Gateway requirement, inference, and generator warnings"},{"location":"perfsonar/tools_scripts/#backups-and-safety","text":"Before applying changes, the script creates a timestamped backup of existing NetworkManager connections. It prefers rsync when available and falls back to cp -a . If the backup fails, the script aborts without removing existing configurations.","title":"Backups and safety"},{"location":"perfsonar/tools_scripts/#tests","text":"A small set of unit-style tests is provided under tests/ . These are designed to exercise pure validation and sanitization helpers without modifying system configuration. They source the script (functions only) and run checks in a non-destructive way. Run the tests: cd docs/perfsonar ./tests/run_tests.sh","title":"Tests"},{"location":"perfsonar/tools_scripts/#notes","text":"The script requires Bash (uses local -n namerefs). Run tests on a system with Bash 4.3+. For more extensive validation, run shellcheck -x perfSONAR-pbr-nm.sh and address any issues reported.","title":"Notes"},{"location":"perfsonar/tools_scripts/#contact","text":"Shawn McKee (script author) \u2014 smckee@umich.edu","title":"Contact"},{"location":"perfsonar/tools_scripts/README-lsregistration/","text":"perfSONAR lsregistration helpers \u00b6 This directory includes two helpers for managing the perfSONAR Lookup Service (LS) registration configuration in lsregistrationdaemon.conf . perfSONAR-update-lsregistration.sh \u2014 updates configuration either inside the perfSONAR testpoint container or directly on the host (local mode). perfSONAR-extract-lsregistration.sh \u2014 reads an existing lsregistrationdaemon.conf and generates a self-contained restore script that invokes the updater with all equivalent flags. This is useful after an upgrade or rebuild to re-apply your previous configuration in one step. Update existing configuration (container or local) \u00b6 Script: perfSONAR-update-lsregistration.sh Container mode (default): copies /etc/perfsonar/lsregistrationdaemon.conf into a temp area, applies requested changes, writes it back into the container, and restarts lsregistrationdaemon inside the container. Key flags: --container NAME (default: perfsonar-testpoint ), `--engine auto|docker|podman` (default: `auto`). Local mode: operates directly on the host filesystem without a container. Key flags: --local , --conf PATH (default: `/etc/perfsonar/lsregistrationdaemon.conf`). Attempts a best-effort restart of lsregistrationdaemon on the host. Examples: # Update a few fields inside the container sudo ./perfSONAR-update-lsregistration.sh \\ --container perfsonar-testpoint \\ --site-name \"Acme Co.\" --domain example.org \\ --project WLCG --project OSG \\ --admin-name \"pS Admin\" --admin-email admin@example.org # Update the host file directly (non-container use) sudo ./perfSONAR-update-lsregistration.sh --local \\ --conf /etc/perfsonar/lsregistrationdaemon.conf \\ --city Berkeley --region CA --country US Generate a restore script from an existing conf \u00b6 Script: perfSONAR-extract-lsregistration.sh Reads values from an existing lsregistrationdaemon.conf (by default /etc/perfsonar/lsregistrationdaemon.conf ). Writes an executable restore script to /tmp/ that invokes perfSONAR-update-lsregistration.sh with the equivalent flags. Supports generating for container restore (default) or local restore. Common options: --conf PATH \u2014 source conf to parse (default: /etc/perfsonar/lsregistrationdaemon.conf ). --script PATH \u2014 path the restore script will call (default: ./perfSONAR-update-lsregistration.sh ). --local or --container NAME \u2014 choose local vs container restore target. --engine auto|docker|podman \u2014 include engine selection in container mode. --out PATH \u2014 where to write the restore script (default: /tmp/perfSONAR-restore-lsregistration-<timestamp>.sh ). --no-sudo \u2014 omit sudo in the generated command. Examples: # Build a container restore script (default container name) ./perfSONAR-extract-lsregistration.sh \\ --conf /etc/perfsonar/lsregistrationdaemon.conf \\ --script ./perfSONAR-update-lsregistration.sh # Build a local restore script that targets the host file directly ./perfSONAR-extract-lsregistration.sh --local \\ --target-conf /etc/perfsonar/lsregistrationdaemon.conf \\ --out /tmp/perfSONAR-restore-local.sh After generation, run the restore script to re-apply your configuration: sudo /tmp/perfSONAR-restore-lsregistration-YYYYmmddTHHMMSSZ.sh Notes \u00b6 Both scripts are Bash and require a modern Bash (4+). Use shellcheck for linting if making changes. In container mode, the updater restarts services inside the container; in local mode, it attempts to restart the host service.","title":"LS registration helpers (README)"},{"location":"perfsonar/tools_scripts/README-lsregistration/#perfsonar-lsregistration-helpers","text":"This directory includes two helpers for managing the perfSONAR Lookup Service (LS) registration configuration in lsregistrationdaemon.conf . perfSONAR-update-lsregistration.sh \u2014 updates configuration either inside the perfSONAR testpoint container or directly on the host (local mode). perfSONAR-extract-lsregistration.sh \u2014 reads an existing lsregistrationdaemon.conf and generates a self-contained restore script that invokes the updater with all equivalent flags. This is useful after an upgrade or rebuild to re-apply your previous configuration in one step.","title":"perfSONAR lsregistration helpers"},{"location":"perfsonar/tools_scripts/README-lsregistration/#update-existing-configuration-container-or-local","text":"Script: perfSONAR-update-lsregistration.sh Container mode (default): copies /etc/perfsonar/lsregistrationdaemon.conf into a temp area, applies requested changes, writes it back into the container, and restarts lsregistrationdaemon inside the container. Key flags: --container NAME (default: perfsonar-testpoint ), `--engine auto|docker|podman` (default: `auto`). Local mode: operates directly on the host filesystem without a container. Key flags: --local , --conf PATH (default: `/etc/perfsonar/lsregistrationdaemon.conf`). Attempts a best-effort restart of lsregistrationdaemon on the host. Examples: # Update a few fields inside the container sudo ./perfSONAR-update-lsregistration.sh \\ --container perfsonar-testpoint \\ --site-name \"Acme Co.\" --domain example.org \\ --project WLCG --project OSG \\ --admin-name \"pS Admin\" --admin-email admin@example.org # Update the host file directly (non-container use) sudo ./perfSONAR-update-lsregistration.sh --local \\ --conf /etc/perfsonar/lsregistrationdaemon.conf \\ --city Berkeley --region CA --country US","title":"Update existing configuration (container or local)"},{"location":"perfsonar/tools_scripts/README-lsregistration/#generate-a-restore-script-from-an-existing-conf","text":"Script: perfSONAR-extract-lsregistration.sh Reads values from an existing lsregistrationdaemon.conf (by default /etc/perfsonar/lsregistrationdaemon.conf ). Writes an executable restore script to /tmp/ that invokes perfSONAR-update-lsregistration.sh with the equivalent flags. Supports generating for container restore (default) or local restore. Common options: --conf PATH \u2014 source conf to parse (default: /etc/perfsonar/lsregistrationdaemon.conf ). --script PATH \u2014 path the restore script will call (default: ./perfSONAR-update-lsregistration.sh ). --local or --container NAME \u2014 choose local vs container restore target. --engine auto|docker|podman \u2014 include engine selection in container mode. --out PATH \u2014 where to write the restore script (default: /tmp/perfSONAR-restore-lsregistration-<timestamp>.sh ). --no-sudo \u2014 omit sudo in the generated command. Examples: # Build a container restore script (default container name) ./perfSONAR-extract-lsregistration.sh \\ --conf /etc/perfsonar/lsregistrationdaemon.conf \\ --script ./perfSONAR-update-lsregistration.sh # Build a local restore script that targets the host file directly ./perfSONAR-extract-lsregistration.sh --local \\ --target-conf /etc/perfsonar/lsregistrationdaemon.conf \\ --out /tmp/perfSONAR-restore-local.sh After generation, run the restore script to re-apply your configuration: sudo /tmp/perfSONAR-restore-lsregistration-YYYYmmddTHHMMSSZ.sh","title":"Generate a restore script from an existing conf"},{"location":"perfsonar/tools_scripts/README-lsregistration/#notes","text":"Both scripts are Bash and require a modern Bash (4+). Use shellcheck for linting if making changes. In container mode, the updater restarts services inside the container; in local mode, it attempts to restart the host service.","title":"Notes"},{"location":"personas/backups/quick-deploy_landing_original/","text":"The original docs/personas/quick-deploy/landing.md as read before replacement. ```markdown title: Quick Deploy \u2014 perfSONAR Testpoint (Quick Deploy) description: Quick, minimal steps to deploy a perfSONAR testpoint for WLCG/OSG monitoring. persona: quick-deploy owners: [networking-team@osg-htc.org] status: draft tags: [quickstart, perfSONAR, testpoint] # Quick Deploy \u2014 perfSONAR Testpoint This page provides a concise, tested minimal path to deploy a perfSONAR testpoint suitable for WLCG/OSG monitoring. Use the Quickstart for a fast, verified deployment. For optional features (fail2ban, selinux, nftables, automation) see the linked pages. - Quickstart: `quickstart-perfsonar-testpoint.md` - Optional features and automated setup: `automated-setup/README.md` Follow the quickstart if you need a working testpoint in a single admin session. ```","title":"Quick deploy landing original"},{"location":"personas/backups/research_landing_original/","text":"The original docs/personas/research/landing.md as read before replacement. --- title: Research \u2014 Architecture & Rationale description: Background, architecture, and data pipeline information for OSG perfSONAR monitoring. persona: research owners: [networking-team@osg-htc.org] status: draft tags: [architecture, research] --- This area is for readers who want to understand system architecture, data pipelines, and development notes. See `architecture.md` and `data-pipeline.md` .","title":"Research landing original"},{"location":"personas/backups/troubleshoot_landing_original/","text":"The original docs/personas/troubleshoot/landing.md as read before replacement. --- title: Troubleshoot \u2014 Networking & perfSONAR description: Triage checklist and playbooks for network troubleshooting in OSG/WLCG. persona: troubleshoot owners: [networking-team@osg-htc.org] status: draft tags: [troubleshoot, playbook] --- ## Troubleshoot \u2014 Triage and Playbooks This landing page points you to triage checklists, playbooks, and common issue pages. Start with the triage checklist and follow the relevant playbook. - `triage-checklist.md` - `playbooks/`","title":"Troubleshoot landing original"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/","text":"Installing a perfSONAR Testpoint for WLCG/OSG \u00b6 This quick-deploy playbook walks WLCG/OSG site administrators through the end-to-end installation, configuration, and validation of a perfSONAR testpoint on Enterprise Linux 9 (EL9). Each phase references tooling that already lives in this repository so you can automate as much as possible while still capturing the site-specific information required by OSG/WLCG operations. Prerequisites and Planning \u00b6 Before you begin, gather the following information: Hardware details: hostname, BMC/iLO/iDRAC credentials (if used), interface names, available storage. Network data: IPv4/IPv6 assignments for each NIC, default gateway, internal/external VLAN information, PSConfig registration URLs. Operational contacts: site admin email, OSG facility name, latitude/longitude, usage policy link. Repository artifacts: the scripts referenced below are in docs/perfsonar/ in this repository. Repository artifacts: the scripts referenced below are in docs/perfsonar/ in this repository. Existing perfSONAR configuration: If you are replacing or upgrading an existing perfSONAR instance, capture its configuration and registration data before taking services offline. Useful items to collect include: /etc/perfsonar/ configuration files, especially lsregistrationdaemon.conf any site-specific psconfig or testpoint config files stored in container volumes or host paths exported firewall, monitoring, and cron jobs that the current instance relies on The repository includes a helper script docs/perfsonar/tools_scripts/perfSONAR-update-lsregistration.sh which can copy and update lsregistrationdaemon.conf from running containers or the host; it can be used to extract registration config for re-use or migration. If you need to re-register or migrate metadata, run that script (or copy the lsregistrationdaemon.conf manually) and keep a copy in your change log. Note: the full repository clone/checkout instructions have been moved to Step 2 (after Step 1) so you can perform the clone once the host is provisioned. Note: All shell commands assume an interactive root shell. Prefix with sudo when running as a non-root user. Step 1 \u2013 Install and Harden EL9 \u00b6 Provision EL9: Install AlmaLinux, Rocky Linux, or RHEL 9 with the Minimal profile. Apply baseline updates (and verify dependencies): Use the repository's helper to check for required tools and print copy/paste install commands. Then apply OS updates and any remaining baseline packages. From a local clone of this repository (recommended): cd /opt/networking ./docs/perfsonar/tools_scripts/check-deps.sh Alternative: Download and run directly curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/check-deps.sh -o ./check-deps.sh chmod 0755 ./check-deps.sh ./check-deps.sh Apply updates and install baseline packages On EL9, apply updates and install common baseline packages, then add any packages suggested by the checker (copy/paste the printed dnf line): dnf update -y dnf install -y epel-release chrony vim git # (Optional) install any additional packages suggested by check-deps.sh # e.g., dnf install -y NetworkManager rsync curl openssl nftables Set the hostname and time sync: Note when you have multiple NICs pick one to be the hostname. That should also be the NIC that hosts the default route (See step 2 below). System configuration commands hostnamectl set-hostname <testpoint-hostname> systemctl enable --now chronyd timedatectl set-timezone <Region/City> Disable unused services: Service cleanup commands systemctl disable --now firewalld NetworkManager-wait-online dnf remove -y rsyslog Record NIC names: Commands to list network interfaces nmcli device status ip -br addr Document interface mappings; you will need them for the policy-based routing configuration. Step 2 \u2013 Clone the Repository \u00b6 This guide references multiple scripts from the osg-htc/networking repository. Clone the repository to your testpoint host for easy access to all tools. Recommended locations: Networking repo: /opt/networking (configuration scripts and documentation) perfSONAR testpoint compose bundle: /opt/testpoint (if using containerized testpoint) # Clone the networking repository to /opt cd /opt git clone https://github.com/osg-htc/networking.git # Optional: if deploying the perfSONAR testpoint container, clone it separately # git clone https://github.com/perfsonar/testpoint.git /opt/testpoint After cloning, all script examples in this guide that reference docs/perfsonar/tools_scripts/ assume you're running commands from /opt/networking . Note: All shell commands assume an interactive root shell. Prefix with sudo when running as a non-root user. Step 3 \u2013 Configure Policy-Based Routing (PBR) \u00b6 The repository ships an enhanced script docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh that automates NetworkManager configuration and routing rules and can auto-generate its config file. Script location in the repository: Directory (browse) Raw file (direct download) Stage the script: From a local clone of this repository: cd /opt/networking install -m 0755 docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh ~/perfsonar-pbr-nm.sh cd ~ Alternative: Download directly from the repository URL curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh -o ./perfSONAR-pbr-nm.sh chmod 0755 ./perfSONAR-pbr-nm.sh Auto-generate /etc/perfSONAR-multi-nic-config.conf : use the script\u2019s generator to detect NICs, addresses, prefixes, and gateways and write a starting config you can review/edit. Auto-generation is opt-in; it does not run by default. Preview (no changes): ~/perfsonar-pbr-nm.sh --generate-config-debug Write the config file to /etc/perfSONAR-multi-nic-config.conf : ~/perfsonar-pbr-nm.sh --generate-config-auto Then open the file and adjust any site-specific values (e.g., confirm DEFAULT_ROUTE_NIC , add any NIC_IPV4_ADDROUTE entries, or replace \u201c-\u201d for unused IP/gateway fields). Gateways required for addresses Any NIC with an IPv4 address must also have an IPv4 gateway, and any NIC with an IPv6 address must have an IPv6 gateway. If the generator cannot detect a gateway, it adds a WARNING block to the generated file listing affected NICs. Edit NIC_IPV4_GWS / NIC_IPV6_GWS accordingly before applying changes. Gateway prompts During generation, the script attempts to detect gateways per-NIC. If a NIC has an IP address but no gateway could be determined, it will prompt you interactively to enter an IPv4 and/or IPv6 gateway (or - to skip). Prompts are skipped in non-interactive sessions or when you use --yes . Execute the script: Rehearsal (no changes, extra logging recommended on first run): ~/perfsonar-pbr-nm.sh --dry-run --debug Apply changes non-interactively (auto-confirm): ~/perfsonar-pbr-nm.sh --yes Or run interactively and answer the confirmation prompt when ready: ~/perfsonar-pbr-nm.sh Missing gateways at apply time If the loaded config still contains - for a gateway on a NIC that has an IP address, the script will prompt you interactively to provide a gateway before applying changes. Use --yes (or run non-interactively) to suppress prompts; in that case, missing gateways will cause validation to fail so you can correct the config first. The script creates a timestamped backup of existing NetworkManager profiles, seeds routing tables, and applies routing rules. Review /var/log/perfSONAR-multi-nic-config.log after the run and retain it with your change records. DNS: forward and reverse entries (required) \u00b6 All IP addresses that will be used for perfSONAR testing MUST have DNS entries: a forward (A/AAAA) record and a matching reverse (PTR) record. This is required so remote test tools and site operators can reliably reach and identify your host, and because some measurement infrastructure and registration systems perform forward/reverse consistency checks. For single-stack IPv4-only hosts: ensure A and PTR are present and consistent. For single-stack IPv6-only hosts: ensure AAAA and PTR are present and consistent. For dual-stack hosts: both IPv4 and IPv6 addresses used for testing must have matching forward and reverse records (A+PTR and AAAA+PTR). Example: Bash script to automate DNS verification The perfSONAR-multi-nic-config.conf contains the arrays NIC_IPV4_ADDRS and NIC_IPV6_ADDRS (with - for unused entries). You can automate a forward/reverse consistency check using dig or host by sourcing the config and iterating the arrays. # quick DNS consistency check using /etc/perfSONAR-multi-nic-config.conf set -euo pipefail CONFIG = /etc/perfSONAR-multi-nic-config.conf [ -f \" $CONFIG \" ] || { echo \"Config not found: $CONFIG \" > & 2 ; exit 2 ; } source \" $CONFIG \" check_ip () { local ip = $1 local family = $2 # 4 or 6 # strip CIDR if present ip = ${ ip %%/* } [ \" $ip \" = \"-\" ] && return 0 # PTR lookup (reverse) ptr = $( dig +short -x \" $ip \" | head -n1 ) if [ -z \" $ptr \" ] ; then echo \"MISSING PTR for $ip \" return 1 fi # remove trailing dot from PTR ptr = ${ ptr %. } # Forward lookup for the hostname returned by PTR if [ \" $family \" = \"4\" ] ; then fwd = $( dig +short A \" $ptr \" | tr '\\n' ' ' ) else fwd = $( dig +short AAAA \" $ptr \" | tr '\\n' ' ' ) fi if ! echo \" $fwd \" | grep -qw \" $ip \" ; then echo \"INCONSISTENT: PTR $ptr does not resolve back to $ip (resolved: $fwd )\" return 1 fi echo \"OK: $ip \u21c4 $ptr \" return 0 } errors = 0 for ip in \" ${ NIC_IPV4_ADDRS [@] :- } \" ; do if [ \" $ip \" ! = \"-\" ] ; then check_ip \" $ip \" 4 || errors = $(( errors + 1 )) fi done for ip in \" ${ NIC_IPV6_ADDRS [@] :- } \" ; do if [ \" $ip \" ! = \"-\" ] ; then check_ip \" $ip \" 6 || errors = $(( errors + 1 )) fi done if (( errors > 0 )) ; then echo \"DNS verification failed ( $errors problem(s)). Fix DNS (forward/reverse) before running tests.\" > & 2 exit 1 fi echo \"DNS forward/reverse checks passed for configured addresses.\" Notes and automation tips: The script above uses dig (bind-utils package) which is commonly available; you can adapt it to use host if preferred. Run the check as part of your provisioning CI or as a pre-flight check before enabling measurement registration. For large sites or many addresses, parallelize the checks (xargs -P) or use a small Python script that leverages dns.resolver for async checks. If your PTR returns a hostname with a trailing dot, the script strips it before the forward check. If any addresses fail these checks, correct the DNS zone (forward and/or reverse) and allow DNS propagation before proceeding with registration and testing. Download and run the DNS checker You can download and run the DNS checker directly on the host (or from any machine that has network visibility to your DNS servers). The script expects /etc/perfSONAR-multi-nic-config.conf to exist and be readable. # Download (curl) curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/check-perfsonar-dns.sh -o ./check-perfsonar-dns.sh # Or download with wget # wget -O ./check-perfsonar-dns.sh https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/check-perfsonar-dns.sh chmod 0755 ./check-perfsonar-dns.sh # Install DNS tools if missing (EL9) sudo dnf install -y bind-utils # Debian/Ubuntu alternative # sudo apt-get update && sudo apt-get install -y dnsutils # Run the check (needs to read /etc/perfSONAR-multi-nic-config.conf) sudo ./check-perfsonar-dns.sh Notes: If you downloaded the script to a different path, either move it to the host and run it there, or copy /etc/perfSONAR-multi-nic-config.conf to the machine where you run the check (recommended only for temporary verification; treat the config as sensitive). The script uses dig (preferred) or host as a fallback; ensure one of those is installed. Run this check after you generate or edit /etc/perfSONAR-multi-nic-config.conf and before you register or start active measurements. Verify the routing policy: nmcli connection show ip rule show ip route show table <table-id> Confirm that non-default interfaces have their own routing tables and that the default interface owns the system default route. Step 4 \u2013 Configure nftables, SELinux, and Fail2Ban \u00b6 Use docs/perfsonar/tools_scripts/perfSONAR-install-nftables.sh to configure a hardened nftables profile with optional SELinux and Fail2Ban support. Script location in the repository: Directory (browse) Raw file (direct download) Prerequisites (not installed by the script): nftables must already be installed and available ( nft binary) for firewall configuration. fail2ban must be installed if you want the optional jail configuration. SELinux tools (e.g., getenforce , policycoreutils ) must be present to attempt SELinux configuration. If any prerequisite is missing, the script skips that component and continues. Stage the installer: From a local clone of this repository: cd /opt/networking install -m 0755 docs/perfsonar/tools_scripts/perfSONAR-install-nftables.sh ~/perfsonar-install-nftables.sh cd ~ Alternative: Download directly from the repository URL curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-install-nftables.sh -o ~/perfsonar-install-nftables.sh chmod 0755 ~/perfsonar-install-nftables.sh Run with desired options: ~/perfsonar-install-nftables.sh --selinux --fail2ban --yes Use --yes to skip the interactive confirmation prompt (omit it if you prefer to review the summary and answer manually). Add --dry-run for a rehearsal that only prints the planned actions. The script writes nftables rules for perfSONAR services, derives SSH allow-lists from /etc/perfSONAR-multi-nic-config.conf , optionally adjusts SELinux, and enables Fail2Ban jails\u2014only if those components are already installed. How SSH allow-lists and validation work SSH allow-list derivation: CIDR values in NIC_IPV4_PREFIXES / NIC_IPV6_PREFIXES paired with corresponding addresses are treated as subnets. Address entries without a prefix are treated as single hosts. The script logs the resolved lists (IPv4/IPv6 subnets and hosts) for review. Validation and output: The generated nftables file is validated with nft -c -f before being written; on validation failure, nothing is installed and a message is logged. Output locations: rules \u2192 /etc/nftables.d/perfsonar.nft , log \u2192 /var/log/perfSONAR-install-nftables.log , backups \u2192 /var/backups/perfsonar-install-<timestamp> . Preview nftables rules before applying You can preview the fully rendered nftables rules (no changes are made): ~/perfsonar-install-nftables.sh --print-rules Manually add extra management hosts/subnets If you need to allow additional SSH sources not represented by your NIC-derived prefixes, edit /etc/nftables.d/perfsonar.nft and add entries to the appropriate sets: set ssh_access_ip4_subnets { type ipv4_addr flags interval elements = { 192.0.2.0/24, 198.51.100.0/25 } } set ssh_access_ip4_hosts { type ipv4_addr elements = { 203.0.113.10, 203.0.113.11 } } set ssh_access_ip6_subnets { type ipv6_addr flags interval elements = { 2001:db8:1::/64 } } set ssh_access_ip6_hosts { type ipv6_addr elements = { 2001:db8::10 } } Then validate and reload (root shell): nft -c -f /etc/nftables.d/perfsonar.nft systemctl reload nftables || systemctl restart nftables Confirm firewall state and security services: Verification commands nft list ruleset sestatus systemctl status fail2ban Document any site-specific exceptions (e.g., additional allowed management hosts) in your change log. Step 5 \u2013 Deploy the Containerized perfSONAR Testpoint \u00b6 We\u2019ll run the official testpoint image from the GitHub Container Registry using Podman, but we\u2019ll show Docker-style commands so you can choose either tool. We\u2019ll bind-mount host paths so edits on the host are reflected inside the containers. Key paths to persist on the host: /opt/testpoint/psconfig \u2192 container /etc/perfsonar/psconfig /etc/apache2 \u2192 container /etc/apache2 (Apache configs) /var/www/html \u2192 container /var/www/html (webroot for Toolkit and ACME challenges) /etc/letsencrypt \u2192 container /etc/letsencrypt (certs/keys, if using Let\u2019s Encrypt) Install container tooling (Podman and optional Docker-style compose): dnf install -y podman podman-compose python3-pip pip3 install --upgrade docker-compose Tip: you can use either podman-compose or docker-compose in the steps below. Substitute the command that matches your preference. Prepare directories on the host: mkdir -p /opt/testpoint/psconfig mkdir -p /var/www/html mkdir -p /etc/apache2 mkdir -p /etc/letsencrypt Seed defaults from the testpoint container (first run without host bind-mounts for Apache/webroot so we can copy the initial content out): ??? example \"Create minimal compose and start the container\" Create a minimal compose file at ` / opt / testpoint / docker - compose . yml ` : ``` yaml version : \"3.9\" services : testpoint : container_name : perfsonar - testpoint image : ghcr . io / perfsonar / testpoint : 5.2 . 4 - systemd network_mode : \"host\" cgroup : host environment : - TZ = UTC restart : unless - stopped tmpfs : - / run - / run / lock - / tmp volumes : - / sys / fs / cgroup : / sys / fs / cgroup : rw tty : true pids_limit : 8192 cap_add : - CAP_NET_RAW ``` Bring it up with your preferred tool : ``` bash ( cd / opt / testpoint ; podman - compose up - d ) # or: (cd /opt/testpoint; docker-compose up -d) ``` Copy baseline content out of the running container to the host: # Use docker cp or podman cp (either works) docker cp perfsonar-testpoint:/etc/apache2 /etc/apache2 docker cp perfsonar-testpoint:/var/www/html /var/www/html docker cp perfsonar-testpoint:/etc/perfsonar/psconfig /opt/testpoint/psconfig If SELinux is enforcing, we\u2019ll relabel these paths when we mount (using :z / :Z below), so you don\u2019t need manual chcon . Replace the compose file with bind-mounts that map host paths directly, and (optionally) add a certbot sidecar for Let\u2019s Encrypt. You can download a ready-to-use compose file from this repository: Browse Download directly: curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/docker-compose.yml -o /opt/testpoint/docker-compose.yml ??? example \"Complete docker-compose.yml with bind-mounts and certbot\" Or create / edit ` / opt / testpoint / docker - compose . yml ` with the following content : Note : The provided compose file ships with ` io . containers . autoupdate = registry ` labels pre - set for Podman auto - update . ``` yaml version : \"3.9\" services : testpoint : container_name : perfsonar - testpoint image : ghcr . io / perfsonar / testpoint : 5.2 . 4 - systemd network_mode : \"host\" cgroup : host environment : - TZ = UTC restart : unless - stopped tmpfs : - / run - / run / lock - / tmp volumes : - / sys / fs / cgroup : / sys / fs / cgroup : rw - / opt / testpoint / psconfig : / etc / perfsonar / psconfig : Z - / var / www / html : / var / www / html : z - / etc / apache2 : / etc / apache2 : z - / etc / letsencrypt : / etc / letsencrypt : z tty : true pids_limit : 8192 cap_add : - CAP_NET_RAW # Optional: Let\u2019s Encrypt renewer sharing HTML and certs with testpoint certbot : image : certbot / certbot container_name : certbot network_mode : \"host\" restart : unless - stopped entrypoint : [ \"/bin/sh\" , \"-c\" , \"trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;\" ] depends_on : - testpoint volumes : - / var / www / html : / var / www / html : z - / etc / letsencrypt : / etc / letsencrypt : z ``` Restart with the new compose: ( cd /opt/testpoint ; podman-compose down ) ( cd /opt/testpoint ; podman-compose up -d ) # or docker-compose down && docker-compose up -d Optional \u2013 obtain your first Let\u2019s Encrypt certificate: The certbot sidecar above continuously renews existing certs. For the initial issuance, run a one-shot command and then reload Apache inside the testpoint container: # Issue (HTTP-01 webroot challenge). Replace values accordingly. docker run --rm --net = host -v /var/www/html:/var/www/html -v /etc/letsencrypt:/etc/letsencrypt \\ certbot/certbot certonly --webroot -w /var/www/html -d <SERVER_FQDN> \\ --email <LETSENCRYPT_EMAIL> --agree-tos --no-eff-email # Gracefully reload Apache within the testpoint container (or restart the service) docker exec -it perfsonar-testpoint bash -lc 'systemctl reload httpd || apachectl -k graceful || true' Notes Ensure port 80 on the host is reachable from the internet while issuing certificates. All shared paths use SELinux-aware :z / :Z to permit container access on enforcing hosts. Verify: curl -fsS http://localhost/toolkit/ | head -n 5 docker ps # or podman ps --- Step 6 \u2013 Register and Configure with WLCG/OSG \u00b6 PerfSONAR toolkit configuration: Browse to https://<SERVER_FQDN>/toolkit and complete the local toolkit setup wizard. Configuration details to populate Site contact email Usage policy URL Location (latitude/longitude) Export the configuration via /etc/perfsonar/psconfig/nodes/local.json for record keeping OSG/WLCG registration workflow: Registration steps and portals Register the host in OSG topology . Create or update a GGUS ticket announcing the new measurement point. In GOCDB , add the service endpoint org.opensciencegrid.crc.perfsonar-testpoint bound to this host. pSConfig enrollment: For each active NIC, register with the psconfig service so measurements cover all paths. ??? info \"Registration command and verification\" Example registration : ` `` bash /usr/bin/psconfig psconfig remote --configure-archives add --url https://psconfig.opensciencegrid.org/pub/auto/<NIC-FQDN> `` ` Confirm the resulting files in `/etc/perfsonar/psconfig/pscheduler.d/` map to the correct interface addresses ( `ifaddr` tags ). 1. ** Document memberships :** update your site wiki or change log with assigned mesh names , feed URLs , and support contacts . Update Lookup Service registration inside the container \u00b6 Use the helper script to edit /etc/perfsonar/lsregistrationdaemon.conf inside the running perfsonar-testpoint container and restart the daemon only if needed. Script (browse): perfSONAR-update-lsregistration.sh Raw (download): raw link Install and run examples (root shell): curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-update-lsregistration.sh \\ -o ~/perfSONAR-update-lsregistration.sh chmod 0755 ~/perfSONAR-update-lsregistration.sh # Preview changes only ~/perfSONAR-update-lsregistration.sh --dry-run --site-name \"Acme Co.\" --project WLCG --admin-email admin@example.org --admin-name \"pS Admin\" # Apply common updates and restart the daemon inside the container ~/perfSONAR-update-lsregistration.sh \\ --site-name \"Acme Co.\" --domain example.org --project WLCG --project OSG \\ --city Berkeley --region CA --country US --zip 94720 \\ --latitude 37 .5 --longitude -121.7469 \\ --admin-name \"pS Admin\" --admin-email admin@example.org Step 7 \u2013 Post-Install Validation \u00b6 Perform these checks before handing the host over to operations: System services: Verify Podman and compose services systemctl status podman systemctl --user status podman-compose@perfsonar-testpoint.service Ensure both are active/green. Container health: Check container status and logs podman ps --format 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}' podman logs pscheduler-agent | tail Network path validation: Test network connectivity and routing pscheduler task throughput --dest <remote-testpoint> tracepath -n <remote-testpoint> Confirm traffic uses the intended policy-based routes (check ip route get <dest> ). Toolkit diagnostics: visit the Toolkit UI \u2192 Dashboard \u2192 Host Status to confirm pScheduler, MaDDash, and owamp/bwctl services report healthy. Security posture: Check firewall, fail2ban, and SELinux nft list ruleset | grep perfsonar fail2ban-client status ausearch --message AVC --just-one Investigate any SELinux denials or repeated Fail2Ban bans. LetsEncrypt certificate check: Verify certificate validity openssl s_client -connect <SERVER_FQDN>:443 -servername <SERVER_FQDN> | openssl x509 -noout -dates -issuer Ensure the issuer is Let\u2019s Encrypt and the validity period is acceptable. Reporting: Run perfSONAR diagnostic reports Run the perfSONAR toolkit daily report and send outputs to operations: pscheduler troubleshoot toolkit-system-health Ongoing Maintenance \u00b6 Schedule quarterly re-validation of routing policy and nftables rules. Apply dnf update monthly and reboot during the next maintenance window. Monitor psconfig feeds for changes in mesh participation. Track certificate expiry ( certbot renew --dry-run ) if you rely on Let\u2019s Encrypt. Automatic image updates and safe restarts \u00b6 Keep containers current and only restart them when their image actually changes. Auto-update via labels and a systemd timer Add an auto-update label to services in your compose file (both testpoint and certbot if used): services : testpoint : # ... labels : - io.containers.autoupdate=registry certbot : # ... labels : - io.containers.autoupdate=registry This instructs Podman to check the registry for newer images and restart only if an update is pulled. Enable the Podman auto-update timer (runs daily by default): systemctl enable --now podman-auto-update.timer Run ad-hoc when desired and preview: podman auto-update --dry-run podman auto-update Inspect recent runs: systemctl list-timers | grep podman-auto-update journalctl -u podman-auto-update --since \"1 day ago\"","title":"Quickstart: perfSONAR Testpoint"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#installing-a-perfsonar-testpoint-for-wlcgosg","text":"This quick-deploy playbook walks WLCG/OSG site administrators through the end-to-end installation, configuration, and validation of a perfSONAR testpoint on Enterprise Linux 9 (EL9). Each phase references tooling that already lives in this repository so you can automate as much as possible while still capturing the site-specific information required by OSG/WLCG operations.","title":"Installing a perfSONAR Testpoint for WLCG/OSG"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#prerequisites-and-planning","text":"Before you begin, gather the following information: Hardware details: hostname, BMC/iLO/iDRAC credentials (if used), interface names, available storage. Network data: IPv4/IPv6 assignments for each NIC, default gateway, internal/external VLAN information, PSConfig registration URLs. Operational contacts: site admin email, OSG facility name, latitude/longitude, usage policy link. Repository artifacts: the scripts referenced below are in docs/perfsonar/ in this repository. Repository artifacts: the scripts referenced below are in docs/perfsonar/ in this repository. Existing perfSONAR configuration: If you are replacing or upgrading an existing perfSONAR instance, capture its configuration and registration data before taking services offline. Useful items to collect include: /etc/perfsonar/ configuration files, especially lsregistrationdaemon.conf any site-specific psconfig or testpoint config files stored in container volumes or host paths exported firewall, monitoring, and cron jobs that the current instance relies on The repository includes a helper script docs/perfsonar/tools_scripts/perfSONAR-update-lsregistration.sh which can copy and update lsregistrationdaemon.conf from running containers or the host; it can be used to extract registration config for re-use or migration. If you need to re-register or migrate metadata, run that script (or copy the lsregistrationdaemon.conf manually) and keep a copy in your change log. Note: the full repository clone/checkout instructions have been moved to Step 2 (after Step 1) so you can perform the clone once the host is provisioned. Note: All shell commands assume an interactive root shell. Prefix with sudo when running as a non-root user.","title":"Prerequisites and Planning"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-1-install-and-harden-el9","text":"Provision EL9: Install AlmaLinux, Rocky Linux, or RHEL 9 with the Minimal profile. Apply baseline updates (and verify dependencies): Use the repository's helper to check for required tools and print copy/paste install commands. Then apply OS updates and any remaining baseline packages. From a local clone of this repository (recommended): cd /opt/networking ./docs/perfsonar/tools_scripts/check-deps.sh Alternative: Download and run directly curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/check-deps.sh -o ./check-deps.sh chmod 0755 ./check-deps.sh ./check-deps.sh Apply updates and install baseline packages On EL9, apply updates and install common baseline packages, then add any packages suggested by the checker (copy/paste the printed dnf line): dnf update -y dnf install -y epel-release chrony vim git # (Optional) install any additional packages suggested by check-deps.sh # e.g., dnf install -y NetworkManager rsync curl openssl nftables Set the hostname and time sync: Note when you have multiple NICs pick one to be the hostname. That should also be the NIC that hosts the default route (See step 2 below). System configuration commands hostnamectl set-hostname <testpoint-hostname> systemctl enable --now chronyd timedatectl set-timezone <Region/City> Disable unused services: Service cleanup commands systemctl disable --now firewalld NetworkManager-wait-online dnf remove -y rsyslog Record NIC names: Commands to list network interfaces nmcli device status ip -br addr Document interface mappings; you will need them for the policy-based routing configuration.","title":"Step 1 \u2013 Install and Harden EL9"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-2-clone-the-repository","text":"This guide references multiple scripts from the osg-htc/networking repository. Clone the repository to your testpoint host for easy access to all tools. Recommended locations: Networking repo: /opt/networking (configuration scripts and documentation) perfSONAR testpoint compose bundle: /opt/testpoint (if using containerized testpoint) # Clone the networking repository to /opt cd /opt git clone https://github.com/osg-htc/networking.git # Optional: if deploying the perfSONAR testpoint container, clone it separately # git clone https://github.com/perfsonar/testpoint.git /opt/testpoint After cloning, all script examples in this guide that reference docs/perfsonar/tools_scripts/ assume you're running commands from /opt/networking . Note: All shell commands assume an interactive root shell. Prefix with sudo when running as a non-root user.","title":"Step 2 \u2013 Clone the Repository"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-3-configure-policy-based-routing-pbr","text":"The repository ships an enhanced script docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh that automates NetworkManager configuration and routing rules and can auto-generate its config file. Script location in the repository: Directory (browse) Raw file (direct download) Stage the script: From a local clone of this repository: cd /opt/networking install -m 0755 docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh ~/perfsonar-pbr-nm.sh cd ~ Alternative: Download directly from the repository URL curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh -o ./perfSONAR-pbr-nm.sh chmod 0755 ./perfSONAR-pbr-nm.sh Auto-generate /etc/perfSONAR-multi-nic-config.conf : use the script\u2019s generator to detect NICs, addresses, prefixes, and gateways and write a starting config you can review/edit. Auto-generation is opt-in; it does not run by default. Preview (no changes): ~/perfsonar-pbr-nm.sh --generate-config-debug Write the config file to /etc/perfSONAR-multi-nic-config.conf : ~/perfsonar-pbr-nm.sh --generate-config-auto Then open the file and adjust any site-specific values (e.g., confirm DEFAULT_ROUTE_NIC , add any NIC_IPV4_ADDROUTE entries, or replace \u201c-\u201d for unused IP/gateway fields). Gateways required for addresses Any NIC with an IPv4 address must also have an IPv4 gateway, and any NIC with an IPv6 address must have an IPv6 gateway. If the generator cannot detect a gateway, it adds a WARNING block to the generated file listing affected NICs. Edit NIC_IPV4_GWS / NIC_IPV6_GWS accordingly before applying changes. Gateway prompts During generation, the script attempts to detect gateways per-NIC. If a NIC has an IP address but no gateway could be determined, it will prompt you interactively to enter an IPv4 and/or IPv6 gateway (or - to skip). Prompts are skipped in non-interactive sessions or when you use --yes . Execute the script: Rehearsal (no changes, extra logging recommended on first run): ~/perfsonar-pbr-nm.sh --dry-run --debug Apply changes non-interactively (auto-confirm): ~/perfsonar-pbr-nm.sh --yes Or run interactively and answer the confirmation prompt when ready: ~/perfsonar-pbr-nm.sh Missing gateways at apply time If the loaded config still contains - for a gateway on a NIC that has an IP address, the script will prompt you interactively to provide a gateway before applying changes. Use --yes (or run non-interactively) to suppress prompts; in that case, missing gateways will cause validation to fail so you can correct the config first. The script creates a timestamped backup of existing NetworkManager profiles, seeds routing tables, and applies routing rules. Review /var/log/perfSONAR-multi-nic-config.log after the run and retain it with your change records.","title":"Step 3 \u2013 Configure Policy-Based Routing (PBR)"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#dns-forward-and-reverse-entries-required","text":"All IP addresses that will be used for perfSONAR testing MUST have DNS entries: a forward (A/AAAA) record and a matching reverse (PTR) record. This is required so remote test tools and site operators can reliably reach and identify your host, and because some measurement infrastructure and registration systems perform forward/reverse consistency checks. For single-stack IPv4-only hosts: ensure A and PTR are present and consistent. For single-stack IPv6-only hosts: ensure AAAA and PTR are present and consistent. For dual-stack hosts: both IPv4 and IPv6 addresses used for testing must have matching forward and reverse records (A+PTR and AAAA+PTR). Example: Bash script to automate DNS verification The perfSONAR-multi-nic-config.conf contains the arrays NIC_IPV4_ADDRS and NIC_IPV6_ADDRS (with - for unused entries). You can automate a forward/reverse consistency check using dig or host by sourcing the config and iterating the arrays. # quick DNS consistency check using /etc/perfSONAR-multi-nic-config.conf set -euo pipefail CONFIG = /etc/perfSONAR-multi-nic-config.conf [ -f \" $CONFIG \" ] || { echo \"Config not found: $CONFIG \" > & 2 ; exit 2 ; } source \" $CONFIG \" check_ip () { local ip = $1 local family = $2 # 4 or 6 # strip CIDR if present ip = ${ ip %%/* } [ \" $ip \" = \"-\" ] && return 0 # PTR lookup (reverse) ptr = $( dig +short -x \" $ip \" | head -n1 ) if [ -z \" $ptr \" ] ; then echo \"MISSING PTR for $ip \" return 1 fi # remove trailing dot from PTR ptr = ${ ptr %. } # Forward lookup for the hostname returned by PTR if [ \" $family \" = \"4\" ] ; then fwd = $( dig +short A \" $ptr \" | tr '\\n' ' ' ) else fwd = $( dig +short AAAA \" $ptr \" | tr '\\n' ' ' ) fi if ! echo \" $fwd \" | grep -qw \" $ip \" ; then echo \"INCONSISTENT: PTR $ptr does not resolve back to $ip (resolved: $fwd )\" return 1 fi echo \"OK: $ip \u21c4 $ptr \" return 0 } errors = 0 for ip in \" ${ NIC_IPV4_ADDRS [@] :- } \" ; do if [ \" $ip \" ! = \"-\" ] ; then check_ip \" $ip \" 4 || errors = $(( errors + 1 )) fi done for ip in \" ${ NIC_IPV6_ADDRS [@] :- } \" ; do if [ \" $ip \" ! = \"-\" ] ; then check_ip \" $ip \" 6 || errors = $(( errors + 1 )) fi done if (( errors > 0 )) ; then echo \"DNS verification failed ( $errors problem(s)). Fix DNS (forward/reverse) before running tests.\" > & 2 exit 1 fi echo \"DNS forward/reverse checks passed for configured addresses.\" Notes and automation tips: The script above uses dig (bind-utils package) which is commonly available; you can adapt it to use host if preferred. Run the check as part of your provisioning CI or as a pre-flight check before enabling measurement registration. For large sites or many addresses, parallelize the checks (xargs -P) or use a small Python script that leverages dns.resolver for async checks. If your PTR returns a hostname with a trailing dot, the script strips it before the forward check. If any addresses fail these checks, correct the DNS zone (forward and/or reverse) and allow DNS propagation before proceeding with registration and testing. Download and run the DNS checker You can download and run the DNS checker directly on the host (or from any machine that has network visibility to your DNS servers). The script expects /etc/perfSONAR-multi-nic-config.conf to exist and be readable. # Download (curl) curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/check-perfsonar-dns.sh -o ./check-perfsonar-dns.sh # Or download with wget # wget -O ./check-perfsonar-dns.sh https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/check-perfsonar-dns.sh chmod 0755 ./check-perfsonar-dns.sh # Install DNS tools if missing (EL9) sudo dnf install -y bind-utils # Debian/Ubuntu alternative # sudo apt-get update && sudo apt-get install -y dnsutils # Run the check (needs to read /etc/perfSONAR-multi-nic-config.conf) sudo ./check-perfsonar-dns.sh Notes: If you downloaded the script to a different path, either move it to the host and run it there, or copy /etc/perfSONAR-multi-nic-config.conf to the machine where you run the check (recommended only for temporary verification; treat the config as sensitive). The script uses dig (preferred) or host as a fallback; ensure one of those is installed. Run this check after you generate or edit /etc/perfSONAR-multi-nic-config.conf and before you register or start active measurements. Verify the routing policy: nmcli connection show ip rule show ip route show table <table-id> Confirm that non-default interfaces have their own routing tables and that the default interface owns the system default route.","title":"DNS: forward and reverse entries (required)"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-4-configure-nftables-selinux-and-fail2ban","text":"Use docs/perfsonar/tools_scripts/perfSONAR-install-nftables.sh to configure a hardened nftables profile with optional SELinux and Fail2Ban support. Script location in the repository: Directory (browse) Raw file (direct download) Prerequisites (not installed by the script): nftables must already be installed and available ( nft binary) for firewall configuration. fail2ban must be installed if you want the optional jail configuration. SELinux tools (e.g., getenforce , policycoreutils ) must be present to attempt SELinux configuration. If any prerequisite is missing, the script skips that component and continues. Stage the installer: From a local clone of this repository: cd /opt/networking install -m 0755 docs/perfsonar/tools_scripts/perfSONAR-install-nftables.sh ~/perfsonar-install-nftables.sh cd ~ Alternative: Download directly from the repository URL curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-install-nftables.sh -o ~/perfsonar-install-nftables.sh chmod 0755 ~/perfsonar-install-nftables.sh Run with desired options: ~/perfsonar-install-nftables.sh --selinux --fail2ban --yes Use --yes to skip the interactive confirmation prompt (omit it if you prefer to review the summary and answer manually). Add --dry-run for a rehearsal that only prints the planned actions. The script writes nftables rules for perfSONAR services, derives SSH allow-lists from /etc/perfSONAR-multi-nic-config.conf , optionally adjusts SELinux, and enables Fail2Ban jails\u2014only if those components are already installed. How SSH allow-lists and validation work SSH allow-list derivation: CIDR values in NIC_IPV4_PREFIXES / NIC_IPV6_PREFIXES paired with corresponding addresses are treated as subnets. Address entries without a prefix are treated as single hosts. The script logs the resolved lists (IPv4/IPv6 subnets and hosts) for review. Validation and output: The generated nftables file is validated with nft -c -f before being written; on validation failure, nothing is installed and a message is logged. Output locations: rules \u2192 /etc/nftables.d/perfsonar.nft , log \u2192 /var/log/perfSONAR-install-nftables.log , backups \u2192 /var/backups/perfsonar-install-<timestamp> . Preview nftables rules before applying You can preview the fully rendered nftables rules (no changes are made): ~/perfsonar-install-nftables.sh --print-rules Manually add extra management hosts/subnets If you need to allow additional SSH sources not represented by your NIC-derived prefixes, edit /etc/nftables.d/perfsonar.nft and add entries to the appropriate sets: set ssh_access_ip4_subnets { type ipv4_addr flags interval elements = { 192.0.2.0/24, 198.51.100.0/25 } } set ssh_access_ip4_hosts { type ipv4_addr elements = { 203.0.113.10, 203.0.113.11 } } set ssh_access_ip6_subnets { type ipv6_addr flags interval elements = { 2001:db8:1::/64 } } set ssh_access_ip6_hosts { type ipv6_addr elements = { 2001:db8::10 } } Then validate and reload (root shell): nft -c -f /etc/nftables.d/perfsonar.nft systemctl reload nftables || systemctl restart nftables Confirm firewall state and security services: Verification commands nft list ruleset sestatus systemctl status fail2ban Document any site-specific exceptions (e.g., additional allowed management hosts) in your change log.","title":"Step 4 \u2013 Configure nftables, SELinux, and Fail2Ban"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-5-deploy-the-containerized-perfsonar-testpoint","text":"We\u2019ll run the official testpoint image from the GitHub Container Registry using Podman, but we\u2019ll show Docker-style commands so you can choose either tool. We\u2019ll bind-mount host paths so edits on the host are reflected inside the containers. Key paths to persist on the host: /opt/testpoint/psconfig \u2192 container /etc/perfsonar/psconfig /etc/apache2 \u2192 container /etc/apache2 (Apache configs) /var/www/html \u2192 container /var/www/html (webroot for Toolkit and ACME challenges) /etc/letsencrypt \u2192 container /etc/letsencrypt (certs/keys, if using Let\u2019s Encrypt) Install container tooling (Podman and optional Docker-style compose): dnf install -y podman podman-compose python3-pip pip3 install --upgrade docker-compose Tip: you can use either podman-compose or docker-compose in the steps below. Substitute the command that matches your preference. Prepare directories on the host: mkdir -p /opt/testpoint/psconfig mkdir -p /var/www/html mkdir -p /etc/apache2 mkdir -p /etc/letsencrypt Seed defaults from the testpoint container (first run without host bind-mounts for Apache/webroot so we can copy the initial content out): ??? example \"Create minimal compose and start the container\" Create a minimal compose file at ` / opt / testpoint / docker - compose . yml ` : ``` yaml version : \"3.9\" services : testpoint : container_name : perfsonar - testpoint image : ghcr . io / perfsonar / testpoint : 5.2 . 4 - systemd network_mode : \"host\" cgroup : host environment : - TZ = UTC restart : unless - stopped tmpfs : - / run - / run / lock - / tmp volumes : - / sys / fs / cgroup : / sys / fs / cgroup : rw tty : true pids_limit : 8192 cap_add : - CAP_NET_RAW ``` Bring it up with your preferred tool : ``` bash ( cd / opt / testpoint ; podman - compose up - d ) # or: (cd /opt/testpoint; docker-compose up -d) ``` Copy baseline content out of the running container to the host: # Use docker cp or podman cp (either works) docker cp perfsonar-testpoint:/etc/apache2 /etc/apache2 docker cp perfsonar-testpoint:/var/www/html /var/www/html docker cp perfsonar-testpoint:/etc/perfsonar/psconfig /opt/testpoint/psconfig If SELinux is enforcing, we\u2019ll relabel these paths when we mount (using :z / :Z below), so you don\u2019t need manual chcon . Replace the compose file with bind-mounts that map host paths directly, and (optionally) add a certbot sidecar for Let\u2019s Encrypt. You can download a ready-to-use compose file from this repository: Browse Download directly: curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/docker-compose.yml -o /opt/testpoint/docker-compose.yml ??? example \"Complete docker-compose.yml with bind-mounts and certbot\" Or create / edit ` / opt / testpoint / docker - compose . yml ` with the following content : Note : The provided compose file ships with ` io . containers . autoupdate = registry ` labels pre - set for Podman auto - update . ``` yaml version : \"3.9\" services : testpoint : container_name : perfsonar - testpoint image : ghcr . io / perfsonar / testpoint : 5.2 . 4 - systemd network_mode : \"host\" cgroup : host environment : - TZ = UTC restart : unless - stopped tmpfs : - / run - / run / lock - / tmp volumes : - / sys / fs / cgroup : / sys / fs / cgroup : rw - / opt / testpoint / psconfig : / etc / perfsonar / psconfig : Z - / var / www / html : / var / www / html : z - / etc / apache2 : / etc / apache2 : z - / etc / letsencrypt : / etc / letsencrypt : z tty : true pids_limit : 8192 cap_add : - CAP_NET_RAW # Optional: Let\u2019s Encrypt renewer sharing HTML and certs with testpoint certbot : image : certbot / certbot container_name : certbot network_mode : \"host\" restart : unless - stopped entrypoint : [ \"/bin/sh\" , \"-c\" , \"trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;\" ] depends_on : - testpoint volumes : - / var / www / html : / var / www / html : z - / etc / letsencrypt : / etc / letsencrypt : z ``` Restart with the new compose: ( cd /opt/testpoint ; podman-compose down ) ( cd /opt/testpoint ; podman-compose up -d ) # or docker-compose down && docker-compose up -d Optional \u2013 obtain your first Let\u2019s Encrypt certificate: The certbot sidecar above continuously renews existing certs. For the initial issuance, run a one-shot command and then reload Apache inside the testpoint container: # Issue (HTTP-01 webroot challenge). Replace values accordingly. docker run --rm --net = host -v /var/www/html:/var/www/html -v /etc/letsencrypt:/etc/letsencrypt \\ certbot/certbot certonly --webroot -w /var/www/html -d <SERVER_FQDN> \\ --email <LETSENCRYPT_EMAIL> --agree-tos --no-eff-email # Gracefully reload Apache within the testpoint container (or restart the service) docker exec -it perfsonar-testpoint bash -lc 'systemctl reload httpd || apachectl -k graceful || true' Notes Ensure port 80 on the host is reachable from the internet while issuing certificates. All shared paths use SELinux-aware :z / :Z to permit container access on enforcing hosts. Verify: curl -fsS http://localhost/toolkit/ | head -n 5 docker ps # or podman ps ---","title":"Step 5 \u2013 Deploy the Containerized perfSONAR Testpoint"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-6-register-and-configure-with-wlcgosg","text":"PerfSONAR toolkit configuration: Browse to https://<SERVER_FQDN>/toolkit and complete the local toolkit setup wizard. Configuration details to populate Site contact email Usage policy URL Location (latitude/longitude) Export the configuration via /etc/perfsonar/psconfig/nodes/local.json for record keeping OSG/WLCG registration workflow: Registration steps and portals Register the host in OSG topology . Create or update a GGUS ticket announcing the new measurement point. In GOCDB , add the service endpoint org.opensciencegrid.crc.perfsonar-testpoint bound to this host. pSConfig enrollment: For each active NIC, register with the psconfig service so measurements cover all paths. ??? info \"Registration command and verification\" Example registration : ` `` bash /usr/bin/psconfig psconfig remote --configure-archives add --url https://psconfig.opensciencegrid.org/pub/auto/<NIC-FQDN> `` ` Confirm the resulting files in `/etc/perfsonar/psconfig/pscheduler.d/` map to the correct interface addresses ( `ifaddr` tags ). 1. ** Document memberships :** update your site wiki or change log with assigned mesh names , feed URLs , and support contacts .","title":"Step 6 \u2013 Register and Configure with WLCG/OSG"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#update-lookup-service-registration-inside-the-container","text":"Use the helper script to edit /etc/perfsonar/lsregistrationdaemon.conf inside the running perfsonar-testpoint container and restart the daemon only if needed. Script (browse): perfSONAR-update-lsregistration.sh Raw (download): raw link Install and run examples (root shell): curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-update-lsregistration.sh \\ -o ~/perfSONAR-update-lsregistration.sh chmod 0755 ~/perfSONAR-update-lsregistration.sh # Preview changes only ~/perfSONAR-update-lsregistration.sh --dry-run --site-name \"Acme Co.\" --project WLCG --admin-email admin@example.org --admin-name \"pS Admin\" # Apply common updates and restart the daemon inside the container ~/perfSONAR-update-lsregistration.sh \\ --site-name \"Acme Co.\" --domain example.org --project WLCG --project OSG \\ --city Berkeley --region CA --country US --zip 94720 \\ --latitude 37 .5 --longitude -121.7469 \\ --admin-name \"pS Admin\" --admin-email admin@example.org","title":"Update Lookup Service registration inside the container"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-7-post-install-validation","text":"Perform these checks before handing the host over to operations: System services: Verify Podman and compose services systemctl status podman systemctl --user status podman-compose@perfsonar-testpoint.service Ensure both are active/green. Container health: Check container status and logs podman ps --format 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}' podman logs pscheduler-agent | tail Network path validation: Test network connectivity and routing pscheduler task throughput --dest <remote-testpoint> tracepath -n <remote-testpoint> Confirm traffic uses the intended policy-based routes (check ip route get <dest> ). Toolkit diagnostics: visit the Toolkit UI \u2192 Dashboard \u2192 Host Status to confirm pScheduler, MaDDash, and owamp/bwctl services report healthy. Security posture: Check firewall, fail2ban, and SELinux nft list ruleset | grep perfsonar fail2ban-client status ausearch --message AVC --just-one Investigate any SELinux denials or repeated Fail2Ban bans. LetsEncrypt certificate check: Verify certificate validity openssl s_client -connect <SERVER_FQDN>:443 -servername <SERVER_FQDN> | openssl x509 -noout -dates -issuer Ensure the issuer is Let\u2019s Encrypt and the validity period is acceptable. Reporting: Run perfSONAR diagnostic reports Run the perfSONAR toolkit daily report and send outputs to operations: pscheduler troubleshoot toolkit-system-health","title":"Step 7 \u2013 Post-Install Validation"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#ongoing-maintenance","text":"Schedule quarterly re-validation of routing policy and nftables rules. Apply dnf update monthly and reboot during the next maintenance window. Monitor psconfig feeds for changes in mesh participation. Track certificate expiry ( certbot renew --dry-run ) if you rely on Let\u2019s Encrypt.","title":"Ongoing Maintenance"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#automatic-image-updates-and-safe-restarts","text":"Keep containers current and only restart them when their image actually changes. Auto-update via labels and a systemd timer Add an auto-update label to services in your compose file (both testpoint and certbot if used): services : testpoint : # ... labels : - io.containers.autoupdate=registry certbot : # ... labels : - io.containers.autoupdate=registry This instructs Podman to check the registry for newer images and restart only if an update is pulled. Enable the Podman auto-update timer (runs daily by default): systemctl enable --now podman-auto-update.timer Run ad-hoc when desired and preview: podman auto-update --dry-run podman auto-update Inspect recent runs: systemctl list-timers | grep podman-auto-update journalctl -u podman-auto-update --since \"1 day ago\"","title":"Automatic image updates and safe restarts"},{"location":"personas/quick-deploy/intro/","tags":["quickstart","quick-deploy"],"text":"Quick Deploy \u2014 Overview \u00b6 This folder contains short, tested procedures to get a working service online quickly. Each quickstart focuses on the minimum steps required for a working deployment, along with a short verification checklist and links to optional hardening or automation pages. Start here if you need a working service in a single admin session. For perfSONAR testpoints the canonical quickstart is quickstart-perfsonar-testpoint-v2.md (recommended). What you will find: Minimal quickstarts (install + verify) Brief verification steps and smoke tests Links to automation and optional hardening (firewall, selinux, nftables) If you are doing repeat deployments, use the automated setup examples in automated-setup/ instead of the manual quickstarts.","title":"Quick Deploy \u2014 Overview"},{"location":"personas/quick-deploy/intro/#quick-deploy-overview","text":"This folder contains short, tested procedures to get a working service online quickly. Each quickstart focuses on the minimum steps required for a working deployment, along with a short verification checklist and links to optional hardening or automation pages. Start here if you need a working service in a single admin session. For perfSONAR testpoints the canonical quickstart is quickstart-perfsonar-testpoint-v2.md (recommended). What you will find: Minimal quickstarts (install + verify) Brief verification steps and smoke tests Links to automation and optional hardening (firewall, selinux, nftables) If you are doing repeat deployments, use the automated setup examples in automated-setup/ instead of the manual quickstarts.","title":"Quick Deploy \u2014 Overview"},{"location":"personas/quick-deploy/landing/","tags":"[quickstart, perfSONAR, testpoint]","text":"Quick Deploy \u2014 perfSONAR Testpoint \u00b6 This landing page links to minimal, tested instructions and automation to get a perfSONAR testpoint running quickly. Quickstart (minimal): quickstart-perfsonar-testpoint-v2.md Automated setups and optional features: automated-setup/README.md If you need a working testpoint in a single admin session, follow the quickstart; if you plan repeated deploys, use the automation examples.","title":"Landing"},{"location":"personas/quick-deploy/landing/#quick-deploy-perfsonar-testpoint","text":"This landing page links to minimal, tested instructions and automation to get a perfSONAR testpoint running quickly. Quickstart (minimal): quickstart-perfsonar-testpoint-v2.md Automated setups and optional features: automated-setup/README.md If you need a working testpoint in a single admin session, follow the quickstart; if you plan repeated deploys, use the automation examples.","title":"Quick Deploy \u2014 perfSONAR Testpoint"},{"location":"personas/quick-deploy/automated-setup/","tags":["automation","ansible"],"text":"This directory contains examples and pointers for automated deployment: ansible/ \u2014 playbooks and roles (example skeleton) docker/ \u2014 example container-based testpoint deployments (optional) Use these examples as a starting point and adapt to site policies.","title":"Automated setup"},{"location":"personas/research/architecture/","tags":["architecture","data-pipeline"],"text":"Architecture (summary) \u00b6 perfSONAR agents (testpoints) central collectors (ELK/Elasticsearch ingestion) configuration services (psconfig / psetf) Diagram and detailed flow to follow.","title":"Architecture"},{"location":"personas/research/architecture/#architecture-summary","text":"perfSONAR agents (testpoints) central collectors (ELK/Elasticsearch ingestion) configuration services (psconfig / psetf) Diagram and detailed flow to follow.","title":"Architecture (summary)"},{"location":"personas/research/intro/","tags":["architecture","research"],"text":"Research \u2014 Overview \u00b6 This area is for readers who want to understand system architecture, data pipelines, and development notes. See architecture.md for component diagrams and data-pipeline.md for ingestion/processing details.","title":"Research \u2014 Overview"},{"location":"personas/research/intro/#research-overview","text":"This area is for readers who want to understand system architecture, data pipelines, and development notes. See architecture.md for component diagrams and data-pipeline.md for ingestion/processing details.","title":"Research \u2014 Overview"},{"location":"personas/research/landing/","tags":["architecture","research"],"text":"Research \u2014 Architecture & Rationale \u00b6 This area is for readers who want to understand system architecture, data pipelines, and development notes. See architecture.md and data-pipeline.md for diagrams and ingestion details. Key pages: architecture.md \u2014 component diagrams and responsibilities data-pipeline.md \u2014 how measurements are ingested, stored, and processed tools/ \u2014 scripts, notebooks, and data access helpers If you want to contribute architecture notes or diagrams, add a short summary and attach a PNG/SVG in docs/personas/research/assets/ and link it from the relevant page.","title":"Landing"},{"location":"personas/research/landing/#research-architecture-rationale","text":"This area is for readers who want to understand system architecture, data pipelines, and development notes. See architecture.md and data-pipeline.md for diagrams and ingestion details. Key pages: architecture.md \u2014 component diagrams and responsibilities data-pipeline.md \u2014 how measurements are ingested, stored, and processed tools/ \u2014 scripts, notebooks, and data access helpers If you want to contribute architecture notes or diagrams, add a short summary and attach a PNG/SVG in docs/personas/research/assets/ and link it from the relevant page.","title":"Research \u2014 Architecture &amp; Rationale"},{"location":"personas/troubleshoot/intro/","tags":["troubleshoot","playbook"],"text":"Troubleshooter \u2014 Overview \u00b6 This area contains short triage checklists, playbooks, and example commands to gather the right information fast. Start with triage-checklist-v2.md to collect the host and network facts, then follow a specific playbook for the observed symptom. Key items: triage-checklist-v2.md \u2014 minimal data collection and verification steps playbooks/ \u2014 scenario-based runbooks (e.g., multi-NIC routing, firewall blocks, container issues)","title":"Troubleshooter \u2014 Overview"},{"location":"personas/troubleshoot/intro/#troubleshooter-overview","text":"This area contains short triage checklists, playbooks, and example commands to gather the right information fast. Start with triage-checklist-v2.md to collect the host and network facts, then follow a specific playbook for the observed symptom. Key items: triage-checklist-v2.md \u2014 minimal data collection and verification steps playbooks/ \u2014 scenario-based runbooks (e.g., multi-NIC routing, firewall blocks, container issues)","title":"Troubleshooter \u2014 Overview"},{"location":"personas/troubleshoot/landing/","text":"Troubleshooter \u2014 Triage and Playbooks \u00b6 This landing page points you to triage checklists, playbooks, and common issue pages. Start with the triage checklist and follow the relevant playbook. triage-checklist-v2.md playbooks/","title":"Landing"},{"location":"personas/troubleshoot/landing/#troubleshooter-triage-and-playbooks","text":"This landing page points you to triage checklists, playbooks, and common issue pages. Start with the triage checklist and follow the relevant playbook. triage-checklist-v2.md playbooks/","title":"Troubleshooter \u2014 Triage and Playbooks"},{"location":"personas/troubleshoot/triage-checklist/","tags":["troubleshoot","checklist"],"text":"Quick triage checklist \u00b6 Gather host information hostnamectl cat /etc/os-release uname -a ip -c a Check basic connectivity ping -c 4 <remote-ip-or-host> traceroute -n <remote-ip-or-host> Verify perfSONAR services and containers systemctl status perfsonar-* ps aux | grep perfsonar sudo podman ps || sudo docker ps Check firewall and ports sudo nft list ruleset sudo ss -ltnp Collect logs and measurements Container logs: sudo podman logs perfsonar-testpoint perfSONAR checks: pscheduler tasks --host localhost Use the scenario playbooks in playbooks/ for step-by-step remediation instructions.","title":"Triage checklist"},{"location":"personas/troubleshoot/triage-checklist/#quick-triage-checklist","text":"Gather host information hostnamectl cat /etc/os-release uname -a ip -c a Check basic connectivity ping -c 4 <remote-ip-or-host> traceroute -n <remote-ip-or-host> Verify perfSONAR services and containers systemctl status perfsonar-* ps aux | grep perfsonar sudo podman ps || sudo docker ps Check firewall and ports sudo nft list ruleset sudo ss -ltnp Collect logs and measurements Container logs: sudo podman logs perfsonar-testpoint perfSONAR checks: pscheduler tasks --host localhost Use the scenario playbooks in playbooks/ for step-by-step remediation instructions.","title":"Quick triage checklist"},{"location":"tools/","text":"Link-check tools \u00b6 This folder contains utilities to check and optionally clean up broken links in the docs/ tree. find_and_remove_broken_links.py Scans Markdown files under docs/ for broken local links (relative links whose targets are missing). Writes a human-readable report to docs/BROKEN_LINKS_REPORT.md . By default it does not modify files. Use --remove to back up and patch files in-place. Example (dry run, just report): python docs/tools/find_and_remove_broken_links.py Example (backup and remove broken links): python docs/tools/find_and_remove_broken_links.py --remove --backup-dir docs/.link_check_backups Notes: External (http/https/mailto) links are not checked by default. To enable external HTTP checks add --check-externals . This may be slow and requires network access. Backups are created per-file with timestamped *.bak.YYYYMMDDTHHMMSSZ suffix under the backup directory. After running, inspect docs/BROKEN_LINKS_REPORT.md for the list of broken links and suggested fixes.","title":"Link-check tools"},{"location":"tools/#link-check-tools","text":"This folder contains utilities to check and optionally clean up broken links in the docs/ tree. find_and_remove_broken_links.py Scans Markdown files under docs/ for broken local links (relative links whose targets are missing). Writes a human-readable report to docs/BROKEN_LINKS_REPORT.md . By default it does not modify files. Use --remove to back up and patch files in-place. Example (dry run, just report): python docs/tools/find_and_remove_broken_links.py Example (backup and remove broken links): python docs/tools/find_and_remove_broken_links.py --remove --backup-dir docs/.link_check_backups Notes: External (http/https/mailto) links are not checked by default. To enable external HTTP checks add --check-externals . This may be slow and requires network access. Backups are created per-file with timestamped *.bak.YYYYMMDDTHHMMSSZ suffix under the backup directory. After running, inspect docs/BROKEN_LINKS_REPORT.md for the list of broken links and suggested fixes.","title":"Link-check tools"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/","text":"Title: docs: fix broken links using automated mapping \u00b6 Body \u00b6 Summary \u00b6 Applied docs/tools/link_mapping.json to replace BROKEN-LINK markers with updated targets. Backups were created under docs/.link_check_backups/ . This branch contains changes to documentation where broken links were replaced with mapped targets or annotated when the resource requires authentication. Files changed \u00b6 docs/index.md docs/network-troubleshooting/osg-debugging-document.md docs/perfsonar/install-testpoint.md docs/perfsonar/psetf.md docs/perfsonar/tools_scripts/README.md docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh Details \u00b6 Replaced internal placeholders and gated links with public or stable alternatives where possible using the mapping file docs/tools/link_mapping.json . For external links that returned 401/403 the script replaced them with plain text indicating \"link requires authentication\" rather than removing them. Mailto entries were handled according to the mapping; the applier was run with --remove-mailto earlier to remove leftover mailto links, and mapping can re-add them if desired. Backups \u00b6 Each modified file has a timestamped backup under docs/.link_check_backups/ with suffix .bak.YYYYMMDDTHHMMSSZ . Testing \u00b6 Preview the site locally (mkdocs) to ensure no rendering regressions. Run the docs link-check tool in dry-run to find remaining issues: python docs/tools/find_and_remove_broken_links.py --check-externals Notes / Next steps \u00b6 Please review replaced URLs for accuracy; some mappings point to public equivalents (e.g., kernel docs) that may not be the preferred internal references. If any replacements are incorrect, they can be changed by editing docs/tools/link_mapping.json and re-running the applier. Consider adding a GitHub Actions workflow to run the link-checker on PRs to prevent regressions. Reviewer suggestions \u00b6 Confirm the chosen public replacements for gated resources (Red Hat docs, internal dashboards). Verify that the perfSONAR psetf and ETF references point to appropriate public pages or internal artifacts depending on access requirements.","title":"Title: docs: fix broken links using automated mapping"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#title-docs-fix-broken-links-using-automated-mapping","text":"","title":"Title: docs: fix broken links using automated mapping"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#body","text":"","title":"Body"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#summary","text":"Applied docs/tools/link_mapping.json to replace BROKEN-LINK markers with updated targets. Backups were created under docs/.link_check_backups/ . This branch contains changes to documentation where broken links were replaced with mapped targets or annotated when the resource requires authentication.","title":"Summary"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#files-changed","text":"docs/index.md docs/network-troubleshooting/osg-debugging-document.md docs/perfsonar/install-testpoint.md docs/perfsonar/psetf.md docs/perfsonar/tools_scripts/README.md docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh","title":"Files changed"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#details","text":"Replaced internal placeholders and gated links with public or stable alternatives where possible using the mapping file docs/tools/link_mapping.json . For external links that returned 401/403 the script replaced them with plain text indicating \"link requires authentication\" rather than removing them. Mailto entries were handled according to the mapping; the applier was run with --remove-mailto earlier to remove leftover mailto links, and mapping can re-add them if desired.","title":"Details"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#backups","text":"Each modified file has a timestamped backup under docs/.link_check_backups/ with suffix .bak.YYYYMMDDTHHMMSSZ .","title":"Backups"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#testing","text":"Preview the site locally (mkdocs) to ensure no rendering regressions. Run the docs link-check tool in dry-run to find remaining issues: python docs/tools/find_and_remove_broken_links.py --check-externals","title":"Testing"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#notes-next-steps","text":"Please review replaced URLs for accuracy; some mappings point to public equivalents (e.g., kernel docs) that may not be the preferred internal references. If any replacements are incorrect, they can be changed by editing docs/tools/link_mapping.json and re-running the applier. Consider adding a GitHub Actions workflow to run the link-checker on PRs to prevent regressions.","title":"Notes / Next steps"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#reviewer-suggestions","text":"Confirm the chosen public replacements for gated resources (Red Hat docs, internal dashboards). Verify that the perfSONAR psetf and ETF references point to appropriate public pages or internal artifacts depending on access requirements.","title":"Reviewer suggestions"}]}