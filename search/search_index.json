{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OSG Networking Area","text":"<p>Welcome to OSG Networking! This is your entry point for networking in the Open Science Grid (OSG) and World-wide LHC Computing Grid (WLCG). Whether you're deploying perfSONAR, troubleshooting network issues, or exploring our monitoring infrastructure, we've organized the documentation around common user journeys.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Choose the path that matches your goal:</p>"},{"location":"#deploy-perfsonar","title":"Deploy perfSONAR","text":"<p>Quick, tested steps to deploy perfSONAR for OSG/WLCG monitoring.</p> <p>Choose your deployment type:</p> <ul> <li>Testpoint (Container) - Lightweight, recommended for most sites</li> <li> <p>Toolkit (RPM) - Full-featured with local web UI and archive</p> </li> <li> <p>Time: 30-90 minutes depending on type</p> </li> <li> <p>Skill level: Systems administrator</p> </li> </ul> <p>\u2192 Quick Deploy Guide</p>"},{"location":"#troubleshoot-network-issues","title":"Troubleshoot Network Issues","text":"<p>Triage checklist and playbooks for diagnosing network problems.</p> <ul> <li> <p>Time: Variable</p> </li> <li> <p>Skill level: Network operator/admin</p> </li> </ul> <p>\u2192 Troubleshooting Guide</p>"},{"location":"#understand-the-system","title":"Understand the System","text":"<p>Architecture, data pipelines, and research documentation.</p> <ul> <li> <p>Time: Reading/reference</p> </li> <li> <p>Skill level: Developer/researcher</p> </li> </ul> <p>\u2192 Architecture &amp; Research</p>"},{"location":"#about-osgwlcg-network-monitoring","title":"About OSG/WLCG Network Monitoring","text":"<p>WLCG and OSG jointly operate a worldwide network of <code>perfSONAR</code> agents that provide an open platform for baselining network performance and debugging issues. This monitoring infrastructure is critical for providing visibility into networks and site connectivity.</p> <p>Key capabilities:</p> <ul> <li> <p>Automated bandwidth and latency testing between sites</p> </li> <li> <p>Centralized measurement storage and analytics</p> </li> <li> <p>Integration with WLCG/OSG dashboards and alerting</p> </li> <li> <p>Community-maintained test meshes</p> </li> </ul> <p>Learn more about perfSONAR in OSG/WLCG \u2192</p>"},{"location":"#network-services-data","title":"Network Services &amp; Data","text":"<p>OSG operates an advanced platform to collect, store, publish and analyze network monitoring data from perfSONAR and other sources. All measurements are available via streaming APIs and dashboards:</p> <ul> <li> <p>perfSONAR Infrastructure Monitoring - monitors perfSONAR network health and service availability</p> </li> <li> <p>OSG Network Datastore - distributed ElasticSearch datastore with JSON API (University of Chicago and University of Nebraska)</p> </li> <li> <p>OSG pSConfig Web Admin - centralized test mesh configuration (contact support for access)</p> </li> <li> <p>WLCG Dashboards - comprehensive performance dashboards combining perfSONAR, FTS, and network traffic data</p> </li> <li> <p>Analytics Platform - ElasticSearch/Kibana/Jupyter for analyzing measurements</p> </li> </ul> <p>!!! note \"MaDDash Deprecation\" The legacy MaDDash instance at maddash.aglt2.org is deprecated. Use WLCG Grafana dashboards instead.</p>"},{"location":"#support-and-feedback","title":"Support and Feedback","text":"<p>For network problems:</p> <ol> <li> <p>Start with the Troubleshooting Guide or ToolkitInfo</p> </li> <li> <p>Contact your site's network provider</p> </li> <li> <p>For OSG-specific support: GOC ticket</p> </li> <li> <p>For WLCG-specific support: GGUS ticket to \"WLCG Network Throughput\" or \"WLCG perfSONAR support\"</p> </li> </ol> <p>For perfSONAR questions: perfSONAR user mailing list</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>perfSONAR Documentation | perfSONAR Project</p> </li> <li> <p>ESNet Fasterdata Guide</p> </li> <li> <p>OSG/WLCG Mesh Configuration</p> </li> <li> <p>perfSONAR Infrastructure Monitoring</p> </li> <li> <p>OSG Analytics Platform</p> </li> <li> <p>WLCG Grafana Dashboards</p> </li> </ul>"},{"location":"BROKEN_LINKS_REPORT/","title":"Broken Links Report","text":"<p>Generated: 2025-12-07T16:21:28.859894Z</p> <p>No broken local links detected (external links were not checked).</p>"},{"location":"host-network-tuning/","title":"Host and Network Tuning (EL9)","text":"<p>Documentation Moved</p> <p>This page has been consolidated. Please see the canonical documentation:</p> <p>Fasterdata Host &amp; Network Tuning Guide</p>"},{"location":"host-network-tuning/#quick-reference","title":"Quick Reference","text":"<p>For ESnet Fasterdata recommendations and the <code>fasterdata-tuning.sh</code> script, see:</p> <ul> <li>Fasterdata Tuning Script Documentation \u2014 Complete guide for audit and apply modes</li> <li>ESnet Fasterdata Host Tuning \u2014 Upstream recommendations</li> <li>ESnet Network Tuning \u2014 Network-level guidance</li> <li>ESnet DTN Tuning \u2014 Data Transfer Node specific tuning</li> </ul>"},{"location":"host-network-tuning/#what-this-covers","title":"What this covers","text":"<ul> <li> <p>Kernel networking sysctls: buffers, congestion control, qdisc, MTU probing</p> </li> <li> <p>Tuned profile: <code>network-throughput</code></p> </li> <li> <p>Per-interface checks: ring buffers, GRO/GSO/TSO on, LRO off, checksum on, txqueuelen, fq qdisc</p> </li> <li> <p>Congestion control: prefer <code>bbr</code> (fallback to <code>cubic</code> if unavailable)</p> </li> </ul>"},{"location":"host-network-tuning/#when-to-use-this","title":"When to use this","text":"<ul> <li> <p>perfSONAR testpoints, DTNs, and other dedicated measurement/transfer hosts on EL9</p> </li> <li> <p>Fresh installs or periodic compliance checks against Fasterdata guidance</p> </li> <li> <p>Not for multi-tenant or latency-sensitive hosts without review</p> </li> </ul>"},{"location":"host-network-tuning/#script-fasterdata-tuningsh","title":"Script: <code>fasterdata-tuning.sh</code>","text":"<p>Path: <code>docs/perfsonar/tools_scripts/fasterdata-tuning.sh</code></p> <p>Modes:</p> <ul> <li> <p><code>audit</code> (default): show current settings vs recommendations, no changes</p> </li> <li> <p><code>apply</code>: set recommended values (requires root)</p> </li> </ul> <p>Options:</p> <ul> <li> <p><code>--mode audit|apply</code>: mode selection</p> </li> <li> <p><code>--target measurement|dtn</code>: host type for scaled recommendations (default: <code>measurement</code>)</p> </li> <li> <p><code>--ifaces eth0,eth1</code>: comma-separated interface list (default: auto-detect physical NICs)</p> </li> <li> <p><code>--color</code>: enable color-coded output (green=compliant, yellow=warning, red=critical)</p> </li> </ul>"},{"location":"host-network-tuning/#host-types","title":"Host Types","text":"<p>The script tailors recommendations by host type:</p> <ul> <li> <p>measurement: perfSONAR testpoints, measurement nodes (primary focus)</p> </li> <li> <p>dtn: Data Transfer Nodes (larger buffers for bulk throughput)</p> </li> </ul> <p>Differences:</p> <ul> <li> <p><code>net.core.netdev_max_backlog</code>: measurement uses 500k at 100Gbps; dtn uses 600k</p> </li> <li> <p><code>txqueuelen</code>: measurement targets 10k/15k/20k; dtn targets 12k/18k/25k (per link speed)</p> </li> </ul>"},{"location":"host-network-tuning/#examples","title":"Examples","text":"<p>Audit a measurement host:</p> <pre><code>bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode audit --target measurement\n</code></pre> <p>Apply tuning for a DTN (100Gbps links):</p> <pre><code>sudo bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode apply --target dtn\n</code></pre> <p>Limit to specific NICs:</p> <pre><code>sudo bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode apply --target measurement --ifaces \"ens1f0np0,ens1f1np1\"\n</code></pre>"},{"location":"host-network-tuning/#what-the-script-does","title":"What the script does","text":"<ul> <li> <p>Sysctl: Sets larger rmem/wmem, netdev backlog, <code>default_qdisc=fq</code>, <code>tcp_mtu_probing=1</code>, TCP r/w mem, timestamps/SACK on, low_latency off; prefers <code>bbr</code> if available. Values scale with fastest NIC link speed and target type (measurement vs dtn). Settings written to <code>/etc/sysctl.d/90-fasterdata.conf</code> in apply mode.</p> </li> <li> <p>Tuned: Ensures <code>network-throughput</code> profile if <code>tuned-adm</code> is present.</p> </li> <li> <p>Ethtool persistence (apply mode): Creates or updates <code>/etc/systemd/system/ethtool-persist.service</code> to persist NIC tunings across reboots (ring buffers, offloads, txqueuelen).</p> </li> <li> <p>Driver checks: Detects driver vendor/version (Mellanox, Broadcom, Intel, other); reports kernel/firmware updates available; provides vendor-specific guidance (e.g., \"keep kernel+linux-firmware current\", \"consider NVIDIA OFED for Mellanox\").</p> </li> <li> <p>Interfaces (per NIC, scaled by link speed):</p> </li> <li> <p>Set <code>txqueuelen</code> to \u2265 10k (measurement) or \u2265 12k (dtn); 20k/25k for 100Gbps</p> </li> <li> <p>Set RX/TX rings to driver max (if reported)</p> </li> <li> <p>Enable GRO/GSO/TSO and checksums; disable LRO</p> </li> <li> <p>Replace qdisc with <code>fq</code></p> </li> </ul>"},{"location":"host-network-tuning/#output-and-logs","title":"Output and logs","text":"<p>The script outputs a Host Info section at the top showing:</p> <ul> <li> <p>Hostname (FQDN)</p> </li> <li> <p>OS name/version (from <code>/etc/os-release</code>)</p> </li> <li> <p>Running kernel version</p> </li> <li> <p>System memory (GiB)</p> </li> <li> <p>SMT status (on/off/unavailable); yellow warning if SMT is off (off-topic: helps isolate jitter in measurement contexts, but should be on by default for throughput)</p> </li> </ul> <p>Then it displays sysctl audit and per-NIC summaries, followed by a summary block showing:</p> <ul> <li> <p>Target type (measurement/dtn)</p> </li> <li> <p>Sysctl mismatches (count)</p> </li> <li> <p>Per-interface issues (tx queue, qdisc, offloads, rings)</p> </li> <li> <p>Driver/version actions (kernel updates available, vendor guidance)</p> </li> <li> <p>SMT control guidance (if SMT is off, suggests toggle commands)</p> </li> <li> <p>Missing tools (ethtool, tuned-adm, cpupower)</p> </li> </ul> <p>Detailed log with full ethtool/sysctl/tc output is written to <code>/tmp/fasterdata-tuning-&lt;UTC&gt;.log</code> (configurable via <code>LOGFILE</code> env var).</p>"},{"location":"host-network-tuning/#getting-the-fasterdata-script","title":"Getting the Fasterdata script","text":"<p>You can download the script directly from the GitHub repo (raw) or the site after it is published:</p> <pre><code>https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\nhttps://osg-htc.org/networking/perfsonar/tools_scripts/fasterdata-tuning.sh\n</code></pre> <p>Install quickly as follows:</p> <pre><code>sudo curl -L -o /usr/local/bin/fasterdata-tuning.sh https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\nsudo chmod +x /usr/local/bin/fasterdata-tuning.sh\n</code></pre> <p>Then run an audit before applying changes:</p> <pre><code>bash /usr/local/bin/fasterdata-tuning.sh --mode audit --target measurement\n</code></pre>"},{"location":"host-network-tuning/#color-output","title":"Color Output","text":"<p>Use <code>--color</code> flag to enable ANSI color codes:</p> <ul> <li> <p>Green: settings comply with recommendations</p> </li> <li> <p>Yellow: warning/attention needed (e.g., SMT off, missing tools, suboptimal settings)</p> </li> <li> <p>Red: critical issues requiring immediate attention (not currently used, reserved for future severity levels)</p> </li> </ul>"},{"location":"host-network-tuning/#driver-guidance","title":"Driver Guidance","text":"<p>The script identifies your NIC drivers and provides upgrade paths:</p> <ul> <li> <p>Mellanox/NVIDIA (<code>mlx5_core</code>, <code>mlx4_en</code>): Keep kernel+linux-firmware current or use NVIDIA OFED (https://network.nvidia.com/products/ethernet-drivers/).</p> </li> <li> <p>Broadcom (<code>bnxt_en</code>, <code>tg3</code>, <code>bnx2x</code>, <code>bnx2</code>): Track distro kernel; use vendor firmware tools (e.g., <code>bnxtnvm</code>) when available.</p> </li> <li> <p>Intel (<code>ixgbe</code>, <code>i40e</code>, <code>ice</code>, <code>e1000e</code>, <code>igb</code>): Update kernel+linux-firmware for latest drivers and firmware blobs.</p> </li> </ul> <p>If a kernel update is available, the summary will recommend: <code>dnf update kernel &amp;&amp; reboot</code>.</p>"},{"location":"host-network-tuning/#cautions","title":"Cautions","text":"<ul> <li> <p>Apply mode changes the running system and writes sysctl to <code>/etc/sysctl.d/90-fasterdata.conf</code>.</p> </li> <li> <p>Persistence of ethtool settings: In apply mode, the script automatically creates or updates <code>/etc/systemd/system/ethtool-persist.service</code> to persist NIC tunings (ring buffers, offloads, txqueuelen) across reboots. The service is enabled automatically.</p> </li> <li> <p>Sysctl settings in <code>/etc/sysctl.d/90-fasterdata.conf</code> persist across reboots.</p> </li> <li> <p>SMT control (if changed) requires GRUB config to persist; see SMT section below.</p> </li> <li> <p>Tuned profile changes persist if tuned-adm writes to its default config.</p> </li> <li> <p>The script assumes EL9 userland (<code>sysctl</code>, <code>ethtool</code>, <code>tc</code>, <code>tuned-adm</code>). It skips steps if tools are missing.</p> </li> <li> <p>If <code>bbr</code> is not available, it falls back to <code>cubic</code> live but keeps <code>bbr</code> in the config for future kernels.</p> </li> <li> <p>Always validate after applying: check <code>podman ps</code>/services, run a quick throughput test, and review <code>dmesg</code>/<code>journal</code> for NIC or driver warnings.</p> </li> <li> <p>To verify ethtool persistence service: <code>systemctl status ethtool-persist</code> and <code>systemctl cat ethtool-persist.service</code></p> </li> </ul>"},{"location":"host-network-tuning/#optional-apply-flags","title":"Optional apply flags","text":"<p>The script supports a few additional opt-in apply flags when run with <code>--mode apply</code>:</p> <ul> <li><code>--apply-iommu</code>: Edit GRUB to add recommended <code>iommu=pt</code> plus vendor-specific flags (Intel/AMD) and regenerate grub. Requires root and careful review before committing. Example:</li> </ul> <pre><code>sudo bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode apply --apply-iommu --yes\n</code></pre> <ul> <li><code>--apply-smt on|off</code>: Apply SMT change at runtime. Use <code>--persist-smt</code> to make the choice persistent in GRUB. Example:</li> </ul> <pre><code>sudo bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode apply --apply-smt off --persist-smt --yes\n</code></pre> <p>Preview (dry-run) example:</p> <pre><code>sudo bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode apply --apply-iommu --dry-run\n</code></pre>"},{"location":"host-network-tuning/#manual-checklist-summary-of-recommendations","title":"Manual checklist (summary of recommendations)","text":"<p>Values shown below are baseline (1Gbps). The script scales them by fastest NIC speed and target type.</p> <ul> <li> <p>Sysctl</p> </li> <li> <p><code>net.core.rmem_max</code>/<code>wmem_max</code>: 536M\u20131G (measurement); 1G (dtn at 100Gbps)s)</p> </li> <li> <p><code>net.core.rmem_default</code>/<code>wmem_default</code>: 128M</p> </li> <li> <p><code>net.core.netdev_max_backlog</code>: 250k\u2013500k (measurement); 250k\u2013600k (dtn)</p> </li> <li> <p><code>net.core.default_qdisc</code>: <code>fq</code></p> </li> <li> <p><code>net.ipv4.tcp_rmem</code>: <code>4096 87380 536870912</code></p> </li> <li> <p><code>net.ipv4.tcp_wmem</code>: <code>4096 65536 536870912</code></p> </li> <li> <p><code>net.ipv4.tcp_congestion_control</code>: <code>bbr</code> (or <code>cubic</code> if <code>bbr</code> absent)</p> </li> <li> <p><code>net.ipv4.tcp_mtu_probing</code>: <code>1</code></p> </li> <li> <p><code>net.ipv4.tcp_window_scaling</code>: <code>1</code></p> </li> <li> <p><code>net.ipv4.tcp_timestamps</code>: <code>1</code></p> </li> <li> <p><code>net.ipv4.tcp_sack</code>: <code>1</code></p> </li> <li> <p><code>net.ipv4.tcp_low_latency</code>: <code>0</code></p> </li> <li> <p>Tuned: <code>tuned-adm profile network-throughput</code></p> </li> <li> <p>Interfaces</p> </li> <li> <p><code>txqueuelen</code> \u2265 10000</p> </li> <li> <p>RX/TX rings at driver max (<code>ethtool -g</code> / <code>-G</code>)</p> </li> <li> <p>GRO/GSO/TSO on; checksums on; LRO off</p> </li> <li> <p>qdisc <code>fq</code> (root)</p> </li> </ul>"},{"location":"host-network-tuning/#verification-after-tuning","title":"Verification after tuning","text":"<pre><code>sysctl -a | egrep \"(rmem|wmem|max_backlog|default_qdisc|tcp_congestion_control|tcp_mtu_probing)\"\ntuned-adm active\n</code></pre> <p>For each NIC: <code>ethtool -k &lt;iface&gt;</code>, <code>ethtool -g &lt;iface&gt;</code>, <code>tc qdisc show dev &lt;iface&gt;</code>, <code>cat /sys/class/net/&lt;iface&gt;/tx_queue_len</code></p> <p>Run a representative throughput test (e.g., <code>iperf3</code>) end-to-end.</p>"},{"location":"host-network-tuning/#additional-topics","title":"Additional Topics","text":""},{"location":"host-network-tuning/#smt-simultaneous-multi-threading","title":"SMT (Simultaneous Multi-Threading)","text":"<p>The script detects and reports SMT status. For most measurement hosts (perfSONAR), SMT should be on to maximize CPU throughput. However, for isolated low-latency workloads, SMT off may reduce jitter.</p> <p>To check SMT status:</p> <pre><code>cat /sys/devices/system/cpu/smt/control\n</code></pre> <p>To enable SMT:</p> <pre><code>echo on | sudo tee /sys/devices/system/cpu/smt/control\n</code></pre> <p>To disable SMT:</p> <pre><code>echo off | sudo tee /sys/devices/system/cpu/smt/control\n</code></pre> <p>Note: SMT changes take effect immediately but are not persisted across reboots. To persist, add to kernel command line (GRUB): <code>nosmt</code> (to disable) or remove it (to enable).</p>"},{"location":"host-network-tuning/#driver-updates","title":"Driver Updates","text":"<p>The script checks for available kernel and driver updates via <code>dnf list --showduplicates kernel</code>. If an update is available, the summary recommends:</p> <pre><code>dnf update kernel &amp;&amp; reboot\n</code></pre> <p>For vendor-specific drivers (Mellanox OFED, Broadcom firmware tools), the script provides URLs and guidance. Always validate driver compatibility before updating in production.</p>"},{"location":"host-network-tuning/#troubleshooting","title":"Troubleshooting","text":"<p>If the script fails to detect certain hardware or settings:</p> <ul> <li> <p>Missing <code>ethtool</code>: Install via <code>dnf install ethtool</code></p> </li> <li> <p>Missing <code>tuned-adm</code>: Install via <code>dnf install tuned</code></p> </li> <li> <p>No cpufreq/governor: Some VMs lack CPU frequency scaling; this is normal</p> </li> <li> <p>No IOMMU: Fasterdata recommends <code>iommu=pt</code> generally, with vendor additions: enable in GRUB (e.g., <code>intel_iommu=on iommu=pt</code> for Intel, <code>amd_iommu=on iommu=pt</code> for AMD) if using SR-IOV or isolation features. Add <code>iommu=pt</code> to improve throughput on high-speed NICs.</p> </li> <li> <p>ethtool-persist.service fails to start: Check <code>/var/log/messages</code> or <code>journalctl -u ethtool-persist</code> for errors; ensure ethtool and ip commands exist at <code>/sbin/</code> paths</p> </li> </ul>"},{"location":"host-network-tuning/#persistence-service-details","title":"Persistence Service Details","text":"<p>When running with <code>--mode apply</code>, the script generates <code>/etc/systemd/system/ethtool-persist.service</code> containing:</p> <pre><code>[Unit]\nDescription=Persist ethtool settings (Fasterdata)\nAfter=network.target\nWants=network.target\n\n[Service]\nType=oneshot\nExecStart=/sbin/ethtool -G &lt;iface&gt; rx &lt;max&gt; tx &lt;max&gt;\nExecStart=/sbin/ethtool -K &lt;iface&gt; gro on gso on tso on rx on tx on lro off\nExecStart=/sbin/ip link set dev &lt;iface&gt; txqueuelen &lt;value&gt;\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>The service is automatically enabled. To verify:</p> <pre><code>systemctl status ethtool-persist.service\nsystemctl cat ethtool-persist.service\n</code></pre> <p>To manually update this service, re-run the script with <code>--mode apply</code> to regenerate it with current NIC settings.</p>"},{"location":"host-network-tuning/#persistence-notes","title":"Persistence notes","text":"<ul> <li> <p>Sysctl settings persist via <code>/etc/sysctl.d/90-fasterdata.conf</code></p> </li> <li> <p>NIC ethtool changes do not persist by default; consider:</p> </li> </ul> <ul> <li> <p>running this script from a boot-time systemd unit, or</p> </li> <li> <p>translating the settings into your network manager configuration</p> </li> </ul>"},{"location":"mini-capability-host-tuning/","title":"Host Optimization and Tuning Testing","text":""},{"location":"mini-capability-host-tuning/#capability-summary","title":"Capability Summary","text":"Attribute Value Capability Name Host Optimization and Tuning (Network, Storage, CPU, Memory) Related Framework Items Congestion Control (BBRv3), Jumbo Frames support, Storage performance, CPU affinity Importance 4 (High) Urgency 3 (Medium) Potential Gains 4\u20135 (Significant to Major) Dependencies RHEL 9, 25+ Gbps NICs, NVMe/SSD storage, Linux kernel 5.x+, ethtool, tc, iostat tools Status Mini-Challenge 1 scheduled for January 2026 Framework Reference See WLCG Capability Test Framework"},{"location":"mini-capability-host-tuning/#overview-and-rationale","title":"Overview and Rationale","text":"<p>We evaluate comprehensive host optimizations for RHEL 9 systems with high-speed NICs (25/40/100/200/400 Gbps) and high-performance storage (NVMe/SSD, dCache, XRootD, EOS). This capability addresses WAN (wide-area network) data transfer performance by optimizing host-level network, storage, CPU, and memory subsystems.</p> <p>Primary Goal: Improve site ability to transfer data across the WAN by tuning hosts to better utilize available bandwidth and reduce transfer latency.</p> <p>Scope: Mini-Challenge 1 optimizes two host subsystems that impact WAN data transfer performance: 1. Network Tuning (ESnet Fasterdata recommendations): Sysctl buffers, qdisc pacing, ethtool offloads, NIC ring buffers \u2014 enables full utilization of high-speed links 2. Storage Tuning (New exploration): I/O scheduler selection, storage queue depth, read-ahead, NUMA affinity \u2014 reduces I/O wait that throttles data transfer throughput</p> <p>Note on Prior Work: An earlier test of congestion control protocols (Edoardo, DC24) focused on algorithm selection alone (BBR vs CUBIC) \u2014 no gains observed. This mini-challenge expands scope to: - Sysctl TCP buffer tuning (rmem/wmem scaled by link speed) - Qdisc packet pacing (fq for fair scheduling, tbf for rate capping) - Ethtool offload optimization (GRO/TSO/GSO tuning) - NIC ring buffer and txqueuelen scaling - Storage I/O scheduler tuning (ioscheduler selection for high-speed storage) - Storage queue depth and I/O concurrency optimization - NUMA-aware memory and I/O affinity - Automated audit and state management (safe rollback via <code>fasterdata-tuning.sh</code> and storage tuning tools)</p> <p>The hypothesis is that comprehensive host optimization across all subsystems (not individual components in isolation) provides significant WAN data transfer throughput and latency improvements, with measurable benefit for: - Data-intensive workflows (HTC job input/output staging, HPC dataset transfers, AI/ML training data) - Site data transfer capacity (higher effective bandwidth utilization, fewer stalled transfers) - Operational efficiency (reduced transfer time, lower host CPU utilization per Gbps transferred)</p>"},{"location":"mini-capability-host-tuning/#capability-tracking-table","title":"Capability Tracking Table","text":"<p>Per the WLCG Capability Test Framework, this table tracks all mini-challenges and their status:</p> Mini-Challenge Status Start Date Expected End Key Sites Primary Focus Outcome MC-1: Network + Storage Tuning Planned Jan 10, 2026 Mar 14, 2026 FNAL, UCSD, Nebraska, BNL, AGLT2, NET2 Network (Fasterdata), Storage I/O, CPU affinity TBD MC-2: Host Tuning + Jumbo Frames Planned Q2 2026 TBD TBD (subset MC-1 + new) Jumbo Frames (MTU 9000), Advanced storage TBD"},{"location":"mini-capability-host-tuning/#tracking-history","title":"Tracking History","text":"<p>This document tracks mini-challenge instances. Clone the section below for each new challenge, incrementing N.</p>"},{"location":"mini-capability-host-tuning/#participants","title":"Participants","text":"<p>(Please add your name): Shawn McKee, Eduardo Bach, Eli Dart, Diego Davila, Garhan Attebury, Asif Shaw, Carlos Gamboa, Hiro Ito, Wendy Dronen, Philippe Laurens</p> <p>Participants &amp; Roles - Shawn McKee (AGLT2 / University of Michigan): Lead, test plan owner, dCache, networking and storage expert; organizing mini-capability challenges and central coordination. - [Vacant]: Testing infrastructure, automation, data aggregation (role open; volunteers welcome). - Eduardo Bach (UC San Diego / SuperCC): Network monitoring, dCache and network admin, results validation. - Eli Dart (LBNL / ESnet): Fasterdata and perfSONAR expert; advisory role on network tuning validation. - Diego Davila (UCSD / USCMS T2): Storage and network expert; CMS data transfer configuration and testing lead. - Garhan Attebury (University of Nebraska / USCMS T2): Network and systems expert; site proponent and test operator for Nebraska. - Asif Shaw (Fermilab / USCMS T1): CMS network and systems expert; FNAL site proponent and transfer testing lead. - Carlos Gamboa (BNL / USATLAS T1): BNL dCache manager; storage tuning and compatibility lead. - Hiro Ito (BNL): FTS and ATLAS data transfer expert; transfer orchestration and validation. - Wendy Dronen (AGLT2 / U. Michigan): System administrator and UM site operator at AGLT2. - Philippe Laurens (AGLT2 / Michigan State): System administrator and AGLT2 MSU site operator. - Others: Additional participants may join; list to be updated as volunteers sign up. </p>"},{"location":"mini-capability-host-tuning/#capability-challenge-1-comprehensive-host-optimization-testing-january-2026","title":"Capability Challenge 1: Comprehensive Host Optimization Testing (January 2026)","text":""},{"location":"mini-capability-host-tuning/#overview-and-advantages","title":"Overview and Advantages","text":"<p>Goal: Validate comprehensive host optimization for RHEL 9 systems, including network tuning (Fasterdata), storage I/O tuning, and CPU/memory optimization. Target: high-speed NICs (25/40/100+ Gbps) and high-performance storage (NVMe/SSD).</p> <p>Expected Advantages: 1. Improved throughput (5\u201315% for single and multi-flow transfers, 10%+ for storage-bound workloads) 2. Reduced latency and jitter (more predictable job scheduling, lower tail latencies) 3. Better CPU efficiency (lower CPU utilization per Gbps via fq pacing; improved I/O efficiency via queue tuning) 4. Enhanced storage performance (faster data staging, reduced I/O wait on batch jobs) 5. Operational simplicity (automated audit and rollback via <code>fasterdata-tuning.sh</code> and storage tuning playbooks) 6. Hardware compatibility (validated across Broadcom, Mellanox, Intel NICs; various storage controllers and SSDs) 7. Production-ready tooling (state save/restore for safe testing and deployment)</p> <p>Differentiators from Prior Work: - Previous congestion control test (DC24) focused on algorithm selection alone (BBR vs CUBIC) - This challenge includes full-stack host optimization: sysctl, qdisc, ethtool, NIC ring buffers, storage I/O scheduler, queue depth, NUMA affinity - Comprehensive approach addresses bottlenecks across network, storage, and CPU subsystems</p>"},{"location":"mini-capability-host-tuning/#plan-1-testing-methodology-and-implementation","title":"Plan-1: Testing Methodology and Implementation","text":"<p>Primary Tools:  - ESnet Fasterdata tuning script (<code>fasterdata-tuning.sh</code> v1.3.1+, https://github.com/osg-htc/networking/blob/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh) - Storage tuning playbooks and tools (Ansible for I/O scheduler, queue depth; <code>fio</code> / <code>iozone</code> for benchmarks)</p>"},{"location":"mini-capability-host-tuning/#state-management-saverestore-with-fasterdata-tuningsh","title":"State Management: Save/Restore with <code>fasterdata-tuning.sh</code>","text":"<p><code>fasterdata-tuning.sh</code> provides structured state capture and restore functionality which is central to safe A/B testing of host tuning. Key points:</p> <ul> <li>Save location: saved states are written as JSON to <code>/var/lib/fasterdata-tuning/saved-states/</code> with filenames like <code>&lt;timestamp&gt;-&lt;label&gt;.json</code> (e.g., <code>20251210T143000Z-baseline.json</code>). Backups of modified files are stored in <code>/var/lib/fasterdata-tuning/backups/</code>.</li> <li>Basic commands:</li> <li>Save baseline state: <code>sudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline</code></li> <li>List saved states: <code>sudo /usr/local/bin/fasterdata-tuning.sh --list-states</code></li> <li>Show difference vs current: <code>sudo /usr/local/bin/fasterdata-tuning.sh --diff-state /var/lib/fasterdata-tuning/saved-states/&lt;file&gt;.json</code></li> <li>Restore a saved state: <code>sudo /usr/local/bin/fasterdata-tuning.sh --restore-state /var/lib/fasterdata-tuning/saved-states/&lt;file&gt;.json</code></li> <li>Auto-save before apply: <code>sudo /usr/local/bin/fasterdata-tuning.sh --mode apply --auto-save-before --label pre-apply</code></li> <li>What is captured: sysctl values (and <code>/etc/sysctl.d/90-fasterdata.conf</code>), per-interface settings (MTU, qdisc, txqueuelen, ethtool feature flags, ring settings), <code>ethtool-persist</code> service content, GRUB/kernel cmdline (for IOMMU/SMT), tuned profile, CPU governor/SMT state, and helpful warnings about non-restorable items.</li> <li>File format: JSON with <code>metadata</code>, <code>sysctl</code>, <code>interfaces</code>, <code>grub</code>, <code>tuned</code>, and other sections (human- and machine-readable for test automation).</li> <li>Restore caveats:</li> <li>Restores sysctl and per-interface runtime settings and will restore persistence artifacts (sysctl file, ethtool service). Some changes (GRUB/kernel cmdline) require a reboot to take effect.</li> <li>Ring buffer values may not be fully reversible on hardware that doesn't support previous values; the state file includes warnings.</li> <li>Always validate a restore on a non-production node first.</li> </ul> <p>Recommended usage in the test plan: - Phase 1 (baseline): run <code>--save-state --label baseline</code> and record the saved filename in the test log. - Phase 2 (apply): use <code>--mode apply --auto-save-before --label pre-apply</code> or run a separate <code>--save-state --label post-apply</code> after applying tuning to capture the tuned state. - Between test iterations: use <code>--diff-state</code> to confirm only expected changes were made. - Phase 4 (rollback and validation): run <code>--restore-state</code> with the baseline file, then verify via <code>--diff-state</code> that the system is back to baseline and re-run a short transfer test to ensure behavior returned to baseline. - Archive the state JSON files alongside test logs for reproducibility and postmortem analysis.</p> <p>Add a short verification checklist to each test run to confirm save/restore success: - <code>sudo /usr/local/bin/fasterdata-tuning.sh --list-states</code> shows saved file - <code>sudo /usr/local/bin/fasterdata-tuning.sh --diff-state &lt;file&gt;</code> shows expected diffs - After <code>--restore-state &lt;file&gt;</code>, verify <code>sysctl -n net.core.rmem_max</code> (or another key) equals the baseline value and that <code>tc qdisc show</code> shows baseline qdisc - Run a short (e.g., 10 minute) transfer to confirm baseline behavior restored</p>"},{"location":"mini-capability-host-tuning/#objectives","title":"Objectives","text":"<ol> <li>Validate WAN data transfer performance improvement: Measure real-world data transfer throughput and latency (GridFTP, XRootD, HTTP/WebDAV) with host tuning applied vs. baseline</li> <li>Validate host tuning effectiveness: Use perfSONAR and diagnostic tools (iperf3, fio) to isolate network and storage bottlenecks</li> <li>Quantify host impact: Measure CPU utilization, memory usage, and I/O wait reduction during WAN transfers</li> <li>Test automation reliability: Confirm <code>fasterdata-tuning.sh</code> and storage playbooks complete without errors on production data transfer nodes</li> <li>Evaluate operational cost: Assess prerequisites, deployment time, post-apply stability, resource overhead on production infrastructure (dCache, XRootD, EOS)</li> <li>Document best practices: Provide deployment guidance for network, storage, and CPU/memory optimization tailored to data transfer workloads</li> <li>Identify site-specific constraints: Discover hardware/firmware limitations and storage software compatibility (e.g., dCache with fq pacing, XRootD with I/O scheduler changes)</li> <li>Inform WLCG infrastructure decisions: Gather evidence for whether host tuning should become a recommended best practice</li> </ol>"},{"location":"mini-capability-host-tuning/#key-considerations","title":"Key Considerations","text":"<ul> <li>Minimal risk: Tuning changes are reversible; state save/restore feature enables easy rollback</li> <li>Staged rollout: Start with 1\u20132 dedicated data transfer nodes per site before broader deployment (may be production or pre-production nodes)</li> <li>Production operations: All testing is performed while regular production transfers and operations are underway; tests will introduce additional transfer flows to stress storage and transfer infrastructure. Coordinate test timing and throttling with site operations to avoid unacceptable disruption.</li> <li>Hardware diversity: Include varied NIC types (Broadcom, Mellanox, Intel) and bond/VLAN configurations to validate tool robustness</li> <li>Storage infrastructure: Sites use existing production storage (dCache, XRootD, EOS); no new storage deployment required</li> <li>Real-world workloads: Test WAN transfers using actual data transfer protocols and tools (GridFTP, XRootD, FTS, Rucio) and complementary synthetic stress flows to expose bottlenecks</li> <li>Baseline preservation: Maintain unmodified reference nodes for comparison; measure baseline WAN transfer performance and current production load before tuning</li> </ul>"},{"location":"mini-capability-host-tuning/#requirements","title":"Requirements","text":"<ul> <li>Data Transfer Infrastructure:</li> <li>Production or pre-production data transfer nodes (GridFTP, XRootD, FTS, dCache, EOS)</li> <li>RHEL 9.x systems with 25 Gbps or faster NICs</li> <li>WAN connectivity to remote test endpoints (other WLCG sites or perfSONAR nodes)</li> <li>perfSONAR nodes or testpoints for baseline network validation</li> <li>Data transfer monitoring (FTS logs, XRootD monitoring, GridFTP logs, perfSONAR dashboards)</li> <li>Network:</li> <li>Administrative access to apply sysctl, ethtool, and tc commands on data transfer nodes</li> <li>Ability to test on production or dedicated data transfer VLANs</li> <li>Network diagnostic tools (iperf3, netperf, ping, traceroute, mtr)</li> <li>Storage:</li> <li>Existing production storage (dCache, XRootD, EOS, NFS, or direct-attached NVMe/SSD)</li> <li>I/O scheduler change capability (change via <code>/sys/block/*/queue/scheduler</code>)</li> <li>Storage diagnostic tools (fio, iostat, iotop)</li> <li>General:</li> <li>Systems administration and storage operations time (~6\u20138 hours per site)</li> <li>Centralized communication and result tracking (shared spreadsheet, git repo, weekly syncs)</li> </ul>"},{"location":"mini-capability-host-tuning/#procedure","title":"Procedure","text":""},{"location":"mini-capability-host-tuning/#phase-1-setup-and-baseline-week-12","title":"Phase 1: Setup and Baseline (Week 1\u20132)","text":"<p>WAN Data Transfer Baseline (PRIMARY): 1. Deploy test harness on 3 USCMS + 3 USATLAS sites (6 sites total; 2 data transfer nodes per site) 2. Perform baseline WAN data transfer measurements (no tuning applied):    - GridFTP/FTS transfers: 1-hour sustained transfer to remote WLCG site; measure throughput, transfer time, stalls    - XRootD transfers (if available): Read/write to remote XRootD endpoint; measure throughput and latency    - HTTP/WebDAV (if available): Download/upload tests to remote storage endpoint    - perfSONAR tests: Automated throughput tests to remote perfSONAR nodes; validate link capacity and baseline RTT 3. Monitor host impact during baseline transfers:    - CPU utilization (top, sar) for data transfer process and kernel I/O    - Memory usage and TCP buffer utilization    - I/O wait percentage (iostat, sar)    - Network statistics (ethtool -S, retransmits, drops)   - Record production load: Note baseline production transfer rates and schedule windows; document typical transfer concurrency so added test flows can be interpreted in context4. Capture system configuration (kernel, NIC drivers, firmware, storage software versions) 4. Save baseline state and record the saved filename (example):    - <code>sudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline</code>    - Run <code>sudo /usr/local/bin/fasterdata-tuning.sh --list-states</code> to note the saved filename (e.g., <code>/var/lib/fasterdata-tuning/saved-states/20251210T143000Z-baseline.json</code>) and include it in test logs</p> <p>Network Diagnostic Tests (SECONDARY): 1. Run perfSONAR on-demand tests (iperf3, ping) to validate link capacity and baseline RTT 2. Single-flow and multi-flow TCP throughput (iperf3) to isolate pure network performance 3. Document any observed bottlenecks (CPU saturation, I/O wait, retransmits)</p> <p>Storage Diagnostic Tests (SECONDARY): 1. Identify baseline I/O scheduler for storage devices (cat /sys/block/*/queue/scheduler) 2. Run fio benchmarks on storage paths used by data transfer software:    - Sequential read/write throughput (large block sizes matching transfer tools)    - I/O latency percentiles (p50, p95, p99) 3. Capture storage system configuration (device model, firmware, NUMA topology, dCache/XRootD/EOS configuration)</p>"},{"location":"mini-capability-host-tuning/#phase-2-deployment-week-34","title":"Phase 2: Deployment (Week 3\u20134)","text":"<p>Network Tuning: 1. Run audit on all network test nodes: <code>./fasterdata-tuning.sh --mode audit --target measurement</code> 2. Apply tuning on half of test nodes (one per site): <code>sudo ./fasterdata-tuning.sh --mode apply --target measurement</code> 3. Log all changes to sysctl, ethtool, and tc settings 4. Verify persistence across reboot</p> <p>Storage Tuning: 1. For each storage device, identify best I/O scheduler for your workload (candidates: <code>mq-deadline</code>, <code>noop</code>, <code>bfq</code>):    - Test each scheduler via: <code>echo \"scheduler-name\" | sudo tee /sys/block/*/queue/scheduler</code>    - Run fio benchmarks (same tests as baseline) and compare results 2. Select tuned I/O scheduler based on performance and latency results 3. Apply tuned scheduler to half of storage test nodes (one per site) 4. Optionally apply NUMA affinity (if storage controller visible in numa topology): <code>numactl -m N -C N fio [test]</code> 5. Document selected scheduler and any NUMA settings</p>"},{"location":"mini-capability-host-tuning/#phase-3-performance-testing-week-57-overlaps-phase-2","title":"Phase 3: Performance Testing (Week 5\u20137, overlaps Phase 2)","text":"<p>WAN Data Transfer Performance Testing (PRIMARY - weekly, 2\u20133 iterations per configuration): 1. Real-world data transfer tests:    - GridFTP/FTS transfers: 1\u20132 hour sustained transfers to remote WLCG sites; measure throughput improvement vs. baseline    - XRootD transfers: Read/write to remote endpoints; measure throughput and latency improvement    - HTTP/WebDAV: Upload/download tests; measure throughput and transfer time reduction    - perfSONAR automated tests: Scheduled throughput tests to validate link utilization improvement 2. Host impact measurements (compare tuned vs. baseline):    - CPU utilization during transfers (should decrease with tuning)    - I/O wait percentage (should decrease with storage tuning)    - Memory usage and TCP buffer efficiency    - Network retransmits and packet loss (should stay low or decrease) 3. Data transfer software validation:    - Verify GridFTP/XRootD/FTS continue to operate correctly with tuning applied    - Check for compatibility issues with dCache/XRootD/EOS and fq pacing or I/O scheduler changes    - Monitor transfer logs for errors or performance warnings 4. Stability and long-duration testing:    - Run multi-hour transfers (4+ hours) to detect stability issues    - Monitor system logs (dmesg, syslog) for errors    - Verify no service restarts or transfer failures</p> <p>Global Configuration Sweep (synchronized across all sites):</p> <p>Purpose: Measure the end-to-end impact when all data transfer hosts are placed on the same configuration, then change the entire fleet to a different configuration and repeat the measurements. This avoids partial-path inconsistencies and better isolates host-level effects on WAN transfers.</p> <p>Steps: 1. Prepare configuration variants: Define the configurations to compare (e.g., <code>baseline</code> (stock), <code>network-tuned</code> (Fasterdata apply with fq), <code>network+storage-tuned</code> (Fasterdata + I/O scheduler), <code>tbf-cap</code> (tbf cap + storage tune)). Document the exact commands/playbooks used to apply each. 2. Record baseline: On every test host, save baseline state and record filenames:    - <code>sudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline</code>    - Use <code>sudo /usr/local/bin/fasterdata-tuning.sh --list-states</code> and save the returned filenames centrally (one per host). 3. Apply configuration to all hosts: Use an orchestration tool (Ansible recommended) to run the apply on all data-transfer nodes simultaneously or in a controlled batch. Example Ansible ad-hoc:</p> <pre><code>ansible data-transfer -i inventory -m shell -a \"sudo /usr/local/bin/fasterdata-tuning.sh --mode apply --apply-packet-pacing --yes\"\n</code></pre> <p>For storage changes use an Ansible playbook that sets <code>/sys/block/*/queue/scheduler</code> and any NUMA affinity settings. 4. Confirm successful apply: On all hosts, run an audit to verify the expected changes are in place and collect JSON output to central logging:    - <code>ansible data-transfer -m shell -a \"sudo /usr/local/bin/fasterdata-tuning.sh --mode audit --json\" -o &gt; audit-outputs/&lt;config&gt;-audit.json</code> 5. Run synchronized WAN transfers: Coordinate start times (within 1 minute) across sites and run the signed transfer jobs (GridFTP/FTS/XRootD) for the defined duration. These runs may include additional synthetic/stress transfer flows (short-duration bursts or long-run sustained flows) specifically designed to stress storage and transfer subsystems while production transfers continue. Collect per-host, production, and transfer-system logs and note any service impact or throttling events. 6. Save tuned state: After verification and before heavy testing, save the tuned state on each host:    - <code>sudo /usr/local/bin/fasterdata-tuning.sh --save-state --label network-tuned</code> 7. Repeat for each configuration: Restore baseline or apply the next configuration across all hosts and repeat steps 4\u20136. For restore between configs use <code>--restore-state</code> with the recorded baseline file or the appropriate saved-state file for that configuration. 8. Aggregate and compare: For each configuration, run at least 3 iterations of the transfer tests; aggregate results, compute means and 95% confidence intervals, and perform paired comparisons between configurations to detect statistically significant differences.</p> <p>Verification checklist for each global sweep: - All hosts report the expected audit results (<code>--mode audit --json</code>) for the current configuration - Saved state files are present and recorded centrally for each host - Transfer job start times are synchronized (within 1 minute) across all sites - Record production transfer load and note interactions with added stress flows - Logs (GridFTP, XRootD, FTS, perfSONAR, host metrics) are collected and archived under <code>logs/&lt;config&gt;/</code> - After restore, a short transfer confirms baseline behavior</p> <p>Notes and cautions: - Always test restores on non-production nodes first. If GRUB/cmdline changes are present, schedule a reboot window and test restores with coordination to avoid service disruption. - If any host cannot restore to its previous ring buffer values, document the limitation and exclude the host from the cross-configuration comparison or mark it as a special-case in the analysis.</p> <p>Network Diagnostic Tests (SECONDARY - for troubleshooting): 1. perfSONAR on-demand tests (iperf3) to isolate network performance:    - Single-flow and multi-flow TCP throughput    - Verify BBR congestion control is active and performing correctly    - Confirm fq qdisc packet pacing without breaking VLAN/bond interfaces 2. Network protocol validation:    - Check for unexpected packet loss or reordering (tcpdump sampling)    - Verify NIC offloads (GRO/TSO/GSO) are functioning correctly</p> <p>Storage Diagnostic Tests (SECONDARY - for troubleshooting): 1. Run fio benchmarks to isolate storage performance changes:    - Compare sequential read/write throughput to baseline    - Measure I/O latency improvement with tuned I/O scheduler 2. Monitor storage subsystem during data transfers:    - iostat -x for I/O queue depth and service time    - Verify no storage bottlenecks (queue saturation, throttling)    - Check dCache/XRootD/EOS logs for storage-related warnings</p>"},{"location":"mini-capability-host-tuning/#phase-4-costrisk-assessment-week-8","title":"Phase 4: Cost/Risk Assessment (Week 8)","text":"<p>Network Assessment: 1. Document time to apply network tuning per site (includes audit, apply, testing) 2. Quantify resource overhead (CPU for fq qdisc, memory for larger TCP buffers) 3. Identify any compatibility issues (driver bugs, performance regressions, bond/VLAN issues) 4. Test rollback procedure:    - Restore baseline state: <code>sudo /usr/local/bin/fasterdata-tuning.sh --restore-state /var/lib/fasterdata-tuning/saved-states/&lt;baseline-file&gt;.json</code>    - Confirm restoration with <code>sudo /usr/local/bin/fasterdata-tuning.sh --diff-state /var/lib/fasterdata-tuning/saved-states/&lt;baseline-file&gt;.json</code> (should show no unexpected diffs)    - Reboot if GRUB/kernel cmdline changes were recorded in the state and required for full restoration    - Run a short WAN transfer to verify transfer behavior returned to baseline and log the results</p> <p>Storage Assessment: 1. Document time to identify and apply best I/O scheduler per site 2. Quantify resource overhead (CPU for scheduler changes, memory for queue depth tuning) 3. Identify hardware constraints (e.g., \"Scheduler X not supported on this NVMe firmware\") 4. Test rollback procedure:    - Restore baseline I/O scheduler and configuration (use site tools or <code>echo \"&lt;scheduler&gt;\" | sudo tee /sys/block/*/queue/scheduler</code>)    - Confirm that storage settings match the captured baseline (compare fio short-run results and <code>iostat -x</code> metrics)    - If a storage change required kernel/module reload or other action, document the steps and validate with a short transfer test</p>"},{"location":"mini-capability-host-tuning/#phase-5-analysis-and-reporting-week-910","title":"Phase 5: Analysis and Reporting (Week 9\u201310)","text":"<ol> <li>Aggregate results across 4 sites</li> <li>Calculate throughput/latency improvements (statistical significance testing)</li> <li>Produce site-specific and aggregate reports</li> <li>Recommendations for production deployment</li> </ol>"},{"location":"mini-capability-host-tuning/#metrics-1-measurements-and-monitoring","title":"Metrics-1: Measurements and Monitoring","text":""},{"location":"mini-capability-host-tuning/#primary-metrics-wan-data-transfer-performance","title":"Primary Metrics: WAN Data Transfer Performance","text":"Metric Method Target Success Criterion GridFTP/FTS throughput Real transfers to remote sites Baseline + 10\u201315% Significant improvement XRootD transfer throughput XRootD read/write to remote endpoint Baseline + 10% Improvement or stable HTTP/WebDAV throughput Upload/download to remote storage Baseline + 10% Improvement or stable perfSONAR throughput Automated tests to remote nodes Link capacity - 5% Near-line-rate utilization Transfer completion time GridFTP/FTS transfer duration Baseline - 10% Faster transfers CPU utilization during transfer top, sar (during WAN transfer) &lt;70% at saturation Improved efficiency I/O wait during transfer iostat, sar (%iowait) &lt;20% during transfer Storage not bottleneck Network retransmits ss -ti, ethtool -S &lt;0.01% of packets Stable or improved"},{"location":"mini-capability-host-tuning/#secondary-metrics-network-diagnostic-tests-perfsonar-iperf3","title":"Secondary Metrics: Network Diagnostic Tests (perfSONAR, iperf3)","text":"Metric Method Target Success Criterion Single-flow throughput iperf3 -t 3600 (1 hour) Baseline + 10% Improvement or no regression Multi-flow throughput iperf3 -P 10 (10 parallel) Baseline + 5% Improvement or no regression RTT latency perfSONAR ping, iperf3 Stable vs. baseline No degradation Memory usage (TCP buffers) /proc/meminfo &lt;5% of system RAM Acceptable overhead"},{"location":"mini-capability-host-tuning/#secondary-metrics-storage-diagnostic-tests-fio","title":"Secondary Metrics: Storage Diagnostic Tests (fio)","text":"Metric Method Target Success Criterion Sequential read throughput fio (seq-read, direct I/O) Baseline + 5% Improvement or stable Sequential write throughput fio (seq-write, direct I/O) Baseline + 5% Improvement or stable I/O latency (p50, p99) fio (latency histogram) &lt;10ms (p50) Stable or improved I/O queue depth utilization iostat -x (avgqu-sz) Optimized per scheduler No bottleneck"},{"location":"mini-capability-host-tuning/#operational-metrics","title":"Operational Metrics","text":"<ul> <li>Data transfer software compatibility: GridFTP, XRootD, FTS, dCache, EOS operate correctly with tuning</li> <li>Reboot safety: Tuning persists correctly post-reboot; transfers resume without manual intervention</li> <li>Rollback success: State restore returns system to baseline; data transfer performance returns to baseline</li> <li>Deployment time: Minutes to complete audit and apply tuning on a data transfer node</li> <li>Hardware compatibility: Pass audit on 100% of tested NIC types (Broadcom, Mellanox, Intel) and storage backends</li> <li>Storage I/O scheduler selection: Optimal scheduler documented for dCache, XRootD, EOS on different storage hardware</li> <li>NUMA affinity impact: Measure WAN transfer performance change with I/O process affinity to storage controller NUMA node</li> </ul>"},{"location":"mini-capability-host-tuning/#monitoring-infrastructure","title":"Monitoring Infrastructure","text":"<ul> <li>Data transfer logs: GridFTP transfer logs, XRootD monitoring, FTS dashboard, dCache/EOS admin logs</li> <li>perfSONAR: Automated throughput tests and historical performance data</li> <li>iperf3/netperf: Network diagnostic tests for troubleshooting</li> <li>fio: Storage diagnostic tests for troubleshooting I/O bottlenecks</li> <li>sysstat (sar): CPU, memory, I/O wait, context switches during transfers</li> <li>ethtool -S: NIC statistics (errors, drops, retransmits)</li> <li>iostat -x: Storage I/O queue depth, service time, utilization</li> <li>ss -ti: TCP connection statistics (retransmits, congestion window)</li> <li>systemd journal: ethtool-persist service logs and tuning application errors</li> </ul>"},{"location":"mini-capability-host-tuning/#cost-benefit-1-cost-and-benefit-analysis","title":"Cost-Benefit-1: Cost and Benefit Analysis","text":""},{"location":"mini-capability-host-tuning/#benefit-estimation","title":"Benefit Estimation","text":"<ol> <li>WAN data transfer throughput improvement: If baseline GridFTP transfer is 15 Gbps and tuning achieves 18 Gbps (20% gain on 100 Gbps link):</li> <li>Benefit = <code>(18 - 15) / 15 \u00d7 100 = 20% faster WAN transfers</code></li> <li>Operational impact: For a site transferring 10 PB/year, this saves ~60 days of transfer time annually</li> <li>Cost savings: Reduced transfer duration = lower risk of transfer failures, faster job turnaround</li> <li>Host efficiency improvement: Lower CPU utilization and I/O wait during transfers:</li> <li>Benefit: More headroom for concurrent transfers and batch jobs on same host</li> <li>Operational impact: Can increase number of concurrent FTS transfers per node by 10\u201315%</li> <li>Storage-bound workload improvement: If storage I/O is bottleneck, tuning can improve WAN transfer by reducing I/O wait:</li> <li>Benefit: Transfers no longer stalled waiting for storage read/write</li> <li>Example: XRootD reads complete faster \u2192 higher sustained transfer rate</li> <li>Latency reduction: Lower jitter and fewer retransmissions enable more stable high-speed transfers:</li> <li>Benefit: Fewer transfer failures and restarts</li> <li>Operational impact: Improved success rate for large (multi-TB) file transfers</li> <li>Combined impact (network + storage): For sites with both network and storage tuning, WAN transfer improvements of 15\u201325% are achievable</li> </ol>"},{"location":"mini-capability-host-tuning/#cost-estimation","title":"Cost Estimation","text":"<ol> <li>Personnel: </li> <li>Site admins (network tuning): 3\u20134 hours per site for tuning application and WAN transfer testing</li> <li>Site storage operations (storage tuning): 2\u20133 hours per site for I/O scheduler selection and validation</li> <li>Site data transfer team: 2\u20133 hours per site for GridFTP/XRootD/FTS testing and log analysis</li> <li>Central team: 8\u201312 hours for test coordination, perfSONAR setup, result analysis</li> <li>Total: ~60\u201370 site-hours across 4 sites</li> <li>Infrastructure: </li> <li>Use existing production or pre-production data transfer nodes (GridFTP, XRootD, dCache, EOS)</li> <li>Use existing perfSONAR nodes for baseline validation</li> <li>Use existing WAN connectivity to remote WLCG sites</li> <li>Storage for transfer logs and baseline captures (~500 MB per site)</li> <li>No new hardware or network infrastructure required</li> <li>Runtime: </li> <li>WAN transfer baseline testing: 3\u20134 hours per site (includes multiple transfer tests)</li> <li>Network and storage diagnostic baseline: 2 hours per site</li> <li>Tuning application and reboot validation: 1 hour per site</li> <li>Tuned WAN transfer testing: 4\u20135 hours per site (includes multiple transfer iterations)</li> <li>Total test runtime: ~4 weeks (overlapping phases, allowing time for scheduled WAN transfers)</li> </ol>"},{"location":"mini-capability-host-tuning/#tools-and-cost","title":"Tools and Cost","text":"<ul> <li>Network tuning tools: <code>fasterdata-tuning.sh</code> (open-source, free)</li> <li>Storage benchmarking tools: <code>fio</code>, <code>iozone</code>, <code>bonnie++</code> (open-source, free)</li> <li>System analysis tools: <code>iostat</code>, <code>ethtool</code>, <code>numactl</code> (standard RHEL 9, free)</li> <li>Automation: Ansible playbooks for storage tuning (to be developed; ~4 hours effort)</li> </ul>"},{"location":"mini-capability-host-tuning/#cost-benefit-comparison","title":"Cost-Benefit Comparison","text":"<ul> <li>Scenario A (WAN transfer 15\u201320% improvement, 10 PB/year site):</li> <li>Benefit: 15\u201320% faster WAN transfers = 1.5\u20132 PB more data transferred in same time window OR 50\u201360 days saved annually</li> <li>Cost: ~60\u201370 site-hours for testing + tuning deployment; amortized over 1 year = negligible</li> <li> <p>Recommendation: Deploy to production data transfer nodes</p> </li> <li> <p>Scenario B (WAN transfer 5\u201310% improvement, storage software compatibility concerns):</p> </li> <li>Benefit: Modest WAN transfer improvement; may require monitoring for dCache/XRootD stability</li> <li>Cost: As above + ongoing monitoring overhead</li> <li> <p>Recommendation: Selective deployment; document constraints (e.g., \"dCache pool nodes need specific fq settings\"); deploy where stable</p> </li> <li> <p>Scenario C (No WAN transfer improvement observed, but diagnostic tests show gains):</p> </li> <li>Benefit: iperf3 shows improvement but real transfers don\u2019t \u2192 bottleneck elsewhere (application layer, remote site, storage backend)</li> <li>Cost: As above</li> <li> <p>Recommendation: Document findings; investigate application-layer bottlenecks; defer host tuning until root cause identified</p> </li> <li> <p>Scenario D (Site-specific hardware issue: regression observed):</p> </li> <li>Benefit: None; specific NIC or storage controller incompatible with tuning</li> <li>Cost: As above + troubleshooting time</li> <li>Recommendation: Rollback immediately; document hardware constraint; share with community for awareness</li> </ul>"},{"location":"mini-capability-host-tuning/#contingency","title":"Contingency","text":"<p>If a site observes instability, performance regression, or compatibility issues: - Immediate action: Restore baseline state using <code>--restore-state</code> feature of fasterdata-tuning.sh and storage rollback playbook - Investigation: Capture logs and hardware details (NIC model, storage device, firmware versions) for root cause analysis - Fallback: Tuning is optional; sites can defer deployment until issue is resolved or workaround is identified - Knowledge sharing: Document site-specific constraints (e.g., \"Broadcom BCM5719 driver does not support fq pacing\") for the community</p>"},{"location":"mini-capability-host-tuning/#schedule-1-timeline-january-2026","title":"Schedule-1: Timeline (January 2026)","text":"Phase Duration Dates Deliverables Planning &amp; Coordination 1 week Jan 1\u201310 Site participant list, test plan review, hardware inventory Setup &amp; Baseline 2 weeks Jan 10\u201324 Baseline measurements, system configs captured, baseline states saved Deployment &amp; Testing 4 weeks Jan 24\u2013Feb 21 Tuning applied, weekly test runs, logs and raw data collected Analysis &amp; Reporting 2 weeks Feb 21\u2013Mar 7 Results aggregated, cost-benefit analysis, final report Presentation 1 week Mar 7\u201314 Summary slides for WLCG/LHCONE meetings <p>Key Milestones: - Dec 18, 2025: Submit CHEP 2026 abstract - Jan 10: Participant kickoff call - Jan 24: All sites ready for testing - Feb 7: Interim results discussion - Feb 9\u201313, 2026: Report initial results at ATLAS Software &amp; Computing Week - Mar 14: Final presentation at next WLCG or LHCONE meeting</p> <p>Conference &amp; Publication Milestones: - May 25\u201329, 2026: Present results at CHEP 2026 - Jun 30, 2026: Finalize CHEP paper on results</p>"},{"location":"mini-capability-host-tuning/#team-1-participants-and-responsibilities","title":"Team-1: Participants and Responsibilities","text":""},{"location":"mini-capability-host-tuning/#central-coordination","title":"Central Coordination","text":"<ul> <li>Shawn McKee (U. Michigan / AGLT2): Lead, test plan owner, dCache, network and storage tuning expert; main organizer and contact for mini-challenge logistics</li> <li>[Volunteer Needed]: Testing infrastructure, automation, data aggregation and analysis; test harness lead (volunteer needed)</li> <li>Eduardo Bach (UC San Diego / SuperCC): Network monitoring, perfSONAR coordination and results validation; dCache and network admin</li> <li>Eli Dart (LBNL / ESnet): Fasterdata advisor, perfSONAR and network tuning validation</li> <li>Diego Davila (UCSD / USCMS T2): Storage and CMS data transfer expert; assists with transfer-job setup and validation</li> <li>Hiro Ito (BNL): FTS and transfer orchestration expert; advisor for ATLAS transfer testing</li> </ul>"},{"location":"mini-capability-host-tuning/#uscms-sites-3-sites","title":"USCMS Sites (3 sites)","text":"<ol> <li>T1 Site (Fermilab)</li> <li>Network Proponent: Asif Shaw (FNAL)</li> <li>Storage/Data Transfer Proponent: [Site storage or data transfer engineer]</li> <li>Responsibilities: <ul> <li>Deploy network tuning on data transfer nodes (production or pre-production)</li> <li>Baseline and tuned WAN transfer tests: GridFTP/FTS to remote sites, perfSONAR validation</li> <li>Storage tuning: Identify and apply best I/O scheduler for GridFTP backends</li> <li>Monitor host impact: CPU, I/O wait, retransmits during WAN transfers</li> <li>Document compatibility and any site-specific constraints</li> </ul> </li> <li> <p>Effort: ~7\u20138 hours per site</p> </li> <li> <p>T2 Site (UCSD)</p> </li> <li>Network Proponent: Diego Davila (UCSD)</li> <li>Storage/Data Transfer Proponent: Diego Davila (UCSD)</li> <li>Responsibilities: <ul> <li>Same as Fermilab; focus on UCSD transfer infrastructure and XRootD/CMS workflows</li> <li>Explore NUMA-aware tuning for data transfer processes if applicable</li> <li>Test multi-flow WAN transfers (concurrent FTS jobs) to validate tuning under load</li> </ul> </li> <li> <p>Effort: ~7\u20138 hours per site</p> </li> <li> <p>T3 Site (Nebraska)</p> </li> <li>Network Proponent: Garhan Attebury (University of Nebraska)</li> <li>Storage/Data Transfer Proponent: [Site storage or data transfer engineer]</li> <li>Responsibilities: <ul> <li>Deploy tuning and run baseline/tuned WAN transfer tests; focus on Nebraskan infrastructure</li> <li>Validate multi-site transfer behavior and concurrency under load</li> <li>Document any site-specific constraints or firmware issues</li> </ul> </li> <li>Effort: ~7\u20138 hours per site</li> </ol>"},{"location":"mini-capability-host-tuning/#usatlas-sites-3-sites","title":"USATLAS Sites (3 sites)","text":"<ol> <li>T1 Site (BNL)</li> <li>Network Proponent: [Site admin name]</li> <li>Storage/Data Transfer Proponent: Carlos Gamboa (BNL)</li> <li>Responsibilities: <ul> <li>Deploy on data transfer nodes; focus on stability and rollback validation</li> <li>Baseline and tuned WAN transfer tests: XRootD, GridFTP, or Rucio/FTS transfers to remote sites</li> <li>Storage tuning: Test I/O scheduler options for EOS or dCache storage backend</li> <li>Collaborate with Hiro Ito for FTS orchestration and ATLAS-specific transfer validation</li> <li>Extensive error log collection for dCache/XRootD compatibility analysis</li> <li>Validate rollback procedure does not disrupt production transfers</li> </ul> </li> <li> <p>Effort: ~7\u20138 hours per site</p> </li> <li> <p>T2 Site (AGLT2)</p> </li> <li>Network Proponent: Wendy Dronen (AGLT2)</li> <li>Storage/Data Transfer Proponent: Shawn McKee (AGLT2)</li> <li>Responsibilities: <ul> <li>Same as BNL; additional focus on AGLT2 hardware and mixed NIC compatibility</li> <li>Validate tuning against AGLT2 dCache pools and transfer nodes</li> <li>Document any firmware-specific constraints for storage or NIC drivers</li> </ul> </li> <li> <p>Effort: ~7\u20138 hours per site</p> </li> <li> <p>T3 Site (NET2)</p> </li> <li>Network Proponent: Eduardo Bach (NET2)</li> <li>Storage/Data Transfer Proponent: Eduardo Bach (NET2)</li> <li>Responsibilities: <ul> <li>NET2 site proponent for ATLAS; validate transfer behavior for ATLAS workflows</li> <li>Collaborate on cross-site Global Configuration Sweeps and ATLAS-specific transfer validation</li> </ul> </li> <li> <p>Effort: ~7\u20138 hours per site</p> </li> <li> <p>T2 Site (UC Irvine)</p> </li> <li>Network Proponent: [Site admin name]</li> <li>Storage/Data Transfer Proponent: [Site storage or data transfer engineer]</li> <li>Responsibilities: <ul> <li>Same as BNL; additional focus on hardware compatibility (mixed NIC types, storage controllers)</li> <li>Document firmware-specific constraints for I/O schedulers or fq pacing</li> <li>Test WAN transfers with tuning on mixed hardware to validate portability</li> </ul> </li> <li>Effort: ~7\u20138 hours per site</li> </ol>"},{"location":"mini-capability-host-tuning/#advisory-committee","title":"Advisory Committee","text":"<ul> <li>Eli Dart (LBNL/ESnet): Network tuning (Fasterdata) validation and guidance; perfSONAR expertise</li> <li>Dale Carder (LBNL): Storage and network architecture guidance</li> <li>Hiro Ito (BNL): FTS and ATLAS transfer orchestration and validation</li> <li>[TBD WLCG Data Transfer Expert]: GridFTP, XRootD, FTS, and Rucio expertise; data transfer best practices</li> <li>[TBD Storage Software Expert]: dCache, XRootD, EOS tuning and compatibility guidance</li> <li>Additional WLCG Network Operations, Storage, and Data Management contacts as needed</li> </ul>"},{"location":"mini-capability-host-tuning/#communication","title":"Communication","text":"<ul> <li>Weekly syncs: Mondays 2 PM ET during active testing phases (Jan\u2013Mar 2026)</li> <li>Shared spreadsheet: Track WAN transfer tests, measurement results (GridFTP/XRootD throughput, perfSONAR tests, host CPU/I/O wait), issues, and blockers</li> <li>Central repo: All logs (GridFTP, XRootD, FTS, perfSONAR, fio, iperf3), scripts, Ansible playbooks, and analysis pushed to <code>/root/Git-Repositories/networking/</code> branch <code>mini-challenge-1</code></li> <li>Escalation: Any blocker (transfer failures, data corruption, dCache/XRootD errors, driver bug) reported immediately to central coordination team</li> <li>Data transfer dashboards: Use existing FTS, perfSONAR, and site monitoring dashboards to track real-time performance</li> </ul>"},{"location":"mini-capability-host-tuning/#evaluation-criteria-1-success-metrics-and-decision-framework","title":"Evaluation Criteria-1: Success Metrics and Decision Framework","text":""},{"location":"mini-capability-host-tuning/#success-criteria","title":"Success Criteria","text":"<ol> <li>\u2705 Data quality: Baseline and tuned measurements completed for \u226580% of planned test cycles (network and storage)</li> <li>\u2705 No regressions: Throughput does not decrease by &gt;5% after tuning; latency does not increase by &gt;10% (or isolated to specific hardware)</li> <li>\u2705 Stability: All nodes complete test phases without unplanned reboots or service errors</li> <li>\u2705 Reproducibility: Tuning changes are consistently applied and measured across all sites</li> <li>\u2705 Cost acceptable: Total site effort \u226425 hours per site (network + storage)</li> <li>\u2705 Storage insights: I/O scheduler recommendations documented for each site's hardware</li> </ol>"},{"location":"mini-capability-host-tuning/#gono-go-decision-framework","title":"Go/No-Go Decision Framework","text":"<ul> <li>Go for production recommendation: If \u22652 sites show \u22655% improvement in network throughput OR \u22655% improvement in storage I/O, with no major regressions</li> <li>Conditional recommendation: If improvement is site-specific or hardware-specific (e.g., NVMe benefits from noop, SSD from mq-deadline); recommend per-site evaluation and document constraints</li> <li>Partial recommendation: If only network OR only storage shows improvement; recommend for the subsystem showing gains; defer other pending resolution</li> <li>No recommendation: If no improvement observed or significant regressions encountered; document findings and defer for future kernel/driver versions</li> </ul>"},{"location":"mini-capability-host-tuning/#results-1-summary-and-follow-up","title":"Results-1: Summary and Follow-Up","text":"<p>To be completed by March 14, 2026.</p> <ul> <li>Aggregate WAN data transfer throughput improvement: [TBD] (GridFTP, XRootD, FTS)</li> <li>perfSONAR throughput improvement: [TBD]</li> <li>Host efficiency improvement (CPU utilization, I/O wait reduction): [TBD]</li> <li>Sites deploying network tuning to production: [TBD]</li> <li>Sites deploying storage tuning to production: [TBD]</li> <li>Data transfer software compatibility findings: [TBD] (dCache, XRootD, EOS, GridFTP, FTS)</li> <li>Key lessons learned: [TBD]</li> <li>I/O scheduler recommendations by storage software and hardware: [TBD]</li> <li>NUMA affinity impact on WAN transfers: [TBD]</li> <li>Recommended next steps: [TBD]</li> <li>Full report location: <code>/root/Git-Repositories/networking/docs/reports/mini-challenge-1-final-report.md</code> (or similar)</li> </ul>"},{"location":"mini-capability-host-tuning/#capability-challenge-n1","title":"Capability Challenge N+1","text":"<p>\u2026</p>"},{"location":"network-troubleshooting/","title":"Network and Network Troubleshooting Documentation","text":"<p>This page will eventually include pointers to various network troubleshooting resources. In general, when users suspect a network problem the procedure is:</p> <ul> <li> <p>Document the problem: Basically you need to describe the problem you encountered. Provide any relevant details like the exact command you used, any errors or warning you got and any problems you observed. This can help you better understand the problem and will allow you to easily \"hand-off\" troubleshooting to an expert</p> </li> <li> <p>Gather relevant data: Run tests (see Guide below) and capture the results.</p> </li> <li> <p>Contact your local network support (this is sometimes where users don't know where to go). Google or your campus web-pages should be able to help you find the right contact.</p> </li> <li> <p>Escalate, if the problem persists (See below)</p> </li> </ul> <p>ESnet maintains a very useful page on network troubleshooting at &lt;https://fasterdata.es.net/performance- testing/troubleshooting/&gt;</p> <p>We have a (older) draft version of the OSG network debugging document that describes things in much more detail. If you have comments, questions or suggestions, please contact Shawn McKee.</p> <p>If you want to learn more about perfSONAR and its various components, the Network Startup Resource Center maintains a list of training videos at https server learn.nsrc.org/perfsonar.</p>"},{"location":"network-troubleshooting/#information-on-contacting-network-support","title":"Information on Contacting Network Support","text":"<p>There are numerous regional and campus network operations centers that have their own ticketing systems for reporting problems. I encourage you to identify how to contact your local campus network support personnel. Suggestion: try using Google with \"YourUniversity network trouble ticket\" as the search terms (of course substitute your university name for YourUniversity).</p> <p>Below are the links you can use to report problems to OSG or the major Research and Education networks. If you can't determine who your local contact should be or they are unable to help you resolve the issue, you should open a ticket with one of these entities:</p> <p>INTERNET2: If you are located at a University in the United States you are likely connected to Internet2. You can find the details on opening a ticket with Internet2 at:</p> <ul> <li>https://noc.net.internet2.edu/internet2/help/index.html</li> </ul> <p>This is typically a good choice for support beyond your campus.</p> <p>ESNET: If your problem involves a national laboratory in the United States, a university connected to ESnet or a trans-Atlantic network problem, then you may want to contact ESnet:</p> <ul> <li>http://es.net/about/contact-us/</li> </ul> <p>This page contains information on opening ticket and pointers to relevant tools and documentation. ESnet serves the Department of Energy national labs.</p> <p>OSG: If you have questions or unusual problems you think are OSG related, feel free to contact the OSG GOC. You can also open network related tickets and OSG can help to 'triage' your request and get it to the right people:</p> <ul> <li>https://support.opensciencegrid.org/support/home</li> </ul> <p>WLCG: WLCG has a dedicated support unit, which will assist in debugging the problem and triaging the issue. More details can be found at:</p> <ul> <li>https://twiki.cern.ch/twiki/bin/view/LCG/NetworkTransferMetrics#Network_Throughput_Support_Unit</li> </ul>"},{"location":"osg-network-analytics/","title":"OSG Network Analytics","text":"<ul> <li> <p>OSG Analytics platform http://atlas-kibana.mwt2.org/s/networking/app/kibana#/dashboard/Default?_g=()</p> </li> <li> <p>WLCG dashboards https://monit-grafana-open.cern.ch/d/000000523/home?orgId=16</p> </li> </ul>"},{"location":"osg-network-services/","title":"OSG Network Services","text":"<p>Open Science Grid is providing a number of network-related services to support its members and collaborators in monitoring, understanding, measuring, diagnosing and managing the networks used to support their work.</p> <p>Here we will provide an overview of the OSG network services and where to find more information.</p> <p>Much of the work to create effective analytics is being done in the NSF funded SAND project which is exploring technologies and methodologies to analyze, summarize and visualize the various networking metrics we have available in the OSG networking data pipeline. As some of these tools and dashboards become production ready, we will add them to this documentation.</p>"},{"location":"perfsonar-in-osg/","title":"Overview of perfSONAR","text":"<p>For those not familiar with <code>perfSONAR</code>, this page provides a quick overview of what it is and why we recommend its deployment at OSG and WLCG sites.</p> <p>OSG is working to support the scientific networking needs of it's constituents and collaborators. To do this, we are recommending all sites deploy perfSONAR so we can measure, monitor and diagnose the OSG (and WLCG) networks.</p>"},{"location":"perfsonar-in-osg/#motivation","title":"Motivation","text":"<p>Distributed scientific computing relies upon networks to interconnect resources and make them usable for scientific workflows. This dependency upon the network means that issues in our networks can significantly impact the behavior of all the various cyber-infrastructure components that rely upon it. Compounding the problem is that networks, by their nature, are distributed and typically involve many different \"owners\" and administrators. When a problem arises somewhere along a network path, it can be very difficult to identify and localize.</p> <p>This was the context for the formation of the perfSONAR collaboration. This collaboration is focused on developing and deploying the <code>perfSONAR</code> software suite in support of network monitoring across the global set of research and education (R&amp;E) networks. The Open Science Grid (OSG) has chosen to base the core of its network monitoring framework on <code>perfSONAR</code> because of both the capabilities of the toolkit for measuring our networks and its global acceptance as the defacto network monitoring infrastructure of first choice.</p> <p>The &lt;https://www.perfsonar.net/about/what-is-perfsonar/&gt; provides a succinct summary: perfSONAR is a network measurement toolkit designed to provide federated coverage of paths, and help to establish end-to-end usage expectations. There are 1000s of perfSONAR instances deployed world wide, many of which are available for open testing of key measures of network performance. This global infrastructure helps to identify and isolate problems as they happen, making the role of supporting network users easier for engineering teams, and increasing productivity when utilizing network resources.</p> <p>How can OSG/WLCG members and collaborators understand, maintain and effectively utilize the networks that form the basis of their distributed collaborations?</p> <p>Our answer starts by providing visibility into our networks by the deployment of <code>perfSONAR</code>. perfSONAR allows us to regularly and consistently measure a set of network metrics that we can use to understand how our networks are operating. When problems arise, the data, along with access to the <code>perfSONAR</code> tools, can be used to diagnose and localize problems. The presence of perfSONAR toolkit deployments across our sites and networks makes identifying and fixing network problems feasible.</p> <p>We strongly recommend that all OSG (and WLCG) sites deploy <code>perfSONAR</code> instances. You can choose between:</p> <ul> <li>Testpoint (container-based) - Recommended for most sites, lightweight deployment</li> <li>Toolkit (RPM-based) - For sites needing local web UI and measurement archive</li> </ul> <p>Before installing, consult the requirements and deployment models guidance.</p> <p>See the Quick Deploy landing page for a comparison and installation guides.</p> <p>Note</p> <p>more effectively support those sites if network issues are suspected.</p> <p>All OSG and WLCG sites should deploy two <code>perfSONAR</code> instances: one to measure latency/packet loss and one to measure bandwidth. It is possible to install both versions on a single host with at least two NICs by following the instructions at multiple NIC guidance page.</p> <p>!!! warning It is very important that the perfSONAR instances be located in the same subnet as the primary storage for the site. This is to ensure that we are measuring as much of the network path involved with data transfer as possible.</p> <p>The WLCG Network Throughput Working Group is responsible for monitoring the WLCG/OSG instances and for defining and maintaining the mesh-configurations that we use to control perfSONAR testing. Please contact us if you have questions or suggestions related to perfSONAR testing amongst WLCG sites.</p> <p>For anyone maintaining/using perfSONAR we suggest to join either/both of the following mailing lists:</p> <ul> <li> <p>User's Mailing List The perfSONAR project maintains a mailing list for communication on matters of installation, configuration, bug reports, or general performance discussions: https://lists.internet2.edu/sympa/subscribe/perfsonar-user</p> </li> <li> <p>Announcement Mailing List The perfSONAR project also maintains a low volume mailing list used for announcements related to software updates and vulnerabilities: https://lists.internet2.edu/sympa/subscribe/perfsonar-announce</p> </li> </ul>"},{"location":"perfsonar-in-osg/#changes-for-perfsonar-50","title":"Changes for perfSONAR 5.0","text":"<p>The first release of perfSONAR 5.0.0 was available on April 17, 2023, followed by a version supporting EL8/EL9 on June 21, 2023 (version 5.0.3). All sites following our recommendation of having auto-updates enabled should have upgraded during within 1-2 days after the releases.   Version 5 marks a transition for the OSG/WLCG perfSONAR deployment, enabling us to migrate to a new network data pipeline where perfSONAR hosts directly send their measurement data to our central Elasticsearch instance via an HTTP-Archiver and Logstash.</p> <p>Highlights include:</p> <ul> <li> <p>Use of Opensearch for the measurement archive, replacing ESmond</p> </li> <li> <p>Support for new OSes, include EL8 and EL9.</p> </li> <li> <p>No longer supports ISO install option.</p> </li> <li> <p>Significant number of bug fixes and new features.</p> </li> </ul> <p>We recommend deploying or updating to the latest version available (v5.2.3 as of October 2025).</p> <p>For a more complete list of changes, see the full release notes at &lt;https://www.perfsonar.net/docs_releasenotes.html&gt;</p>"},{"location":"quick-reference/","title":"\ud83d\ude80 Quick Reference Card","text":"<p>One-page cheat sheet for perfSONAR deployment, configuration, and troubleshooting.</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#essential-contacts","title":"Essential Contacts","text":"<p>| Scenario | Contact | |----------|---------| | OSG Site Issues | GOC Support | | WLCG Issues | GGUS Ticket \u2192 \"WLCG perfSONAR support\" | | perfSONAR Questions | User Mailing List | | Local Network | Your site's network administrator |</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#deployment-quick-start","title":"Deployment Quick Start","text":"","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":"<ul> <li> <p>[ ] EL9 OS installed (AlmaLinux, Rocky, RHEL)</p> </li> <li> <p>[ ] Hostname set: <code>hostnamectl set-hostname &lt;name&gt;</code></p> </li> <li> <p>[ ] Time sync enabled: <code>systemctl enable --now chronyd</code></p> </li> <li> <p>[ ] Network interfaces documented: <code>nmcli device status</code></p> </li> <li> <p>[ ] Required packages: <code>dnf install -y podman podman-compose git curl dig nft</code></p> </li> </ul>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#orchestrated-deploy-recommended","title":"Orchestrated Deploy (Recommended)","text":"<pre><code># Download and run\ncurl -fsSL \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-orchestrator.sh \\\n  -o /tmp/perfSONAR-orchestrator.sh\nchmod 0755 /tmp/perfSONAR-orchestrator.sh\n\n# Interactive (pauses at each step)\n/tmp/perfSONAR-orchestrator.sh\n\n# Non-interactive (auto-confirm all)\n/tmp/perfSONAR-orchestrator.sh --non-interactive --option A\n\n# With Let's Encrypt\n/tmp/perfSONAR-orchestrator.sh --option B --fqdn &lt;FQDN&gt; --email &lt;EMAIL&gt;\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#post-deploy-validation","title":"Post-Deploy Validation","text":"<pre><code># Verify services running\nsystemctl status perfsonar-testpoint\n\n# Check container\npodman ps | grep perfsonar\n\n# Verify pSConfig enrollment\npsconfig remote list\n\n# List scheduled tests\npscheduler tasks --host localhost\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#required-ports-firewall","title":"Required Ports &amp; Firewall","text":"<p>| Port | Protocol | Purpose | Allow From | |------|----------|---------|-----------| | 443 | HTTPS | pScheduler (required) | All perfSONAR nodes | | 5001 | TCP/UDP | iperf (bandwidth) | Mesh nodes | | 8080 | HTTP | pSConfig config | All (or 443) | | 9000 | TCP | Logging | Central server |</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#open-firewall-nftables","title":"Open Firewall (nftables)","text":"<pre><code># Add rule to allow 443\nsudo nft add rule inet filter input tcp dport 443 accept\n\n# Verify\nnft list table filter\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#common-commands","title":"Common Commands","text":"","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#container-management","title":"Container Management","text":"<pre><code># Status\npodman ps -a | grep perfsonar\nsystemctl status perfsonar-testpoint\n\n# View logs\npodman logs perfsonar-testpoint\npodman logs -f perfsonar-testpoint          # follow\n\n# Restart\nsystemctl restart perfsonar-testpoint\n\n# Stop/Start\nsystemctl stop perfsonar-testpoint\nsystemctl start perfsonar-testpoint\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#pscheduler-tests","title":"pScheduler &amp; Tests","text":"<pre><code># List all tasks\npscheduler tasks --host localhost\n\n# View scheduled tests (JSON format)\npscheduler tasks --host localhost --format json\n\n# Run manual test\npscheduler task add --host &lt;local&gt; --dest &lt;remote&gt; \\\n  --test-type latencybg\n\n# Check pScheduler status\nsystemctl status perfsonar-pscheduler-agent\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#network-configuration","title":"Network Configuration","text":"<pre><code># View interfaces\nnmcli device status\nip -br addr\n\n# Check routing\nip route show\nip rule show\n\n# View firewall rules\nnft list ruleset\n\n# Check listening ports\nss -ltnp | grep -E '(443|5001)'\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#host-tuning","title":"Host Tuning","text":"<pre><code># Download tuning script\ncurl -fsSL \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh \\\n  -o /tmp/fasterdata-tuning.sh\nchmod 0755 /tmp/fasterdata-tuning.sh\n\n# Audit (no changes)\n/tmp/fasterdata-tuning.sh audit\n\n# Apply tuning\nsudo /tmp/fasterdata-tuning.sh apply\n\n# For DTN (large buffers)\nsudo /tmp/fasterdata-tuning.sh apply --target dtn\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#ls-registration","title":"LS Registration","text":"<pre><code># Update registration\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh update\n\n# Auto-enroll in mesh\n/opt/perfsonar-tp/tools_scripts/perfSONAR-auto-enroll-psconfig.sh \\\n  --fqdn $(hostname -f) \\\n  --profile latency\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#troubleshooting-quick-checklist","title":"Troubleshooting Quick Checklist","text":"<pre><code># 1. System info\nhostnamectl\ncat /etc/os-release\nuname -a\n\n# 2. Connectivity\nping -c 3 8.8.8.8\nping -c 3 psconfig.opensciencegrid.org\n\n# 3. Container status\npodman ps -a\npodman logs perfsonar-testpoint | head -50\n\n# 4. Services\nsystemctl status perfsonar-*\nsystemctl status podman\n\n# 5. Network\nip -br addr\nnetstat -ltnp | grep -E '(443|5001|8080)'\nnft list ruleset | head -20\n\n# 6. pScheduler\npsconfig remote list\npscheduler tasks --host localhost\n\n# 7. DNS\ndig psconfig.opensciencegrid.org\nnslookup $(hostname -f)\n\n# 8. Firewall test\ncurl -v https://psconfig.opensciencegrid.org/\nnc -zv &lt;remote_testpoint&gt; 443\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#configuration-files","title":"Configuration Files","text":"<p>| File | Purpose | |------|---------| | <code>/etc/perfsonar/</code> | Config backups (from legacy toolkit) | | <code>/opt/perfsonar- tp/docker-compose.yml</code> | Container definition | | <code>/opt/perfsonar-tp/tools_scripts/</code> | Helper scripts | | <code>/etc/NetworkManager/conf.d/</code> | NIC configuration (if using PBR) | | <code>/etc/nftables.conf</code> | Firewall rules | | <code>~/.ssh/authorized_keys</code> | SSH keys |</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#log-locations","title":"Log Locations","text":"<p>| Source | Location | |--------|----------| | Container | <code>podman logs perfsonar-testpoint</code> | | Systemd | <code>journalctl -u perfsonar-testpoint -f</code> | | Host | <code>/var/log/messages</code> (EL9) | | Kernel | <code>dmesg</code> | | Firewall | <code>dmesg \\| grep -i nft</code> |</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#documentation-links","title":"Documentation Links","text":"<p>| Topic | Link | |-------|------| | Installation | Quick Deploy Guide | | Troubleshooting | Troubleshooter Guide | | Host Tuning | Fasterdata Tuning Tuning](host-network-tuning.md) | | Architecture | System Overview | | Tools | Tools &amp; Scripts | | FAQ | perfSONAR FAQ | | Official Docs | docs.perfsonar.net |</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#performance-benchmarks","title":"Performance Benchmarks","text":"","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#expected-bandwidth","title":"Expected Bandwidth","text":"<p>| Link Speed | Expected Throughput | |------------|-------------------| | 1 Gbps | 900+ Mbps | | 10 Gbps | 9+ Gbps | | 100 Gbps | 90+ Gbps |</p> <p>Depends on testpoint tuning, network conditions, and competing tests.</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#normal-latency","title":"Normal Latency","text":"<ul> <li> <p>Local campus network: &lt; 5 ms</p> </li> <li> <p>Same region: 10-50 ms</p> </li> <li> <p>Cross-country: 50-150 ms</p> </li> <li> <p>Transatlantic: 100-200 ms</p> </li> </ul>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#emergency-procedures","title":"Emergency Procedures","text":"","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#container-wont-start","title":"Container Won't Start","text":"<pre><code># 1. Check logs\npodman logs perfsonar-testpoint\n\n# 2. Verify image\npodman images | grep perfsonar\n\n# 3. Free disk space\npodman system prune -a\n\n# 4. Restart service\nsystemctl restart perfsonar-testpoint\n\n# 5. Check ports\nss -ltnp | grep -E '(443|5001)'\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#container-lost-network","title":"Container Lost Network","text":"<pre><code># 1. Check network\nip link show\nnmcli device status\n\n# 2. Restart network\nsystemctl restart NetworkManager\n\n# 3. Restart container\nsystemctl restart perfsonar-testpoint\n\n# 4. Verify routes\nip route show\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#tests-not-running","title":"Tests Not Running","text":"<pre><code># 1. Check enrollment\npsconfig remote list\n\n# 2. Verify connectivity\ncurl -v https://psconfig.opensciencegrid.org/\n\n# 3. Check pScheduler\nsystemctl status perfsonar-pscheduler-agent\npscheduler tasks --host localhost\n\n# 4. Restart all services\nsystemctl restart perfsonar-testpoint\n\n# 5. Escalate\n# Contact: [Troubleshooter Guide](personas/troubleshoot/landing.md)\n</code></pre>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<p>| Issue | Quick Fix | |-------|-----------| | Port 443 in use | <code>ss -ltnp \\| grep 443</code> \u2192 kill process | | Volume permission denied | <code>sudo chown 65534:65534 /volume/path</code> | | DNS not resolving | <code>systemctl restart systemd-resolved</code> | | firewall blocking | <code>sudo nft add rule inet filter input tcp dport 443 accept</code> | | High latency | Run: <code>/tmp/fasterdata-tuning.sh apply</code> | | Tests scheduled but not running | <code>pscheduler tasks</code> \u2192 check network connectivity | | Cannot reach remote testpoint | Check firewall on both ends, verify 443 is open |</p>","tags":["reference","cheat-sheet","commands"]},{"location":"quick-reference/#version-information","title":"Version Information","text":"<ul> <li> <p>Quick Deploy: v1.4.0+</p> </li> <li> <p>perfSONAR: 5.0.3+</p> </li> <li> <p>OS: EL9 (AlmaLinux, Rocky, RHEL)</p> </li> <li> <p>Container: Podman/Docker</p> </li> </ul> <p>Last Updated: December 2025 For issues: troubleshooter landing or support contacts</p>","tags":["reference","cheat-sheet","commands"]},{"location":"web-site-management/","title":"Website Management &amp; Operations","text":"<p>This page outlines recommended processes for building, publishing and managing the OSG Networking site (MkDocs) with a focus on persona-based documentation. The goal is to maintain <code>docs/</code> as the single source of truth and manage generated <code>site/</code> through CI.</p>"},{"location":"web-site-management/#overview","title":"Overview","text":"<ul> <li> <p>Use <code>docs/</code> as canonical source of content.</p> </li> <li> <p>Build &amp; verify with <code>mkdocs build</code> locally or via CI.</p> </li> <li> <p>Use CI to deploy to <code>gh-pages</code> or a hosted platform; avoid committing generated <code>site/</code> to the repo when possible.</p> </li> </ul>"},{"location":"web-site-management/#local-development-testing","title":"Local development &amp; testing","text":"<ol> <li>Create a Python virtualenv and install dependencies:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -U pip\npip install mkdocs mkdocs-material pymdown-extensions\n</code></pre> <ol> <li>Run a local preview server:</li> </ol> <pre><code>mkdocs serve -a 0.0.0.0:8000\n</code></pre> <ol> <li>Build the site for local verification:</li> </ol> <pre><code>mkdocs build --clean -d site\n</code></pre>"},{"location":"web-site-management/#ci-publishing","title":"CI &amp; Publishing","text":"<ol> <li> <p>CI should run the following steps on PRs and pushes to <code>master</code>:</p> </li> <li> <p><code>mkdocs build --clean</code> (fail on build errors)</p> </li> <li> <p>Run link checks and the <code>verify-site-scripts.sh</code> script to assert docs/site parity for changed files (optional if not keeping <code>site/</code> in repo)</p> </li> <li> <p>Run <code>autoupdate-scripts-sha.sh</code> to update <code>*.sha256</code> files when scripts change in docs.</p> </li> <li> <p>CI publish step (if you want to auto-deploy): use <code>peaceiris/actions-gh-pages</code> or <code>JamesIves/github-pages-deploy-action</code> to publish the <code>site/</code> directory to the <code>gh-pages</code> branch or a host.</p> </li> </ol>"},{"location":"web-site-management/#keep-docs-canonical","title":"Keep <code>docs/</code> canonical","text":"<ul> <li> <p>Prefer editing and reviewing changes to files under <code>docs/</code>.</p> </li> <li> <p>If you must edit <code>site/</code> (e.g., for manual content patches), follow the same review process and back-propagate changes into <code>docs/</code>.</p> </li> </ul>"},{"location":"web-site-management/#persona-content-operational-workflow","title":"Persona content &amp; operational workflow","text":"<ul> <li> <p>Persona pages live under <code>docs/personas/&lt;persona&gt;/</code> and should include the canonical <code>landing.md</code>, <code>intro.md</code>, and other materials.</p> </li> <li> <p>Owners and status metadata should be included in frontmatter (owner email or team, status: proposed/draft/stable). This helps review and governance.</p> </li> </ul>"},{"location":"web-site-management/#actions-we-automated","title":"Actions we automated","text":"<ul> <li> <p>CI verification: <code>.github/scripts/verify-site-scripts.sh</code> \u2014 verifies <code>docs/</code> script copies and <code>site/</code> parity for changed scripts.</p> </li> <li> <p>Autoupdate: <code>.github/scripts/autoupdate-scripts-sha.sh</code> \u2014 updates per script <code>*.sha256</code> and <code>scripts.sha256</code> when a script in <code>docs/</code> changes in a PR.</p> </li> </ul>"},{"location":"web-site-management/#migration-next-steps","title":"Migration / Next steps","text":"<ol> <li> <p>Consider removing <code>site/</code> from the repo if CI deployment is configured and stable; commit <code>site/</code> removal with a PR that updates CI to publish built site to <code>gh-pages</code>.</p> </li> <li> <p>Add a <code>web-site-management.md</code> page (this page) with step-by-step instructions for maintainers.</p> </li> <li> <p>Add code owners for <code>docs/</code> and <code>personas</code> to ensure consistent review.</p> </li> </ol> <p>If anything in this workflow should be changed (e.g., we continue to check in site), we can adapt the CI accordingly to keep both the ease of <code>site/</code> updates and code reviewing safeguards.</p>"},{"location":"features/fail2ban/","title":"fail2ban","text":"<p>When to enable fail2ban: when exposed services accept external credentials (ssh, web admin). Example configuration and a sample jail are provided.</p>"},{"location":"features/nftables/","title":"nftables","text":"<p>Example rules and tips for debugging blocked tests.</p>"},{"location":"features/selinux/","title":"SELinux","text":"<p>Short guidance: prefer permissive mode for initial deployment only; then create minimal allow rules and move to enforcing mode.</p>"},{"location":"network-troubleshooting/osg-debugging-document/","title":"OSG Debugging Documentation","text":""},{"location":"network-troubleshooting/osg-debugging-document/#edited-by-j-zurawski-internet2-s-mckee-university-of-michigan","title":"Edited By: J. Zurawski \u2013 Internet2, S. McKee \u2013 University of Michigan","text":""},{"location":"network-troubleshooting/osg-debugging-document/#february-4th-2013","title":"February 4th 2013","text":"<p>Note</p> <p>This document is old but still may have useful information.  Many tools it references may no longer be supported or available.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#abstract","title":"Abstract","text":"<p>Scientific progress relies on intricate systems of computers and the networks that connect them.  Often it is the case that scientific data is gathered via a well defined process, information is digitized, stored, transmitted, and processed by members of large and distributed collaborations.  The Open Science Grid advances science through the concept of distributed computing \u2013 the process for sharing resources through a unified software framework focused on the common tasks of data movement, processing, and analysis.</p> <p>Networks are an integral part of the distributed computing process.  Similar to the computational and storage resources, it is crucial that all networking components, on the complete end-to-end path, are functional and free of physical and logical flaws.  A rich set of measurement and monitoring tools exists to provide network operations staff and end users a window into the functionality of networks, despite the fact that these actors do not have direct control over the complete path their data may travel.</p> <p>This document discusses common measurement and monitoring tools available to the OSG community, and strategies to deploy, use, and interpret the results they produce.  The end goal is to give end users more insight into network behavior, and assist local and remote networking staffs as they correct damaging performance problems that will impact the scientific process.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#introduction","title":"Introduction","text":"<p>The process of science is often complicated when viewed as a complete system.  At the core of any project, there is a mechanism to observe or simulate some system, and produce meaningful results that will be interpreted and scrutinized between experimentation runs.   The machinery that surrounds this process can be as benign as simple cameras, or as complex as the Large Hadron Collider and its associated experiments.  Other common components include ways to digitize, store, process, and share the end result of experimentation \u2013 often done using computational systems.</p> <p>Computation falls into 3 broad classifications, all of which are required to implement the paradigm of scientific computing:</p> <ul> <li> <p>Storage \u2013 Readable and writable physical medium used for temporary or long term residency of gathered data.</p> </li> <li> <p>Processing \u2013 Specially designed hardware and software that iterates over collected data sets looking for pre-defined triggers and results.</p> </li> <li> <p>Networking \u2013 Interconnecting hardware and software used to facilitate communication between storage and processing components both on a local, and fully distributed basis.</p> </li> </ul> <p>When fully realized, even a small facility can contribute a great amount of resources to the overall goal of scientific advancement.  In practice it may be the case that a lab consisting of a single researcher can pull data sets from a centralized location, perform carefully selected segments of an entire set of analysis that is required, and return any relevant results as they are discovered.  When used in an inductive fashion, one can imagine the overall throughout that a VO such as the LHC project is able to attain through 100s of distributed facilities and 1000s of collaborating researchers.</p> <p>Complexity is present as we travel down the individual technology items in the above scenario.  Often it is the case that ideal performance is hard to attain due to the intertwined nature of the mechanisms involved.  For example, data must be written and read from physical medium.  Often this step is slower due to the mechanical nature of the process, and struggles to keep up with faster technologies such as processing or transmission on network infrastructure. Equally, it may be the case that a flaw in the network infrastructure, such as a failing component, can introduce data loss that must be compensated for through retransmission.  Retransmission implies additional work for storage and processing components that must waste resources to overcome a fixable, but often unnoticed, problem.</p> <p>Network performance monitoring is a relatively unseen, but still extremely necessary, practice.  This statement is true due to the nature of network use through application software and communication protocols.  Application developers wish to unburden the user with details about \"how\" data may be moved between facilities.  Care is taken to design applications in such a way that the user is simply presented with options related to a source and destination only, and little or no insight into the path taken or the current conditions that may be present.  The aforementioned situation where a failing component institutes data loss results in only one symptom to the end user: lower than expected throughput.  Many users may not notice, or have become complacent, with low performance situations.  Some may write this off as \"the network is slow\", or perhaps will not notice at all due to experiences with home connections that are often 2 orders of magnitude slower than what is possible in a typical academic environment.</p> <p>Software exists to monitor network performance in many different ways.  For example it is possible take a measure of network throughput, and simulate the behavior of a file transfer application.  It is also possible to observe network stability (e.g. jitter) over time to simulate video or audio transmission.  These basic observations are powerful when used both in a local environment, as well as on an end-to-end basis.  In either case, software must be deployed and available to the community on points of interest: specifically on the local and shared network infrastructure distributed around the world.  perfSONAR is a software framework that simplifies network debugging activities by making it easier to deploy measurement tools, and facilities the sharing of results.  It is currently deployed on many communal networking resources in the R&amp;E community, including backbones, regional networks, campuses, and individual laboratories.</p> <p>Once perfSONAR is deployed, it becomes possible to troubleshoot situations that result in low throughout for the end user in a straightforward manner.  It is important to note that when something like this occurs, it is not the sole responsibility of the end user to debug and solve a networking problem; rather it is their duty to report the problem and provide as much information as possible to local or remote network staffs so they may learn about the issue, and interpret the results so as to lead to quick and efficient problem resolution.  Locating this staff may be challenging, but many organizations maintain a dedicated Network Operations Center (NOC) whose staff are ready to accept trouble reports and act in an appropriate manner.    Section 8 details locations you may turn to for additional support.</p> <p>This document will expand upon these topics in the remaining sections, and conclude with information where additional resources beyond a simple introduction to these topics can be found.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#the-scientific-networking-process","title":"The Scientific Networking Process","text":"<p>There is a rich ecosystem of components available for monitoring and managing the scientific networking process.  This myriad of hardware and software must work together to complete the overall goal of interpreting gathered or simulated observations.  Each component we will discuss has the ability to be installed, operated, and maintained in different ways.  Individual brands or versions are not important, but the overall idea of each will be explored.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#hosts","title":"Hosts","text":"<p>Server or \"host\" hardware and software can be used in many different ways.  Often it is the case that these components serve as computational resources for processing data, or provide access to underlying data stored on physical media.  It may also be the case that software designed to \"glue\" components of a framework together (e.g. processing mail, authenticating users, providing mappings between names and addresses) is installed on a dedicated or shared host resource.</p> <p>Hosts must contain an operating system: software designed to control and maintain the underlying hardware such as storage media, network interface cards, processors, and other peripheral devices.  Operating system hardware can vary in functionality; completely interactive systems such as those found on laptops can be more pleasing for a human user vs. that of a no frills batch processing system designed to only iterate over scientific data.  The choice of operating system will vary from use case to use case.</p> <p>The footing provided by the hardware and operating system serves as the basis for the remainder of the components in this discussion.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#protocols","title":"Protocols","text":"<p>Protocols are software algorithms implemented on hosts and networking components, and are used to facilitate communication strategies.  Protocols are constructed in a \"layered\" fashion, and are designed to handle certain aspects of the overall communication plan.  For instance a protocol may be used to communicate between two network devices, and may involve the use of different patterns of electrical or optical signal.  On top of this basic system of signals we may construct a different protocol that is focused on communication between end hosts, and is able to break up the notion of a user's files into small chunks so they can be sent reliably end-to-end.</p> <p>Protocols evolve with the underlying technology, and often can be tuned to specific use cases.  One such protocol, Transmission Control Protocol (TCP), is widely used in applications that many users are familiar with such as web browsing, mail transfer, and file exchange.  Early incarnations of TCP were designed for the networks of the 1980s; slower, less reliable, constructions than what is present in the networks of the 2010s.  As such TCP must be instructed, via configuration on a host or network devices operating environment,  that it should behave in a different and more efficient manner.</p> <p>With the protocol in place, we can now begin to discuss applications that are able to use the network to communicate in an automated fashion.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#applications","title":"Applications","text":"<p>Applications are specific use cases, programmed as software, and made available to end users via a host's operating environment.  There are numerous applications we use on a daily basis \u2013 web browsers to fetch remote content, word processing applications to type papers, mail and instant messaging clients to exchange information in near real time. Scientific applications normally focus on performing a single task (e.g. end-to-end data movement, visualization, data transformation, data analysis) on either a local or distributed basis.</p> <p>In the case of distribution, it becomes necessary to interact with the underlying network through the use of a protocol. File transfer is a specialized application that takes is interested in either sending a local file to a remote location, or retrieving remote data to bring locally.  In either case the application must broker with a protocol, such as TCP, that is available on both ends of the transfer.  Through a series of API calls information is segmented into transmission chunks and sent reliably though the communication medium.  Most of this is handled transparently from the user's perspective, and as such they are not given much in the way of feedback beyond a pass or failure, and some notion of how long it took.</p> <p>Understanding more about the network can be enlightening exercise for users who are unaware of the complexity and span of components that are required for operation.  This will be discussed in the remaining sections.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#lab-local-area-network","title":"Lab Local Area Network","text":"<p>The first step in the networking tree is often the interconnections between components local to the user.  This may consist of the storage and processing nodes in a single rack or data center used for scientific processing, connected via technology consistent with the tasks they are performing.  Cluster nodes may use a high speed interconnect such as InfiniBand; servers may also just use typical Ethernet at 1Gbps or 10Gbps.</p> <p>In either case, there will be dedicated network equipment with the task of aggregating and controlling traffic flow to the local devices, and serving as an uplink to the next network in the chain (the campus).   Monitoring and management of this local environment is a good idea, either through passive means such as using the SNMP system, or active tests that check the health of transfers on a local basis.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#campus-local-area-network","title":"Campus Local Area Network","text":"<p>The first hop beyond a laboratory environment is a network maintained by campus/local support staff.  It is often the case that this group is maintaining the infrastructure for the use of all end users, and as such will design and maintain things to preserve uniform functionality and performance.</p> <p>Campus networks are an even larger ecosystem of devices given the area they may cover.  It may be the case that the network in the previous section is behind several devices before it has a clean path to the outside world.  It may also be the case that traffic aggregation is extremely high, and congestion becomes a factor during certain parts of the day or times of the year.  These nuances make local performance monitoring crucial for operational soundness.</p> <p>This group is also the first contact that should be exercised in the event of a network performance problem.  While they may not be able to answer for the status of the entire path, they can escalate the problem to the regional or backbone support staff as needed.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#regional-network","title":"Regional Network","text":"<p>A regional network provider aggregates the networks of numerous campuses in a state, country, or pre-defined region. Examples include provider for states of the US (e.g. KanREN), countries (SWITCH, the network of Switzerland), or collaboration between parties without a political boundary (the SOX regional network in the United States).  Regional networks may cover a large geographical area, but often have less equipment than a campus.  The role of a regional aggregator is to take connections (large and small) and condense them into long-haul links that will uplink to a backbone or exchange point as a next step.</p> <p>Regional operations teams have similar performance concerns to that of a campus network.  The aggregation point of several networks can be a critical component, and one of the more likely places that a failing piece of equipment or congestion can impact downstream network users.  Monitoring local and remote components (e.g. maintaining active testing between networks) critical.</p> <p>Regional support teams can be likely candidates for assistance on performance problems, but users are reminded to discuss options with their local staff first before engaging with these groups.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#re-backbone","title":"R&amp;E Backbone","text":"<p>An R&amp;E backbone consists of an aggregation of numerous regional providers.  Capacity must reflect the number and expectations of this group of customers, and often is orders of magnitude higher than other networks that are downstream.</p> <p>As an aggregation point, normally spread over a very large geographical area, traffic flows will be numerous, of mixtures of size and duration, and be destined for diverse destinations domestically and internationally.  Each Point of Presence (PoP) could have a large number of customers integrating, and thus increases the chance of an issue local to this device.</p> <p>As a service to customers, the R&amp;E backbone should consider making test instances available to help bisect and debug challenging problems that may cross the domain.  Backbone support teams are also well trained and have knowledge of performance monitoring.  Some providers such as ESnet and Internet2 have dedicated staffs just to support the troubleshooting of network problems for customers.  An end user is encouraged to seek out these resources, as well as those that are local, when debugging a problem impacting scientific work.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#exchange-point","title":"Exchange Point","text":"<p>An exchange point is normally a location where multiple backbone networks and international transit links (e.g. trans- oceanic links) combine and transit to other domains.  An exchange point is a special case of an aggregation network like a regional in that policies may be different depending on the membership structure.</p> <p>International exchange points suffer from the aforementioned problems of traffic aggregation wherein congestion or equipment failure will have a severe impact on all traffic.  Monitoring these devices is crucial, as in other use cases.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#actor-agent-definitions","title":"Actor &amp; Agent Definitions","text":"<p>There are many actors involved in the process of network management and debugging.  We will highlight three here, as they represent the most critical members of the support team that OSG has available when problems are discovered.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#end-user","title":"End User","text":"<p>The end user is understood to be the primary user and beneficiary of OSG software to process and operate on scientific data sets.  The sophistication of this end user is assumed to be beginner to average in matters related to system and network administration.  In general we assume they are knowledgeable enough to install and maintain OSG software, and connect devices to the networking infrastructure.</p> <p>This actor is assumed to be the primary user of the perfSONAR end user tools, packaged in the OSG VDT distribution. These tools are meant to be run from a system to upstream test machines provided by the campus, regional, or backbone network.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#local-administrator","title":"Local Administrator","text":"<p>The local administrator can be campus support staff responsible for the health of servers or network devices across the greater campus ecosystem.  Their primary responsibility it to ensure uptime of the network for all users, as well as assist in debugging specific problems caused by performance impacting problems on a local basis.</p> <p>This actor may not be able to directly address problems on a regional, national, or international basis but can serve as a liaison with individuals within those stewardship organizations.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#remote-administrator","title":"Remote Administrator","text":"<p>A remote administrator can be regional, backbone, or exchange point staff responsible for the health of remote networking resources.  It is often the case that these individuals may not be aware of a specific use case between remote campuses, but could answer questions about the current health and status of the network they control.</p> <p>These individuals are assumed to be knowledgeable about performance tools, and can work with local staff as needed to make test points available to assist in debugging.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#local-preparations","title":"Local Preparations","text":"<p>A first step to any OSG software installation to support scientific activity is preparation of the local environment. Given the considerations denoted in the previous sections, we will discuss 3 specific preparation activities:</p> <ul> <li> <p>End System Operating System and Protocol Tuning</p> </li> <li> <p>Network Architecture Adjustments</p> </li> <li> <p>Network Configuration Tuning</p> </li> </ul> <p>Each of these steps is considered to be most relevant to the laboratory local environment, although some should be considered for the campus as well.  It is assumed that the end user actor, with the help of local administrators, can make these changes.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#end-system-tuning","title":"End System Tuning","text":"<p>Computer systems are similar to automobiles in that its possible to \"tune\" certain internal aspects and achieve higher performance when using the network.  The operating system and associated protocols like TCP make these changes rather simple to implement.  In general there are several options worth considering:</p> <ul> <li> <p>Network interface cards have an adjustable size for their packet queues</p> </li> <li> <p>Kernel buffers can be increased to support long distance transfers</p> </li> <li> <p>The TCP congestion control algorithm can be changed</p> </li> </ul> <p>ESnet has made a web based resource available to assist with the task of host tuning, it can be found here:</p> <p>http://fasterdata.es.net/host-tuning/</p>"},{"location":"network-troubleshooting/osg-debugging-document/#network-architecture","title":"Network Architecture","text":"<p>Architectural decisions are often involved and will involve the input of local support staff.  In general laboratory architecture should be robust in the following manner:</p> <ul> <li> <p>Multiple uplinks to the campus network to provide capacity and resiliency</p> </li> <li> <p>A limited amount of \"fan in\" (e.g. number of connections) into a given access switch.  It is recommended that as the fan in grows, multiple switches be employed to manage connectivity and congestion</p> </li> <li> <p>Elimination of firewalls from the path.  Security can be implemented by host-based firewalls that restrict ports as well as Access Control Lists (ACLs) to white list sites that are communicated with.  Firewall devices have been known to severely impact traffic for bandwidth intensive applications.</p> </li> <li> <p>Choice of device manufacturer that allows for out of band management and monitoring (e.g. SNMP) of devices</p> </li> <li> <p>Choice of device manufacturer that allows for per-interface tuning of memory buffers (vs. that of a shared memory region)</p> </li> </ul> <p>Network architectural considerations are far too broad to be represented in a single document for the OSG, and the interested reader is encouraged to read the following resource provided by ESnet:</p> <p>http://fasterdata.es.net/science-dmz</p>"},{"location":"network-troubleshooting/osg-debugging-document/#network-configuration-tuning","title":"Network Configuration Tuning","text":"<p>Much like end hosts, network devices have the ability to be \"tuned\" for specific use cases.  This tuning normally centers on enabling or disabling certain features on a router or switch (e.g. policy maps) or adjusting the available memory available to account for a specific use case (e.g. less memory for a video application, more for a throughput intensive tool).</p> <p>As every manufacturer provides different interfaces to the underlying hardware, we cannot make specific recommendations in this document.  The interested reader is encouraged to read this guide provided by ESnet:</p> <p>http://fasterdata.es.net/network-tuning/router-tuning/</p>"},{"location":"network-troubleshooting/osg-debugging-document/#measurement-software","title":"Measurement Software","text":"<p>The available span of network measurement software is vast.  A simple web search will reveal 10s of names, some still active and others long dead.  The R&amp;E community began to standardize on available tools in the later part of the 2000s with an effort to unify measurement tools and infrastructure to support them: perfSONAR.</p> <p>perfSONAR is a framework to simplify end to end network debugging.  It consists of a layer of middleware, designed to sit between the measurement tools and the visualization and analysis that is useful to human users.  A key component of the perfSONAR concept is the pS Performance Toolkit; this completely enclosed operating system packages performance tools and easy to follow GUIs to enable configuration.</p> <p>perfSONAR focuses on several key metrics:</p> <ul> <li> <p>Achievable Bandwidth</p> </li> <li> <p>One Way Latency</p> </li> <li> <p>Round Trip Latency</p> </li> <li> <p>Packet Loss, Duplication, Out of Orderness</p> </li> <li> <p>Interface Utilization, Errors, Discards</p> </li> <li> <p>Layer 3 Path</p> </li> <li> <p>Path MTU</p> </li> </ul> <p>Many of these metrics are calculated through simple tests that can be run from the command line.  The OSG VDT package contains 3 key measurement tools that will be used as we discuss networking debugging in Section 7:</p> <ul> <li> <p>BWCTL \u2013 A tool for measuring end to end bandwidth availability</p> </li> <li> <p>NDT \u2013 A tool designed to diagnose several different aspects of a host and network</p> </li> <li> <p>OWAMP \u2013 A tool designed for measuring one way delays of packets, as well as loss, duplication, and out of orderness.</p> </li> </ul> <p>These 3 command line tools, when installed on a compute or storage node, can be used to launch tests to perfSONAR servers located in the greater R&amp;E networking world, e.g. on the campus, regional, backbone, or exchange point networks.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#debugging-process","title":"Debugging Process","text":"<p>The following sections will discuss the process to install, use, and interpret measurement tools in an OSG software environment.  End users are encouraged to try these steps first, but also contact their local support staff at the earliest possible moment.  Involving support staff will ensure that expert eyes are available to assist with problems, and funnel the requests for help to the proper area (e.g. GOC, other networks, etc.).</p>"},{"location":"network-troubleshooting/osg-debugging-document/#software-installation","title":"Software Installation","text":"<p>Client software can be installed in one of two ways, either though the OSG VDT or via RPMs from the perfSONAR web site.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#osg-software","title":"OSG Software","text":"<p>[INSERT INSTRUCTIONS ON HOW TO INSTALL VDT HERE]</p>"},{"location":"network-troubleshooting/osg-debugging-document/#perfsonar-ps-software","title":"perfSONAR-PS Software","text":"<p>All perfSONAR software is available through an RPM (Red Hat Package Manager) repository to make for easy installation and updates.  The following steps can be taken to install these tools:</p> <ul> <li>Import the Internet2 Signing Key</li> </ul> <p>The following command will import the key.</p> <pre><code>rpm --import https://software.internet2.edu/rpms/RPM-GPG-KEY-Internet2\n</code></pre> <ul> <li>Download RPM Software</li> </ul> <p>The latest version for CentOS 5 and 6 (both x86 and x86_64 architectures) can be found on the the following web site:</p> <p>http://software.internet2.edu</p> <p>Note that SL5 andSL6, RHEL 5 and RHEL 6 should work with these builds.  Those using other operating systems are suggested to try source builds that can be found at the same location.</p> <ul> <li>Run Yum Update</li> </ul> <p>The following command will invoke updates to the yum system, and also prepare the newly installed perfSONAR repository:</p> <p>yum update</p> <ul> <li>Search for Tools</li> </ul> <p>Yum can be searched in the following manner:</p> <p>yum search TOOLNAME</p> <ul> <li>Install Tools</li> </ul> <p>Yum can install tools in a similar fashion, the following command will install the client libraries:</p> <p>yum install owamp-client bwctl-client ndt-client</p> <p>Note that some other tools may be pulled in automatically.  Note that iperf and nuttcp are required for BWCTL to work.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#tool-selection","title":"Tool Selection","text":"<p>Debugging network problems involves running several tools, and gathering results both an end-to-end basis as well as to points in the middle.  Initial tool selection can depend on a couple of factors:</p> <ul> <li> <p>What servers are available on the other end, as well as in the middle</p> </li> <li> <p>What use case is attempting to be debugged</p> </li> <li> <p>How sophisticated is the user running the tools</p> </li> </ul> <p>In general we recommend that users try \" all\" available tools, but in a structured and complete fashion before moving on to new tests.  In general the following recommendation can be made regarding tool selection:</p> <ul> <li> <p>Perform NDT client tests to the closest server possible.  Additional tests to other points in the middle as needed.</p> </li> <li> <p>Perform end-to-end BWCTL tests to establish a baseline bandwidth.  Perform a bisected BWCTL test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem)</p> </li> <li> <p>Perform end-to-end OWAMP tests to establish baseline latency and loss.  Perform a bisected OWAMP test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem)</p> </li> </ul> <p>The following are some examples of how to use the tools from the command line:</p> <ul> <li>NDT</li> </ul> <p>NDT uses a command line client called web100clt.  There are many options available, but in general you must supply a server name, and some debugging flags to get additional output.  Here is a simple invocation:</p> <pre><code>[user@host ~]$ web100clt -n ndt.chic.net.internet2.edu\n\nTesting network path for configuration and performance problems  --  Using IPv6 address\n\nChecking for Middleboxes . . . . . . . . . . . . . . . . . .  Done\n\nchecking for firewalls . . . . . . . . . . . . . . . . . . .  Done\n\nrunning 10s outbound test (client to server) . . . . .  92.16 Mb/s\n\nrunning 10s inbound test (server to client) . . . . . . 90.63 Mb/s\n\nThe slowest link in the end-to-end path is a 100 Mbps Full duplex Fast Ethernet subnet\n\nInformation: Other network traffic is congesting the link\n\nInformation [S2C]: Packet queuing detected: 15.06% (local buffers)\n\nServer &amp;#39;ndt.chic.net.internet2.edu&amp;#39; is not behind a firewall. [Connection to the ephemeral port was successful]\n\nClient is probably behind a firewall. [Connection to the ephemeral port failed]\n\nInformation: Network Middlebox is modifying MSS variable (changed to 1440)\n\nServer IP addresses are preserved End-to-End\n\nClient IP addresses are preserved End-to-End\n</code></pre> <p>To get additional data, try adding the -ll flag, it will produce a more in depth analysis.  NDT is useful as the first step of debugging to gather information about the end host, as well as the basic network configuration.</p> <ul> <li>BWCTL</li> </ul> <p>BWCTL is invoked from the command line with a number of options.  Of these the following are important:</p> <p>-</p> <ul> <li> <p>-f  - Sets the format, supply either a byte based metric (K, M, G) or a bit based metric (k, m, g).</p> </li> <li> <p>\u2013t \u2013 Sets the length of the test in seconds</p> </li> <li> <p>\u2013i \u2013 Specifies the reporting interval (e.g. how often instantaneous bandwidth results are available) in seconds</p> </li> <li> <p>\u2013c \u2013 Specifics the host that will receive the flow of data, e.g. the \"catcher\"</p> </li> <li> <p>\u2013s \u2013 Specifics the host that will send the flow of data, e.g. the \"sender\"</p> </li> </ul> <p>An example of invoking BWCTL can be seen below.  In this example we are sending data from the host we are on to another located in Kansas City MO, on the Internet2 network:</p> <pre><code>[user@host ~]$ bwctl -f m -t 10 -i 1 -c nms-rthr.kans.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 93 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.210 -s -f m -m -p 5011 -t 10 -i 1\nbwctl: start\\_tool: 3568979157.239050\n------------------------------------------------------------\nServer listening on TCP port 5011\nBinding to local address 64.57.16.210\nTCP window size: 0.08 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.16.210 port 5011 connected with 64.57.17.18 port 5011\n[14]  0.0- 1.0 sec    105 MBytes    879 Mbits/sec\n[14]  1.0- 2.0 sec    118 MBytes    990 Mbits/sec\n[14]  2.0- 3.0 sec    118 MBytes    990 Mbits/sec\n[14]  3.0- 4.0 sec    118 MBytes    990 Mbits/sec\n[14]  4.0- 5.0 sec    118 MBytes    990 Mbits/sec\n[14]  5.0- 6.0 sec    118 MBytes    990 Mbits/sec\n[14]  6.0- 7.0 sec    118 MBytes    990 Mbits/sec\n[14]  7.0- 8.0 sec    118 MBytes    990 Mbits/sec\n[14]  8.0- 9.0 sec    118 MBytes    990 Mbits/sec\n[14]  9.0-10.0 sec    118 MBytes    990 Mbits/sec\n[14]  0.0-10.1 sec  1178 MBytes    979 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979172.016198\nRECEIVER END\n</code></pre> <p>This test reveals that over the course of 10 seconds we achieved an average bandwidth of 979Mbps and sent a total of 1178MB of data.  We can reverse the direction of this test in the next example:</p> <pre><code>[user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 75 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.17.18 -s -f m -m -p 5011 -t 10 -i 1\nbwctl: start\\_tool: 3568979241.960327\n------------------------------------------------------------\nServer listening on TCP port 5011\nBinding to local address 64.57.17.18\nTCP window size: 16.0 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.17.18 port 5011 connected with 64.57.16.210 port 5011\n[ID] Interval       Transfer     Bandwidth\n[14]  0.0- 1.0 sec   111 MBytes   929 Mbits/sec\n[14]  1.0- 2.0 sec   118 MBytes   990 Mbits/sec\n[14]  2.0- 3.0 sec   118 MBytes   990 Mbits/sec\n[14]  3.0- 4.0 sec   118 MBytes   990 Mbits/sec\n[14]  4.0- 5.0 sec   118 MBytes   990 Mbits/sec\n[14]  5.0- 6.0 sec   118 MBytes   990 Mbits/sec\n[14]  6.0- 7.0 sec   118 MBytes   990 Mbits/sec\n[14]  7.0- 8.0 sec   118 MBytes   990 Mbits/sec\n[14]  8.0- 9.0 sec   118 MBytes   990 Mbits/sec\n[14]  9.0-10.0 sec   118 MBytes   990 Mbits/sec\n[14]  0.0-10.2 sec  1193 MBytes   984 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979256.889493\nRECEIVER END\n</code></pre> <p>A similar result is seen in that we achieve near 1Gbps bandwidth (e.g. the hosts are only connected at 1Gbps).</p> <p>BWCTL can (and should) be used to check available bandwidth between servers.  Start first on the long path (e.g. end-to- end) then test to resources in the middle.  Note that BWCTL supports a 3 mode operation, wherein you can provide options for both the '-c' and '-s' and perform tests between these two hosts without being physically logged into either:</p> <pre><code>[user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu -c nms-rthr1.hous.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 82 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.130 -s -f m -m -p 5001 -t 10 -i 1\nbwctl: start\\_tool: 3568979772.344387\n\n------------------------------------------------------------\nServer listening on TCP port 5001\nBinding to local address 64.57.16.130\nTCP window size: 0.08 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.16.130 port 5001 connected with 64.57.16.210 port 5001\n[ID] Interval       Transfer     Bandwidth\n[14]  0.0- 1.0 sec   103 MBytes   861 Mbits/sec\n[14]  1.0- 2.0 sec   118 MBytes   990 Mbits/sec\n[14]  2.0- 3.0 sec   118 MBytes   990 Mbits/sec\n[14]  3.0- 4.0 sec   118 MBytes   990 Mbits/sec\n[14]  4.0- 5.0 sec   118 MBytes   990 Mbits/sec\n[14]  5.0- 6.0 sec   118 MBytes   990 Mbits/sec\n[14]  6.0- 7.0 sec   118 MBytes   990 Mbits/sec\n[14]  7.0- 8.0 sec   118 MBytes   990 Mbits/sec\n[14]  8.0- 9.0 sec   118 MBytes   990 Mbits/sec\n[14]  9.0-10.0 sec   118 MBytes   990 Mbits/sec\n[14]  0.0-10.2 sec  1183 MBytes   977 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979785.230833\nRECEIVER END\n</code></pre> <p>BWCTL requires a stable NTP clock to work properly, be sure that NTP is configured before using this tool.</p> <ul> <li>OWAMP</li> </ul> <p>OWAMP is a tool that measures latency, loss, out of orderness, and duplication of packets between a source and a destination.  Note that this test measures each direction independently versus that of the traditional round trip tool ping.  Below is an example of a test:</p> <pre><code>[user@host ~]$ owping owamp.wash.net.internet2.edu\nApproximately 12.6 seconds until results available\n--- owping statistics from [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:60455 to [owamp.wash.net.internet2.edu]:47148 ---\nSID:        00160034d4ba4cad4e8c0485546b4ebf\nfirst:        2013-02-04T15:05:18.240\nlast:        2013-02-04T15:05:27.254\n100 sent, 0 lost (0.000%), 0 duplicates\none-way delay min/median/max = 2.02/2.1/2.06 ms, (err=0.218 ms)\none-way jitter = 0 ms (P95-P50)\nHops = 2 (consistently)\nno reordering\n\n--- owping statistics from [owamp.wash.net.internet2.edu]:47149 to [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:33562 ---\nSID:        00170098d4ba4cad8eb45282697d2cc2\nfirst:        2013-02-04T15:05:18.259\nlast:        2013-02-04T15:05:27.175\n100 sent, 0 lost (0.000%), 0 duplicates\none-way delay min/median/max = 3.19/3.3/3.27 ms, (err=0.218 ms)\none-way jitter = 0 ms (P95-P50)\nHops = 2 (consistently)\nno reordering\n</code></pre> <p>The results clearly state each direction of operation, and any problems that were found.  As in the BWCTL case the tool is highly reliant on stable NTP numbers, so be sure your server is synchronized against an NTP server.</p> <p>OWAMP is a lightweight test and can be used to show minor amounts of packet loss between hosts.  Perform the test on the full end-to-end path, and then bisect the path by testing to points in the middle.  Often low throughput observed via BWTL will show up as packet loss in OWAMP.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#end-to-end-testing","title":"End-to-end Testing","text":"<p>The concept of end-to-end testing is required as a first step in debugging network problems.  Via the OSG tools it is possible to use \"client\" tools as discussed in Section 7.2 to gauge the total end-to-end path.  These client tools can be pointed at a pS Performance Toolkit instance installed on the remote end, or via stand-alone daemon applications started on OSG systems.  In either case, a daemon and client will be needed.</p> <p>The following procedure should be followed:</p> <ul> <li> <p>Notify local networking staff at each end that are noticing problems, and would like to investigate them.  Note that you can run tests end-to-end, and share them when you are complete.</p> </li> <li> <p>Identify Servers on both ends (e.g. standalone pS Performance Toolkit instances or starting daemons on OSG servers)</p> </li> <li> <p>Identify clients on both ends, normally the compute or storage nodes.  Avoid using machines that are not involved in the OSG software process such as laptops.</p> </li> <li> <p>Perform end-to-end testing with:</p> </li> <li> <p>NDT</p> </li> <li> <p>BWCTL</p> </li> <li> <p>OWAMP</p> </li> <li> <p>Perform several tests and always record the results.  It's a good idea to run at different times during the day, and note when you ran the tests</p> </li> <li> <p>Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate.</p> </li> </ul> <p>After end-to-end testing, and examining the results with local and GOC based professionals, it may be time to embark on a larger debugging exercise with partial path decomposition.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#partial-path-decomposition","title":"Partial Path Decomposition","text":"<p>As we saw in Section 7.4, it is necessary to use all tools in a structured and scripted manner.  Deciding to divide the path is no different.  The following steps should be followed:</p> <ul> <li> <p>Using a tool like traceroute or tracepath, find the paths between you and the remote host.  If possible validate the path for the reverse direction as well.  It may be possible that these are different.</p> </li> <li> <p>For one of the networks on the path, usually one in the direct middle (often a backbone network like Internet2, ESnet, or NLR), find a testing host.  If these are not posted on public we pages, send an email to the support teams (e.g. rs@internet2.edu [BROKEN-LINK: mailto:rs@internet2.edu], or trouble@es.net [BROKEN-LINK: mailto:trouble@es.net]).</p> </li> <li> <p>Perform end-to-middle testing from the source and desgination with:</p> </li> <li> <p>NDT</p> </li> <li> <p>BWCTL</p> </li> <li> <p>OWAMP</p> </li> <li> <p>Perform several tests and always record the results.  It's a good idea to run at different times during the day, and note when you ran the tests</p> </li> <li> <p>Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate.</p> </li> <li> <p>If the tests show one 'side' as being better than the other, you can repeat this process by further bisecting the path on the side with the problem.</p> </li> </ul>"},{"location":"network-troubleshooting/osg-debugging-document/#interpreting-results","title":"Interpreting Results","text":"<p>Interpretation of results can be tricky due to the nature of protocols on the network, including TCP.  In general the only symptom that is given off to a problem with TCP is \"low throughput\".  The following are some tips on interpreting results:</p> <ul> <li> <p>NDT will denote if your host does not have the proper amount of tuning.  If it doesn't, please considering following the host tuning steps discussed in Section 5.1</p> </li> <li> <p>NDT will give the first indication of network problems as well, and may indicate the presence of packet loss, link bottlenecks, or congestion.  Since NDT is based on heuristics these results can turn out to be false positives, but are often worthy of following up on.  In the event of congestion, ask local networking staff to see if there are any heavily utilized links.  If packet loss is an issue, ask to see if any interface errors or discards are present.</p> </li> <li> <p>BWCTL, when used in TCP mode, is only useful at nothing high or low throughput.  This is normally good from a capability standpoint, but it cannot tell you anything else about a serious problem</p> </li> <li> <p>OWAMP is useful for detecting loss.  The theory is that if you notice the loss of small UDP packets produced by OWAMP, the same behavior will be seen in the form of low throughput from a tool like BWCTL.</p> </li> <li> <p>OWAMP can also be used to show asymmetric routing (with the aide of tools like traceroute) or if queuing and congestion are becoming a factor in one direction vs. another.</p> </li> <li> <p>Bisecting a path, and being patient, are normally the only ways to narrow down problems.</p> </li> </ul> <p>In addition to these adages, please consider asking your local staff for assistance when you first notice a problem.  If they are unable to help, consult the resources listed in Section 8.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#additional-help","title":"Additional Help","text":"<p>The following locations can be consulted for more help in debugging network problems:</p> <ul> <li> <p>Internet2 Research Support Center \u2013 rs@internet2.edu [BROKEN-LINK: mailto:rs@internet2.edu]</p> </li> <li> <p>ESnet Trouble Reporting \u2013 trouble@es.net [BROKEN-LINK: mailto:trouble@es.net]</p> </li> <li> <p>NLR NOC - noc@nlr.net [BROKEN-LINK: mailto:noc@nlr.net]</p> </li> <li> <p>OSG GOC - goc@opensciencegrid.org [BROKEN-LINK: mailto:goc@opensciencegrid.org]</p> </li> </ul>"},{"location":"network-troubleshooting/osg-debugging-document/#acknowledgements","title":"Acknowledgements","text":"<p>The authors would like to acknowledge and thank the OSG community for their support and feedback into network performance problems and tools that would be useful for end users.</p> <p>The perfSONAR-PS community has been invaluable, and the authors would like to thank them for their generous contributions of software, expertise, and time.</p>"},{"location":"network-troubleshooting/osg-debugging-document/#references","title":"References","text":"<p>TBD</p>"},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/","title":"perfSONAR Testpoint Container Restart Loop Issue","text":""},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/#problem-description","title":"Problem Description","text":"<p>The perfSONAR testpoint container enters a restart loop when using certain docker-compose.yml configurations. The container continuously restarts and fails to initialize systemd properly.</p>"},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/#root-cause","title":"Root Cause","text":"<p>The issue occurs when the docker-compose.yml file is configured with:</p> <ul> <li> <p><code>privileged: true</code></p> </li> <li> <p><code>cgroupns: private</code></p> </li> <li> <p>Missing <code>/sys/fs/cgroup:/sys/fs/cgroup:rw</code> volume mount</p> </li> </ul> <p>The systemd process inside the container requires proper cgroup access to function. Without the cgroup volume mount, systemd cannot initialize properly, causing the container to fail and restart repeatedly.</p>"},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/#solution","title":"Solution","text":"<p>Use the recommended docker-compose.yml configuration from the repository which includes:</p> <pre><code>services:\n  testpoint:\n    container_name: perfsonar-testpoint\n    image: hub.opensciencegrid.org/osg-htc/perfsonar-testpoint:production\n    network_mode: \"host\"\n    cgroup: host  # Use cgroup: host instead of cgroupns: private\n    environment:\n      - TZ=UTC\n    restart: unless-stopped\n    tmpfs:\n      - /run\n      - /run/lock\n      - /tmp\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:rw  # REQUIRED for systemd\n      - /opt/perfsonar-tp/psconfig:/etc/perfsonar/psconfig:Z\n      - /var/www/html:/var/www/html:z\n      - /etc/apache2:/etc/apache2:z\n      - /etc/letsencrypt:/etc/letsencrypt:z\n    tty: true\n    pids_limit: 8192\n    cap_add:\n      - CAP_NET_RAW\n    labels:\n      - io.containers.autoupdate=registry\n</code></pre>"},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/#fixing-existing-deployments","title":"Fixing Existing Deployments","text":"<p>If you have an existing deployment with the restart loop issue:</p> <ol> <li>Stop the containers:</li> </ol> <pre><code>cd /opt/perfsonar-tp\npodman-compose down\n</code></pre> <ol> <li>Update the docker-compose.yml file to use the recommended configuration from:</li> </ol> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/docker-compose.yml \\\n\n    -o /opt/perfsonar-tp/docker-compose.yml\n</code></pre> <ol> <li>Restart the service:</li> </ol> <pre><code>systemctl restart perfsonar-testpoint\n</code></pre>"},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/#verification","title":"Verification","text":"<p>Check that containers are running properly:</p> <pre><code>podman ps\nsystemctl status perfsonar-testpoint\n</code></pre> <p>The perfsonar-testpoint container should show status \"Up\" and not be restarting.</p>"},{"location":"perfsonar/CONTAINER_RESTART_ISSUE/#related-files","title":"Related Files","text":"<ul> <li> <p>Recommended compose file: <code>docs/perfsonar/tools_scripts/docker-compose.yml</code></p> </li> <li> <p>Systemd service installer: <code>docs/perfsonar/tools_scripts/install-systemd-service.sh</code></p> </li> <li> <p>Installation guide (legacy): <code>docs/perfsonar/installation.md</code> (replaces <code>install-testpoint.md</code>)</p> </li> </ul>"},{"location":"perfsonar/FIX_SUMMARY/","title":"perfSONAR Testpoint Container Restart Fix - Summary","text":""},{"location":"perfsonar/FIX_SUMMARY/#problem","title":"Problem","text":"<p>After a host reboot, podman-compose containers for perfSONAR testpoint did not restart automatically because there was no systemd service to manage them. Additionally, the deployed docker-compose.yml configuration had issues that caused the testpoint container to enter a restart loop.</p>"},{"location":"perfsonar/FIX_SUMMARY/#solutions-implemented","title":"Solutions Implemented","text":""},{"location":"perfsonar/FIX_SUMMARY/#1-created-systemd-service-for-automatic-restart","title":"1. Created Systemd Service for Automatic Restart","text":"<p>File: <code>/etc/systemd/system/perfsonar-testpoint.service</code></p> <p>A systemd service unit was created to manage the podman-compose containers and ensure they start automatically on boot.</p> <p>Key features:</p> <ul> <li> <p>Type: oneshot with RemainAfterExit</p> </li> <li> <p>Starts after network-online.target</p> </li> <li> <p>Runs <code>podman-compose up -d</code> on start</p> </li> <li> <p>Runs <code>podman-compose down</code> on stop</p> </li> <li> <p>Automatic restart on failure</p> </li> <li> <p>Enabled by default to start on boot</p> </li> </ul> <p>Status: Service is now installed, enabled, and running successfully on <code>/opt/perfsonar-tp</code>.</p>"},{"location":"perfsonar/FIX_SUMMARY/#2-fixed-container-configuration-issue","title":"2. Fixed Container Configuration Issue","text":"<p>File: <code>/opt/perfsonar-tp/docker-compose.yml</code></p> <p>Updated the docker-compose.yml to use the recommended configuration:</p> <ul> <li> <p>Changed from <code>cgroupns: private</code> to <code>cgroup: host</code></p> </li> <li> <p>Added required <code>/sys/fs/cgroup:/sys/fs/cgroup:rw</code> volume mount</p> </li> <li> <p>Added <code>tty: true</code> for proper terminal handling</p> </li> <li> <p>Removed custom entrypoint wrapper that was causing initialization issues</p> </li> </ul> <p>Result: Containers now start properly without entering a restart loop.</p>"},{"location":"perfsonar/FIX_SUMMARY/#3-created-helper-script","title":"3. Created Helper Script","text":"<p>File: <code>docs/perfsonar/tools_scripts/install-systemd-service.sh</code></p> <p>A new helper script automates the installation and configuration of the systemd service.</p> <p>Features:</p> <ul> <li> <p>Root privilege check</p> </li> <li> <p>Validates podman-compose is installed</p> </li> <li> <p>Validates installation directory exists</p> </li> <li> <p>Creates systemd service file</p> </li> <li> <p>Reloads systemd and enables service</p> </li> <li> <p>Supports custom installation paths</p> </li> <li> <p>Provides helpful usage examples</p> </li> </ul> <p>Usage:</p> <pre><code>sudo bash install-systemd-service.sh [/opt/perfsonar-tp]\n</code></pre>"},{"location":"perfsonar/FIX_SUMMARY/#4-updated-documentation","title":"4. Updated Documentation","text":"<p>Files Updated:</p> <ul> <li> <p><code>docs/perfsonar/installation.md</code> - Added section on enabling automatic restart (legacy: previously <code>install-testpoint.md</code>)</p> </li> <li> <p><code>docs/perfsonar/tools_scripts/README.md</code> - Added systemd service installer documentation</p> </li> <li> <p><code>docs/perfsonar/CONTAINER_RESTART_ISSUE.md</code> - New troubleshooting guide for container restart issues</p> </li> </ul> <p>Key additions:</p> <ul> <li> <p>Instructions for installing systemd service (manual and automated)</p> </li> <li> <p>Useful systemctl commands for managing the service</p> </li> <li> <p>Explanation of why systemd service is needed</p> </li> <li> <p>Troubleshooting guidance</p> </li> </ul>"},{"location":"perfsonar/FIX_SUMMARY/#5-created-ansible-playbook","title":"5. Created Ansible Playbook","text":"<p>File: <code>ansible/playbooks/deploy-testpoint-container.yml</code></p> <p>A complete Ansible playbook for automated deployment of perfSONAR testpoint containers.</p> <p>Features:</p> <ul> <li> <p>Installs required packages (podman, podman-compose, etc.)</p> </li> <li> <p>Downloads and installs tools_scripts</p> </li> <li> <p>Deploys docker-compose.yml</p> </li> <li> <p>Installs and enables systemd service</p> </li> <li> <p>Verifies containers are running</p> </li> <li> <p>Includes health checks</p> </li> </ul> <p>Also updated: <code>ansible/README.md</code> with deployment options and usage examples.</p>"},{"location":"perfsonar/FIX_SUMMARY/#files-modified-in-repository","title":"Files Modified in Repository","text":""},{"location":"perfsonar/FIX_SUMMARY/#new-files-created","title":"New Files Created","text":"<ol> <li> <p><code>docs/perfsonar/tools_scripts/install-systemd-service.sh</code> - Systemd service installer</p> </li> <li> <p><code>ansible/playbooks/deploy-testpoint-container.yml</code> - Ansible deployment playbook</p> </li> <li> <p><code>docs/perfsonar/CONTAINER_RESTART_ISSUE.md</code> - Troubleshooting guide</p> </li> </ol>"},{"location":"perfsonar/FIX_SUMMARY/#files-updated","title":"Files Updated","text":"<ol> <li> <p><code>docs/perfsonar/install-testpoint.md</code> - Added automatic restart section</p> </li> <li> <p><code>docs/perfsonar/tools_scripts/README.md</code> - Added systemd service documentation</p> </li> <li> <p><code>ansible/README.md</code> - Added container deployment section</p> </li> </ol>"},{"location":"perfsonar/FIX_SUMMARY/#testing-and-verification","title":"Testing and Verification","text":""},{"location":"perfsonar/FIX_SUMMARY/#local-system-optperfsonar-tp","title":"Local System (/opt/perfsonar-tp)","text":"<p>\u2705 Systemd service created and enabled \u2705 Containers starting successfully \u2705 perfSONAR testpoint responding on https://localhost/ \u2705 Service will survive reboots (enabled in systemd)</p>"},{"location":"perfsonar/FIX_SUMMARY/#verification-commands","title":"Verification Commands","text":"<pre><code># Check service status\nsystemctl status perfsonar-testpoint\n\n# Check containers\npodman ps\n\n# Test web interface\ncurl -kSfI https://localhost/\n\n# View service logs\njournalctl -u perfsonar-testpoint -f\n</code></pre>"},{"location":"perfsonar/FIX_SUMMARY/#next-steps-for-deployment","title":"Next Steps for Deployment","text":"<p>To apply these fixes to other perfSONAR testpoint deployments:</p> <ol> <li>Manual deployment:</li> </ol> <pre><code>curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-service.sh \\\n    -o /tmp/install-systemd-service.sh\nsudo bash /tmp/install-systemd-service.sh /opt/perfsonar-tp\n</code></pre> <ol> <li>Automated deployment with Ansible:</li> </ol> <pre><code>ansible-playbook -i inventory ansible/playbooks/deploy-testpoint-container.yml\n</code></pre> <ol> <li> <p>Fix existing deployments with restart issues:</p> </li> <li> <p>Update docker-compose.yml to recommended configuration</p> </li> <li> <p>Restart the perfsonar-testpoint service</p> </li> </ol>"},{"location":"perfsonar/FIX_SUMMARY/#benefits","title":"Benefits","text":"<p>\u2705 Automatic restart after reboot - Containers will always start when the host boots \u2705 Service management - Standard systemctl commands for start/stop/restart \u2705 Logging - Centralized logs via journalctl \u2705 Reliability - Automatic restart on failure \u2705 Automation - Ansible playbook for consistent deployments \u2705 Documentation - Clear instructions for users</p>"},{"location":"perfsonar/FIX_SUMMARY/#conclusion","title":"Conclusion","text":"<p>The perfSONAR testpoint container restart issue has been fully resolved. The systemd service is now managing the containers, ensuring they restart automatically after host reboots. Documentation and automation scripts have been updated to help others implement this fix easily.</p>"},{"location":"perfsonar/deployment-models/","title":"perfSONAR Deployment Options","text":"<p>The primary motivation for perfSONAR deployment is to test isolation, i.e. only one end-to-end test should run on a host at a time. This ensures that the test results are not impacted by the other tests. Otherwise it is much more difficult to interpret test results, which may vary due to host effects rather then network effects. Taking this into account it means that perfSONAR measurement tools are much more accurate running on a dedicated hardware and while it may be useful to run them on other hosts such as Data Transfer Nodes the current recommendation is to have specific measurement machine. In addition, as bandwidth testing could impact latency testing, we recommend to deploy two different nodes, each focused on specific set of tests. The following deployment options are currently available:</p> <ul> <li>Bare metal - preffered option in one of two possible configurations:</li> </ul> <pre><code>* Two bare metal servers, one for latency node, one for bandwidth node\n\n* One bare metal server running both latency and bandwidth node together provided that there are two NICs available, please refer to dual NIC section for more details on this.\n</code></pre> <ul> <li>Virtual Machine - if bare metal is not available then it is also possible to run perfSONAR on a VM, however there are a set of additional requirements to fulfill:</li> </ul> <pre><code>* Full-node VM is strongly preferred, having 2 VMs (latency/bandwidth node) on a single bare metal. Mixing perfSONAR VM(s) with others might have an impact on the measurements and is therefore not recommended.\n* VM needs to be configured to have SR-IOV to NIC(s) as well as pinned CPUs to ensure bandwidth tests are not impacted (by hypervisor switching CPUs during the test)\n* Succesfull full speed local bandwidth test is highly recommended prior to putting the VM into production\n</code></pre> <ul> <li>Container - perfSONAR has supported containers from version 4.1 (Q1 2018) and is documented at https://docs.perfsonar.net/install_docker.html but is not typically used in the same way as a full toolkit installation.</li> </ul> <pre><code>* Docker perfSONAR test instance can however still be used by sites that run multiple perfSONAR instances on site for their internal testing as this deployment model allows to flexibly deploy a testpoint which can send results to a local measurement archive running on the perfSONAR toolkit node.\n</code></pre>"},{"location":"perfsonar/deployment-models/#perfsonar-toolkit-vs-testpoint","title":"perfSONAR Toolkit vs Testpoint","text":"<p>The perfSONAR team has documented the types of installations supported at https://docs.perfsonar.net/install_options.html. With the release of version 5, OSG/WLCG sites can choose between two deployment options:</p>"},{"location":"perfsonar/deployment-models/#testpoint-container-based","title":"Testpoint (Container-based)","text":"<p>Installation guide: Install perfSONAR Testpoint</p> <ul> <li>Pros:</li> <li>Simpler deployment when a local web interface is not needed and a central measurement archive is available</li> <li>Less resource intensive for both memory and I/O capacity</li> <li>Container-based updates and management</li> <li>Recommended for most OSG/WLCG sites</li> </ul>"},{"location":"perfsonar/deployment-models/#toolkit-rpm-based","title":"Toolkit (RPM-based)","text":"<p>Installation guide: Install perfSONAR Toolkit</p> <ul> <li>Pros:</li> <li>Local web interface at <code>https://hostname/toolkit</code> for configuration and monitoring</li> <li>Local measurement archive (OpenSearch + Logstash) for on-site data retention</li> <li>Full-featured perfSONAR installation</li> <li>Suitable for sites with specific data retention or regulatory requirements</li> </ul> <p>See the deployment comparison for detailed feature comparison.</p> <ul> <li>Cons</li> </ul> <pre><code>* Measurements are not stored locally\n* No web interface to use for configuration or adding local tests\n* Unable to show results in MaDDash\n</code></pre> <p>While sites are free to choose whatever deployment method they want, we would like to strongly recommend the use of perfSONAR's containerized testpoint. This method was chosen as a \"best practice\" recommendation because of the reduced resource constraints, less components and easier management.</p>"},{"location":"perfsonar/deployment-models/#perfsonar-hardware-requirements","title":"perfSONAR Hardware Requirements","text":"<p>There are two different nodes participating in the network testing, latency node and bandwidth node, while both are running on the exact same perfSONAR toolkit, they have very different requirements. Bandwidth node measures available (or peak) throughput with low test frequency and will thus require NIC with high capacity (1/10/40/100G are supported) as well as enough memory and CPU to support high bandwidth testing. Our recommendation is to match bandwidth node NIC speed with the one installed on the storage nodes as this would provide us with the best match when there are issues to investigate. In case you'd like to deploy high speed (100G) bandwidth node, please consult ESNet tuning guide and 100G tuning presentation. Latency node on the other hand runs low bandwidth, but high frequency tests, sending a continuous stream of packets to measure delay and corresponding packet loss, packet reordering, etc. This means that while it doesn't require high capacity NIC, 1G is usually sufficient, it can impose significant load on the IO to disk as well as CPU as many tests run in parallel and need to continuously store its results into local measurement archive. The minimum hardware requirements to run perfSONAR toolkit are documented here. For WLCG/OSG deployment and taking into account the amount of testing that we perform, we recommend at least the following for perfSONAR 5.0+:</p> <ul> <li> <p>NIC for bandwidth node matching the capacity of the site storage nodes(10/25/40/100G), 1G NIC for latency node (for higher NIC capacities, 40/100G, please check ESNet tuning guide)</p> </li> <li> <p>High clock speede CPU (3.0 Ghz+), fwere cores OK, with at least 32GB+ of RAM (8GB+ if using a Testpoint install)</p> </li> <li> <p>NVMe or SSD disk (128GB should be sufficient) if using full Toolkit install with Opensearch.</p> </li> </ul>"},{"location":"perfsonar/deployment-models/#multiple-nic-network-interface-card-guidance","title":"Multiple NIC (Network Interface Card) Guidance","text":"<p>Many sites would prefer not to have to deploy two servers for cost, space and power reasons.  Since perfSONAR 3.5+ there is a way to install both latency and bandwidth measurement services on a single node, as long as it has at least two NICs (one per 'flavor' of measurement) and sufficient processing power and memory. There are few additional steps required in order to configure the node with multiple network cards:</p> <ul> <li> <p>Please setup source routing as described in the official documentation.</p> </li> <li> <p>You'll need to register two hostnames in OIM/GOCDB (and have two reverse DNS entries) as you would normally for two separate nodes.</p> </li> <li> <p>Instead of configuring just one auto-URL in for the remote URL, please add both, so you'll end up having something like this:</p> </li> </ul> <pre><code>psconfig remote add \"https://psconfig.opensciencegrid.org/pub/auto/&lt;FQDN_latency&gt;\"\npsconfig remote add \"https://psconfig.opensciencegrid.org/pub/auto/&lt;FQDN_throughput&gt;\"\n...\n</code></pre>"},{"location":"perfsonar/faq/","title":"Frequently Asked Questions","text":"<p>Here we will provide details on troubleshooting perfSONAR installations for OSG and WLCG as well as some additional configuration options and a FAQ.</p> <p>A good overview of existing tools provided by perfSONAR toolkit and examples how to use them to identify and isolate network problems can be found at:</p> <p>https://fasterdata.es.net/performance-testing/troubleshooting/network-troubleshooting-quick-reference-guide/</p> <p>We are maintaining a Network Troubleshooting page to guide users in identifying and following up on network problems.</p>"},{"location":"perfsonar/faq/#installing-a-certificate","title":"Installing a certificate","text":"<p>What is the recommended way to install a certificate on my perfSONAR host?</p> <p>We recommend using Let's Encrypt. There is a tutorial that users may find helpful at Secure Apache with Lets Encrypt.</p> <pre><code>&lt;summary&gt;A quick set of steps is shown here:&lt;/summary&gt;\n&lt;p&gt;0. Install certbot with yum, dnf, or snap:  **yum install certbot python3-certbot-apache**&lt;/p&gt;\n&lt;p&gt;1. Certbot needs port 80/443 so stop anything blocking it or using it:  **systemctl stop firewalld  systemctl stop httpd**&lt;/p&gt;\n&lt;p&gt;2. Test with dry-run:  **certbot certonly --standalone --preferred-challenges http --dry-run**&lt;/p&gt;\n&lt;p&gt;3. Get the certificate:  **certbot certonly --standalone --preferred-challenges http**&lt;/p&gt;\n&lt;p&gt;4. Restart httpd:  **systemctl start firewalld  systemctl start httpd**&lt;/p&gt;\n&lt;p&gt;5. Note certificate is installed under  /etc/letsencrypt/&lt;/p&gt;\n&lt;p&gt;6. Tell http where your certificate is:  Edit /etc/httpd/conf.d/ssl.conf&lt;/p&gt;\n    &lt;p&gt;1. Set **SSLCertificateFile** to /etc/letsencrypt/live/FQDN/cert.pem&lt;/p&gt;\n    &lt;p&gt;2. Set **SSLCertificateKeyFile** to /etc/letsencrypt/live/FQDN/privkey.pem&lt;/p&gt;\n    &lt;p&gt;3. Set **SSLCertificateChainFile** to /etc/letsencrypt/live/FQDN/fullchain.pem&lt;/p&gt;\n&lt;p&gt;7. Renew your certficate: **certbot renew --dry-run certbot renew**&lt;/p&gt;\n&lt;p&gt;8. Make a donation :)&lt;/p&gt;\n</code></pre> <p>Thanks to Raul Lopes for these details!</p>"},{"location":"perfsonar/faq/#network-troubleshooting","title":"Network Troubleshooting","text":"<ul> <li>I suspect there is a network performance issue impacting my site</li> </ul> <p>For OSG sites, please open a ticket with GOC. Otherwise please open a GGUS ticket (or assign an existing) one to WLCG Network Throughput support unit.</p>"},{"location":"perfsonar/faq/#service-registration","title":"Service Registration","text":"<p>I got an email after registering with lots of information in it...what do I do?</p> <p>This is part of the process. If you are a new site you will need to attend the next OSG operations meeting. If you are an existing site and have just registered perfSONAR instances you don't have to do anything but feel free to attend the next operations meeting if you have questions or concerns.</p> <p>Once I registered, new tickets were opened concerning perfSONAR...What do I do?</p> <p>This is standard operating procedure and the tickets are to ensure that OSG operations properly gets your new perfSONAR instances registered. You don't have to do anything and the tickets will be closed by OSG operations staff.</p>"},{"location":"perfsonar/faq/#infrastructure-monitoring-check_mk-metrics","title":"Infrastructure Monitoring (check_mk metrics)","text":"<ul> <li>perfSONAR services: versions metric is failing.</li> </ul> <p>This metrics checks if your sonar is at the most recent version. Please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running an older version (3.3-3.5), please update and reconfigure as soon as possible following Installation Guide</p> <ul> <li>perfSONAR configuration: contacts or location metrics are failing</li> </ul> <p>Please check if you have added the administrative information as detailed here</p> <ul> <li>perfSONAR services: bwctl/owamp/pscheduler metrics are failing</li> </ul> <p>This means that we're unable to connect to controller ports of the respective services, please ensure you have correct firewall settings (especially white listed subnets allowed) as described in the Installation Guide . This can also indicate failures of service daemons, please check http://docs.perfsonar.net/FAQ.html for additional details.</p> <ul> <li>perfSONAR services: esmond metric is failing</li> </ul> <p>This means that your measurement archive is not accessible or failing, there can be many possible causes (disk full, httpd not running or inaccessible, etc.), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support.</p> <ul> <li>perfSONAR json summary is failing</li> </ul> <pre><code>* This means the toolkit's homepage is inaccessible, which is required to check many additional services, so in turn all the other metrics will likely be in unknown or critical state. Please check for usual causes (disk full, httpd not running or blocked), we need to be able to access your homepage via HTTP or HTTPS\n</code></pre> <ul> <li>perfSONAR configuration: meshes metric is failing</li> </ul> <p>This indicates that you're missing the recommended mesh configuration. Please follow mesh configuration as detailed in the installation guide. Also, please REMOVE any old mesh configuration, this metric will also fail in case you have both the new mesh config and the old mesh URLs</p> <ul> <li>perfSONAR services: ntp is failing</li> </ul> <p>This indicates that NTP service is not running correctly on your toolkit instance, please note that NTP is critical service. Some things to check include your perfSONAR NTP configuration. If NTP is correctly configured, it is possible you could have a firewall issue:  port 123 UDP must be open.   There is NTP debugging information available on Google (e.g., https://support.ntp.org/bin/view/Support/TroubleshootingNTP). If you still have problems, please open a support ticket (see below).</p> <ul> <li>perfSONAR services: regular testing/pscheduler is failing</li> </ul> <p>This indicates that pscheduler is not working correctly. As this is the core daemon please contact WLCG perfSONAR support unit for help.</p> <ul> <li>There are many tests failing for given sonar, where should I start</li> </ul> <p>Please update and reconfigure your sonar following Installation Guide. Please ensure firewall doesn't block access from the whitelisted subnets that are required for the infrastructure monitoring to work.</p> <ul> <li>Where can I get support on managing WLCG perfSONAR ?</li> </ul> <p>You can open ticket in GGUS to WLCG perfSONAR support unit or contact directly wlcg-perfsonar-support (at cern.ch)</p> <ul> <li>perfSONAR esmond freshness Latency/Bandwidth Direct is failing or gives warning</li> </ul> <p>This metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured tests. This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it.</p> <ul> <li>perfSONAR services ndt/npad is failing</li> </ul> <p>Both metrics check if you have disabled NDT and NPAD. As both NDT and NPAD have been dropped starting with 4.0, this metrics should stay green in most of the cases.</p> <ul> <li>perfSONAR hardware check is failing</li> </ul> <p>Please consult the minimum and recommended hardware requirements.</p>"},{"location":"perfsonar/install-testpoint/","title":"perfSONAR Testpoint Installation (EL9)","text":""},{"location":"perfsonar/install-testpoint/#1-prerequisites","title":"1. Prerequisites","text":""},{"location":"perfsonar/install-testpoint/#bootstrap-the-perfsonar-testpoint-and-tools-recommended","title":"Bootstrap the perfSONAR testpoint and tools (recommended)","text":"<p>Use the bootstrap script to clone the perfSONAR testpoint repo and install helper scripts under /opt/perfsonar- tp/tools_scripts.</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh \\\n    -o /tmp/install_tools_scripts.sh\nchmod 0755 /tmp/install_tools_scripts.sh\n/tmp/install_tools_scripts.sh /opt/perfsonar-tp\n</code></pre>"},{"location":"perfsonar/install-testpoint/#ensure-the-host-is-up-to-date","title":"Ensure the host is up to date","text":"<pre><code>dnf update -y\n</code></pre>"},{"location":"perfsonar/install-testpoint/#install-required-packages","title":"Install required packages","text":"<pre><code>dnf install -y git podman podman-compose nftables iproute\n</code></pre> <p>Note: Podman is the default container engine on EL9. If you wish to use Docker instead, install it appropriately.</p>"},{"location":"perfsonar/install-testpoint/#2-deploy-the-perfsonar-testpoint-container","title":"2. Deploy the perfSONAR Testpoint Container","text":""},{"location":"perfsonar/install-testpoint/#obtain-a-compose-file","title":"Obtain a compose file","text":"<p>You can use a ready-to-run compose file maintained in the osg-htc/networking repository:</p> <pre><code>mkdir -p /opt/perfsonar-tp\ncurl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/docker-compose.yml \\\n    -o /opt/perfsonar-tp/docker-compose.yml\n</code></pre>"},{"location":"perfsonar/install-testpoint/#prepare-configuration-storage","title":"Prepare configuration storage","text":"<pre><code>mkdir -p /opt/perfsonar-tp/psconfig\n</code></pre>"},{"location":"perfsonar/install-testpoint/#edit-the-compose-file-as-needed","title":"Edit the compose file as needed","text":"<p>Edit /opt/perfsonar-tp/docker-compose.yml if you need to customize resource limits or volumes.</p>"},{"location":"perfsonar/install-testpoint/#launch-the-container","title":"Launch the container","text":"<pre><code>(cd /opt/perfsonar-tp; podman-compose up -d)\n</code></pre> <p>Or, if using Docker:</p> <pre><code>(cd /opt/perfsonar-tp; docker-compose up -d)\n</code></pre>"},{"location":"perfsonar/install-testpoint/#enable-automatic-container-restart-on-boot","title":"Enable automatic container restart on boot","text":"<p>To ensure containers restart automatically after a host reboot, install and enable the systemd service:</p> <pre><code># Using the helper script (recommended)\ncurl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-service.sh \\\n    -o /tmp/install-systemd-service.sh\nchmod +x /tmp/install-systemd-service.sh\nsudo /tmp/install-systemd-service.sh /opt/perfsonar-tp\n</code></pre> <p>Or manually create the service file:</p> <pre><code>sudo tee /etc/systemd/system/perfsonar-testpoint.service &gt; /dev/null &lt;&lt; 'EOF'\n[Unit]\nDescription=perfSONAR Testpoint Container Service\nAfter=network-online.target\nWants=network-online.target\nRequiresMountsFor=/opt/perfsonar-tp\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nWorkingDirectory=/opt/perfsonar-tp\nExecStart=/usr/bin/podman-compose up -d\nExecStop=/usr/bin/podman-compose down\nTimeoutStartSec=300\nRestart=on-failure\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl daemon-reload\nsudo systemctl enable perfsonar-testpoint.service\n</code></pre> <p>Useful commands:</p> <ul> <li> <p>Start service: <code>systemctl start perfsonar-testpoint</code></p> </li> <li> <p>Stop service: <code>systemctl stop perfsonar-testpoint</code></p> </li> <li> <p>Restart service: <code>systemctl restart perfsonar-testpoint</code></p> </li> <li> <p>Check status: <code>systemctl status perfsonar-testpoint</code></p> </li> <li> <p>View logs: <code>journalctl -u perfsonar-testpoint -f</code></p> </li> </ul>"},{"location":"perfsonar/install-testpoint/#3-configure-policy-based-routing-for-multi-homed-nics","title":"3. Configure Policy-Based Routing for Multi-Homed NICs","text":"<p>Recommended: use the helper script to generate and apply NetworkManager profiles and routing rules for multi-NIC hosts.</p> <ol> <li> <p>Preview generation (no changes):</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-debug\n</code></pre> </li> <li> <p>Generate the config file automatically:</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-auto\n</code></pre> </li> </ol> <p>Note: The auto-generator intentionally skips NICs that have neither an IPv4 nor an IPv6 gateway (e.g., management-only NICs) to avoid writing non-functional NetworkManager profiles. To include such a NIC in the configuration, set an explicit gateway or mark it as <code>DEFAULT_ROUTE_NIC</code> in <code>/etc/perfSONAR-multi-nic-config.conf</code>.</p> <p>Review and adjust /etc/perfSONAR-multi-nic-config.conf if needed.</p> <ol> <li> <p>Dry run the apply step:</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --dry-run --debug\n</code></pre> </li> <li> <p>Apply changes:</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --yes\n</code></pre> </li> </ol> <p>The script backs up current NetworkManager profiles and logs actions to /var/log/perfSONAR-multi-nic-config.log.</p> <p>If you prefer to configure rules manually, see the example below.</p>"},{"location":"perfsonar/install-testpoint/#manual-example","title":"Manual example","text":"<p>Suppose:</p> <ul> <li> <p>eth0 is for latency tests, IP \\= 192.168.10.10/24, GW \\= 192.168.10.1</p> </li> <li> <p>eth1 is for throughput tests, IP \\= 10.20.30.10/24, GW \\= 10.20.30.1</p> </li> </ul>"},{"location":"perfsonar/install-testpoint/#a-add-custom-routing-tables","title":"a) Add custom routing tables","text":"<p>Edit /etc/iproute2/rt_tables and add:</p> <p>200  eth0table 201  eth1table</p>"},{"location":"perfsonar/install-testpoint/#b-add-routes-and-rules-replace-ips-as-appropriate","title":"b) Add routes and rules (replace IPs as appropriate)","text":"<p># Add rules for eth0 (latency) ip rule add from 192.168.10.10/32 table eth0table</p> <p>ip route add 192.168.10.0/24 dev eth0 scope link table eth0table ip route add default via 192.168.10.1 dev eth0 table eth0table</p> <p># Add rules for eth1 (throughput) ip rule add from 10.20.30.10/32 table eth1table</p> <p>ip route add 10.20.30.0/24 dev eth1 scope link table eth1table ip route add default via 10.20.30.1 dev eth1 table eth1table</p>"},{"location":"perfsonar/install-testpoint/#c-make-persistent","title":"c) Make persistent","text":"<p>For persistent configuration, add these rules and routes to a script (e.g., ./perfsonar-policy-routing.sh in your working directory) and call it from /etc/rc.local (be sure /etc/rc.d/rc.local is executable and enabled), or use NetworkManager\u2019s connection profile route-rules and routes fields for the relevant interfaces.</p> <p>Example systemd unit:</p> <p># /etc/systemd/system/perfsonar-policy-routing.service [Unit] Description\\=PerfSONAR Policy Routing After\\=network.target</p> <p>[Service] Type\\=oneshot ExecStart\\=/path/to/your/working/dir/perfsonar-policy-routing.sh</p> <p>[Install] WantedBy\\=multi-user.target</p> <p>Enable it:</p> <p>systemctl enable --now perfsonar-policy-routing</p>"},{"location":"perfsonar/install-testpoint/#4-firewall-and-security","title":"4. Firewall and security","text":"<p>Recommended: configure nftables (and optionally SELinux and Fail2Ban) using the helper script.</p> <ol> <li>Run with options:</li> </ol> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-install-nftables.sh --selinux --fail2ban --yes\n</code></pre> <ol> <li>Preview rules only:</li> </ol> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-install-nftables.sh --print-rules\n</code></pre> <p>The script writes rules to /etc/nftables.d/perfsonar.nft and logs to /var/log/perfSONAR-install-nftables.log.</p>"},{"location":"perfsonar/install-testpoint/#manual-nftables-example-optional","title":"Manual nftables example (optional)","text":"<p>Below is a sample NFTables rule set that</p> <ul> <li> <p>Allows required perfSONAR measurement ports (especially for testpoint: traceroute, iperf3, OWAMP, etc.)</p> </li> <li> <p>Restricts SSH access to trusted subnets/hosts</p> </li> <li> <p>Accepts ICMP/ICMPv6 and related/permitted connections</p> </li> </ul> <p>/etc/nftables.conf:</p> <pre><code>flush ruleset\n\ntable inet perfsonar {\n\nset allowed_protocols { type inet_proto elements = { icmp, icmpv6 } }\nset allowed_interfaces { type ifname elements = { \"lo\" } }\nset allowed_tcp_dports { type inet_service elements = { 22, 443, 861, 862, 9090, 123, 5201, 5001, 5000, 5101 } }\nset allowed_udp_ports { type inet_service elements = { 123, 5201, 5001, 5000, 5101 } }\nchain allow {\n    ct state established,related accept\n    ct state invalid drop\n    meta l4proto @allowed_protocols accept\n    iifname @allowed_interfaces accept\n    tcp dport @allowed_tcp_dports ct state new accept\n    udp dport @allowed_udp_ports ct state new accept\n    # traceroute and test ranges\n    udp dport 33434-33634 ct state new accept\n    udp dport 18760-19960 ct state new accept\n    udp dport 8760-9960 ct state new accept\n    tcp dport 5890-5900 ct state new accept\n    # SSH controls (add your trusted IPs/subnets)\n    tcp dport 22 ip saddr 192.168.10.0/24 accept\n    tcp dport 22 ip saddr 10.20.30.0/24 accept\n}\nchain input {\n    type filter hook input priority 0; policy drop;\n    jump allow\n    reject with icmpx admin-prohibited\n}\n</code></pre> <p>Apply and persist:</p> <p>nft -f /etc/nftables.conf systemctl enable --now nftables</p>"},{"location":"perfsonar/install-testpoint/#5-optional-perfsonar-testpoint-container-networking","title":"5. Optional: perfSONAR Testpoint Container Networking","text":"<p>If you want the container to use a specific NIC, adjust the docker-compose.systemd.yml to use --network host, or configure the container\u2019s network accordingly. By default, host mode is recommended for testpoint deployments to avoid NAT and ensure direct packet timing.</p>"},{"location":"perfsonar/install-testpoint/#6-confirm-operation","title":"6. Confirm Operation","text":"<p>Check containers:</p> <pre><code>podman ps\n\n# or\n\ndocker ps\n</code></pre> <p>Check logs:</p> <pre><code>podman logs perfsonar-testpoint\n</code></pre> <p>Test connectivity between testpoints.</p>"},{"location":"perfsonar/install-testpoint/#7-optional-configure-perfsonar-remotes","title":"7. (Optional) Configure perfSONAR Remotes","text":"<p>To register your testpoint with a central config:</p> <pre><code>podman exec -it perfsonar-testpoint psconfig remote list\npodman exec -it perfsonar-testpoint psconfig remote --configure-archives add \"https://psconfig.opensciencegrid.org/pub/auto/psb02-gva.cern.ch\"\n--configure-archives add \"&lt;https://psconfig.opensciencegrid.org/pub/auto/psb02-gva.cern.ch&gt;\"\n</code></pre>"},{"location":"perfsonar/install-testpoint/#8-references-further-reading","title":"8. References &amp; Further Reading","text":"<ul> <li>perfSONAR testpoint Docker GitHub</li> <li>perfSONAR Documentation</li> <li>Red Hat Policy Routing [BROKEN-LINK: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/assembly_configuring-policy-based-routing_configuring-and-managing-networking]</li> <li>NFTables Wiki</li> </ul>"},{"location":"perfsonar/installation/","title":"perfSONAR Installation Guide","text":"<p>Legacy Documentation - Modern Guides Available</p> <p>This page contains legacy instructions for traditional installations. As of October 2025, we provide modern installation guides:</p> <p>\ud83d\udc49 For new installations, choose one:</p> <ul> <li>Testpoint (Container) - Recommended for most sites</li> <li>Toolkit (RPM) - For sites needing local web UI and archive</li> <li>Compare options - See deployment comparison</li> </ul> <p>This legacy guide is maintained for reference and special cases only.</p> <p>This page documents installing/upgrading perfSONAR for OSG and WLCG sites. In case this is the first time you're trying to install and integrate your perfSONAR into WLCG or OSG, please consult our overview and possible deployment options before installing. For troubleshooting an existing installation please consult official Troubleshooting Guide, FAQ as well as WLCG/OSG specific FAQ.</p> <p>For any questions or help with WLCG perfSONAR setup, please contact GGUS WLCG perfSONAR support unit or OSG GOC. We strongly recommend anyone maintaining/using perfSONAR to join perfsonaruser and perfsonar- announce mailing lists.</p>"},{"location":"perfsonar/installation/#installation-or-upgrade","title":"Installation or Upgrade","text":"<p>Prior to installing please consult the release notes) for the latest available release. In case you have already an instance running and wish to re-install/update it then please follow our recommendations:</p> <ul> <li> <p>Upgrades: We recommend reinstalling using an EL9 (RHEL,Rocky,Alma) OS for all sites already running a registered instance or planning new installation. The primary reason for this recommendation is to provide a long-term supported OS and to benefit from a 5.x kernel.</p> </li> <li> <p>perfSONAR team provides support for Debian9 and Ubuntu as well, but we recommend to use EL9 to have a common, well understood deployment.</p> </li> <li> <p>Local measurement archive backup is not needed as OSG/WLCG stores all measurements centrally.</p> </li> <li> <p>In case you plan to deploy a single bare metal node with multiple NICs, please consult Multiple NIC Guidance</p> </li> </ul> <p>First, install your chosen EL9 operating system on your host after saving you local configuration if you are \"updating\".</p> <p>The following options are then recommended to install perfSONAR for OSG/WLCG:</p> <p>| Installation method | Link | |---------------------|------| | Toolkit bundle installation | Toolkit Installation Quick Start | | Testpoint bundle installation | Follow the Quick Start above but use <code>dnf install perfsonar-testpoint</code> instead of the toolkit |</p> <p>You can see more details about EL supported installs at &lt;https://docs.perfsonar.net/install_el.html&gt;</p> <p>Note</p> <p>In all cases, we strongly recommend keeping auto-updates enabled. With yum auto-updates there is a possibility that updated packages can \"break\" your perfSONAR install but this risk is accepted in order to have security updates quickly applied.</p> <p>The following additional steps are needed to configure the toolkit to be used in OSG/WLCG in addition to the steps described in the official guide:</p> <ul> <li> <p>Please register your nodes in GOCDB/OIM. For OSG sites, follow the details in OSG Topology below. For non-OSG sites, follow the details in GOCDB</p> </li> <li> <p>Please ensure you have added or updated your administrative information</p> </li> <li> <p>You will need to configure your instance(s) to use the OSG/WLCG mesh-configuration. Please follow the steps below:</p> </li> <li> <p>For toolkit versions 5.0 and higher run: <code>psconfig remote add https://psconfig.opensciencegrid.org/pub/auto/&lt;FQDN&gt;</code> replacing <code>&lt;FQDN&gt;</code> with your host (e.g. <code>psum01.aglt2.org</code>). Verify with <code>psconfig remote list</code>.</p> </li> </ul> <p><code>json === pScheduler Agent === [    {       \"url\" : \"https://psconfig.opensciencegrid.org/pub/auto/psum01.aglt2.org\"       \"configure-archives\" : true    } ]</code></p> <ul> <li> <p>Please remove any old/stale URLs using <code>psconfig remote delete &lt;URL&gt;</code></p> </li> <li> <p>If this is a new instance or you have changed the node's FQDN, you will need to notify     <code>wlcg-perfsonar-support 'at' cern.ch</code> to add/update the hostname in one or more test meshes, which     will then auto-configure the tests. Please indicate if you have preferences for which meshes your     node should be included in (USATLAS, USCMS, ATLAS, CMS, LHCb, Alice, BelleII, etc.). You could also     add any additional local tests via web interface (see Configuring regular tests for details).     Please check which tests are auto-added via central meshes before adding any custom tests to avoid     duplication.</p> </li> </ul> <p>Note</p> <pre><code>Until your host is added on https://psconfig.opensciencegrid.org to one or more meshes by an administrator the automesh configuration above will not return any tests.\n</code></pre> <ul> <li> <p>We strongly recommend configuring perfSONAR in dual-stack mode (both IPv4 and IPv6). In case your site has IPv6 support, the only necessary step is to get both A and AAAA records for your perfSONAR DNS names (as well as ensuring the reverse DNS is in place).</p> </li> <li> <p>Adding communities is optional, but if you do, we recommend putting in WLCG as well as your VO: <code>ATLAS</code>, <code>CMS</code>, etc. This just helps others from the community lookup your instances in the public lookup service. As noted in the documentation you can select from already registered communities as appropriate.</p> </li> <li> <p>Please check that both local and campus firewall has the necessary port openings. Local iptables are configured automatically, but there are ways how to tune the existing set, please see the official firewall guide for details.</p> </li> <li> <p>Once installation is finished, please reboot the node.</p> </li> </ul> <p>For any further questions, please consult official Troubleshooting Guide, FAQ as well as WLCG/OSG specific FAQ or contact directly WLCG or OSG perfSONAR support units.</p>"},{"location":"perfsonar/installation/#maintenance","title":"Maintenance","text":"<p>Provided that you have enabled auto-updates, the only thing that remains is to follow up on any kernel security issues and either patch the node as soon as possible or reboot once the patched kernel is released.</p> <p>In case you'd like to manually update the node please follow the official guide.</p> <p>Using automated configuration tools (such as Chef, Puppet, etc) for managing perfSONAR are not officially supported, but there are some community driven projects that could be helpful, such as HEP- Puppet. As perfSONAR manages most of its configuration automatically via packages and there is very little initial configuration needed, we suggest to keep automated configuration to the minimum necessary to avoid unncessary interventions after auto-updates.</p>"},{"location":"perfsonar/installation/#security-considerations","title":"Security Considerations","text":"<p>The perfSONAR toolkit is reviewed both internally and externally for security flaws and the official documentation provides a lot of information on what security software is available and what firewall ports need to be opened, please see Manage Security for details. The toolkit's purpose is to allow us to measure and diagnose network problems and we therefore need to be cautious about blocking needed functionality by site or host firewalls.   An overview of perfSONAR security is available at https://www.perfsonar.net/deployment_security.html</p> <p>Warning</p> <p>All perfSONAR instances must have port 443 accessible to other perfSONAR instances. Port 443 is used by pScheduler to schedule tests. If unreachable, tests may not run and results may be missing.</p> <p>For sites that are concerned about having port 443 open, there is a possiblity to get a list of hosts to/from which the tests will be initiated. However as this list is dynamic, implementing the corresponding firewall rules would need to be done both locally and on the central/campus firewall in a way that would ensure dynamic updates. It's important to emphasize that port 443 provides access to the perfSONAR web interface as well, which is very useful to users and network administrators to debug network issues.</p> <p>Warning</p> <p>If you have a central/campus firewall verify required port openings in the perfSONAR security documentation. </p>"},{"location":"perfsonar/installation/#enabling-snmp-plugins","title":"Enabling SNMP plugins","text":"<p>Starting from release 4.0.2, perfSONAR toolkit allows to configure passive SNMP traffic from the local routers to be captured and stored in the local measurement archive. This is currently a beta feature that needs further testing and we're looking for volunteers willing to test, please let us know in case you would be interested.</p>"},{"location":"perfsonar/installation/#register-perfsonar-service-in-gocdb","title":"Register perfSONAR Service in GOCDB","text":"<p>This section describes how to register the perfSONAR service in GOCDB.</p> <p>In order to register you perfSONAR services in GOCDB, you should access the proper section of GOC for adding a Service Endpoint</p> <ul> <li>https://goc.egi.eu/portal/index.php?Page_Type=New_Service_Endpoint</li> </ul> <p>You might not be able to access the page if you are not properly registered in GOC, so a snapshot can be found below. In filling the information please follow those simple guidelines:</p> <ul> <li> <p>There are two service types for perfSONAR: net.perfSONAR.Bandwidth and net.perfSONAR.Latency. This is because we suggest to install two perfSONAR boxes at the site (one for latency tests and one for bandwidth tests).</p> </li> <li> <p>Because we recommend separate boxes, two distinct service endpoints should be published with two distinct service types. If a site cannot afford additional hardware, it can install a single perfSONAR box but should still publish both service types (using the same host in the \"host name\" field of the form).</p> </li> <li> <p>For each form (service type) fill at least:</p> <p> * Hosting Site * Service Type * Host Name * Host IP (optional) * Description (optional label used in MaDDash; keep short and unique) </p> </li> <li> <p>Check \"N\" when asked \"Is it a beta service\"</p> </li> <li> <p>Check \"Y\" when asked \"Is this service in production\"</p> </li> <li> <p>Check \"Y\" when asked \"Is this service monitored\"</p> </li> </ul> <ul> <li>GOCDB screen shot for creating a Service Endpoint: </li> </ul>"},{"location":"perfsonar/installation/#register-perfsonar-in-osg-topology","title":"Register perfSONAR in OSG Topology","text":"<p>Each OSG site should have two perfSONAR instances (one for Latency and one for Bandwidth) installed to enable network monitoring. These instances should be located as \"close\" (in a network-sense) as possible to the site's storage. If a logical site is comprised of more than one physical site, each physical site should be instrumented with perfSONAR instances.</p> <p>To add hosts to OSG Topology, please follow the instructions at &lt;https://osg-htc.org/docs/common/registration/&gt;</p> <p>If you have problems or questions please consult our FAQ or alternatively open a ticket with GOC.</p>"},{"location":"perfsonar/multiple-nic-guidance/","title":"Multiple NIC Guidance","text":""},{"location":"perfsonar/multiple-nic-guidance/#multiple-nic-network-interface-card-guidance","title":"Multiple NIC (Network Interface Card) Guidance","text":"<p>The OSG and WLCG recommendation is to deploy two flavors of perfSONAR measurement nodes: 1) a latency instance which continuously measures packet delay and lost and 2) a bandwidth instance measuring the achievable bandwidth.  That implies sites must purchase, deploy and maintain two systems.  Why can't we just run both latency and bandwidth services on a single instance? The problem is that running both latency and bandwidth services on a single node / single NIC may cause interence between the various measurements and introduce \"false-positive\" indications of network problems.</p> <p>Many sites would prefer not to have to deploy two servers for cost, space and power reasons.  Fortunately the perfSONAR developers have provided a way to install both latency and bandwidth measurements services on a single node, as long as it has at least two NICs (one per 'flavor' of measurement) and sufficient processing and memory.  See manage-dual-xface for details on configuring this.</p>"},{"location":"perfsonar/packet-pacing/","title":"Packet Pacing on Linux: fq vs tbf","text":"<p>This document explains how Linux packet pacing works with the <code>fq</code> (Fair Queue) queuing discipline and how it differs from explicit rate limiting with <code>tbf</code> (Token Bucket Filter). It also clarifies how these interact with tuned profiles and site use-cases for perfSONAR/DTN hosts.</p>"},{"location":"perfsonar/packet-pacing/#summary","title":"Summary","text":"<ul> <li>fq: Enables per-flow TCP pacing. Smooths bursts and respects per-flow pacing rates set by the kernel or applications. No hard interface cap.</li> <li>tbf: Enforces an interface-level cap. Useful for policy or hardware constraints. Not per-flow; applies to all traffic through the qdisc.</li> <li>tuned: The <code>network-throughput</code> profile commonly sets <code>net.core.default_qdisc=fq</code>, which activates TCP pacing when the interface uses <code>fq</code>.</li> </ul>"},{"location":"perfsonar/packet-pacing/#fq-fair-queue-and-tcp-pacing","title":"fq (Fair Queue) and TCP Pacing","text":"<ul> <li>Per-flow queues: <code>fq</code> maintains separate queues per flow (e.g., TCP connections).</li> <li>SO_MAX_PACING_RATE: Applications may call <code>setsockopt()</code> with <code>SO_MAX_PACING_RATE</code> to set a desired max pacing rate for a socket. Examples: <code>iperf3</code> and some data transfer tools set this.</li> <li>Kernel-driven pacing: Even when applications do not set <code>SO_MAX_PACING_RATE</code>, modern Linux TCP computes pacing based on congestion window and RTT, and <code>sch_fq</code> uses <code>sk-&gt;sk_pacing_rate</code> to schedule packets, smoothing bursts.</li> <li>Default behavior: If no pacing rate is set, traffic is not hard-capped by <code>fq</code>; flows still benefit from isolation and burst smoothing.</li> <li>Why use fq: Aligns with Fasterdata guidance for high-throughput hosts; improves fairness and latency under load; works well with BBR/CUBIC.</li> </ul>"},{"location":"perfsonar/packet-pacing/#useful-fq-knobs-advanced","title":"Useful <code>fq</code> knobs (advanced)","text":"<ul> <li><code>tc qdisc add dev &lt;if&gt; root fq [quantum &lt;bytes&gt;] [flow_limit &lt;packets&gt;] [low_rate_threshold &lt;bps&gt;]</code></li> <li>Most sites do not need to tune these; the defaults work well with modern kernels.</li> </ul>"},{"location":"perfsonar/packet-pacing/#tbf-token-bucket-filter","title":"tbf (Token Bucket Filter)","text":"<ul> <li>Interface-level rate cap: <code>tbf</code> enforces a maximum transmission rate using a token bucket, with <code>rate</code>, <code>burst</code>, and <code>latency</code> parameters.</li> <li>Not per-flow: All traffic shares the cap; can reduce overall throughput if set below link capacity.</li> <li>When to use: Site policy requiring caps; hardware or upstream constraints; lab/testing scenarios.</li> </ul>"},{"location":"perfsonar/packet-pacing/#example-tbf-command","title":"Example <code>tbf</code> command","text":"<p><pre><code># Cap to 2 Gbps, allow ~1ms burst, 100ms latency budget\nsudo tc qdisc replace dev eth0 root tbf rate 2000mbps burst 250000 latency 100ms\n</code></pre> - Burst guidance: Roughly <code>(rate * 1ms) / 8</code> bytes; clamp between ~1500 bytes and ~10MB. - Remove tbf: <code>sudo tc qdisc del dev eth0 root</code> or replace with <code>fq</code>.</p>"},{"location":"perfsonar/packet-pacing/#interaction-with-tuned","title":"Interaction with tuned","text":"<ul> <li><code>tuned-adm profile network-throughput</code> typically sets <code>net.core.default_qdisc=fq</code> (via sysctl), so new interfaces use <code>fq</code>.</li> <li>Audits should accept <code>fq</code> (TCP pacing) or <code>tbf</code> (explicit cap) as \u201cpacing applied,\u201d depending on site intent.</li> <li>If tuned or other tooling configures <code>tbf</code> unexpectedly, confirm site requirements before changing it to <code>fq</code>.</li> </ul>"},{"location":"perfsonar/packet-pacing/#perfsonardtn-considerations","title":"perfSONAR/DTN considerations","text":"<p>Measurement hosts: Prefer <code>fq</code> for smoothing bursts without artificial caps.  DTN hosts: Default to <code>fq</code>; add <code>tbf</code> only when a specific cap is required. The tuning script supports both paths:   - <code>--apply-packet-pacing</code> \u2192 sets <code>fq</code> (TCP pacing)   - <code>--apply-packet-pacing --use-tbf-cap</code> \u2192 applies <code>tbf</code> with an auto cap (default \u2248 90% of link speed)   - <code>--apply-packet-pacing --tbf-cap-rate 2000mbps</code> \u2192 applies <code>tbf</code> with an explicit cap (deprecated alias: <code>--packet-pacing-rate</code>)</p>"},{"location":"perfsonar/packet-pacing/#ui-cues-in-audits","title":"UI cues in audits","text":"<ul> <li><code>fq</code> is shown in green (preferred)</li> <li><code>tbf</code> is shown in cyan (acceptable when an interface cap is intentional)</li> </ul>"},{"location":"perfsonar/packet-pacing/#verification","title":"Verification","text":"<ul> <li>Show qdisc: <code>tc qdisc show dev &lt;if&gt;</code> should report <code>fq ...</code> or <code>tbf ...</code> at root.</li> <li>Check sysctl: <code>sysctl -n net.core.default_qdisc</code> should be <code>fq</code> under <code>network-throughput</code> tuned profile.</li> <li>Application-level: Tools like <code>iperf3</code> may set <code>SO_MAX_PACING_RATE</code>; otherwise kernel TCP pacing applies with <code>fq</code>.</li> </ul>"},{"location":"perfsonar/packet-pacing/#references","title":"References","text":"<ul> <li>ESnet Fasterdata: Host/network tuning guidance</li> <li>Linux <code>sch_fq</code> documentation and TCP pacing discussions</li> <li><code>tc</code> qdisc manual pages: <code>man tc</code>, <code>man tc-fq</code>, <code>man tc-tbf</code># Packet Pacing for Data Transfer Nodes</li> </ul>"},{"location":"perfsonar/packet-pacing/#overview","title":"Overview","text":"<p>Packet pacing is a critical tuning technique for high-performance Data Transfer Nodes (DTNs) and other hosts that need to move large amounts of data reliably across wide-area networks. By controlling the rate at which packets are sent from the host, packet pacing can dramatically reduce packet loss, prevent receiver buffer overflows, and improve overall throughput \u2014 sometimes by 2-4x on long paths.</p> <p>This document explains why packet pacing matters, how it works, and how to implement it on DTN nodes using Linux traffic control (<code>tc</code>).</p>"},{"location":"perfsonar/packet-pacing/#the-problem-data-rate-bottlenecks-and-packet-loss","title":"The Problem: Data Rate Bottlenecks and Packet Loss","text":"<p>When transferring data across a network, the effective throughput is limited by the minimum of three factors:</p> <ol> <li> <p>Source host read rate \u2014 How fast the sending host can read data from storage/memory</p> </li> <li> <p>Available network bandwidth \u2014 The capacity of the network path</p> </li> <li> <p>Destination write rate \u2014 How fast the receiving host can write data to storage/memory</p> </li> </ol>"},{"location":"perfsonar/packet-pacing/#common-bottleneck-scenarios","title":"Common Bottleneck Scenarios","text":""},{"location":"perfsonar/packet-pacing/#scenario-1-fast-source-slower-network-path","title":"Scenario 1: Fast Source, Slower Network Path","text":"<ul> <li> <p>A 10G DTN sends to a 1G receiver or via a 1G network path</p> </li> <li> <p>Without pacing, the DTN floods the network with packets</p> </li> <li> <p>The receiver cannot keep up, leading to buffer overflow and packet loss</p> </li> <li> <p>TCP backs off, causing dramatic throughput drops</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#scenario-2-multiple-parallel-streams","title":"Scenario 2: Multiple Parallel Streams","text":"<ul> <li> <p>A 10G DTN with 4-8 parallel GridFTP streams to a 10G receiver</p> </li> <li> <p>Total available bandwidth is 10G, but each stream tries to max out its connection</p> </li> <li> <p>Streams compete for buffers on the receiver</p> </li> <li> <p>Packet loss and TCP backing off reduce overall throughput</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#scenario-3-unbalanced-cpunetwork-performance","title":"Scenario 3: Unbalanced CPU/Network Performance","text":"<ul> <li> <p>A fast 40G/100G host with a slower CPU</p> </li> <li> <p>Network can send packets faster than the CPU can process them</p> </li> <li> <p>Receive-side bottleneck at the slower host</p> </li> <li> <p>Packet loss and retransmission overhead</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#scenario-4-long-distance-paths-50-80ms-rtt","title":"Scenario 4: Long-Distance Paths (50-80ms RTT)","text":"<ul> <li> <p>Network paths with high latency across continents</p> </li> <li> <p>Even with adequate bandwidth, mismatched send/receive rates cause issues</p> </li> <li> <p>Packets arrive faster than the receiver can drain them</p> </li> <li> <p>Studies show 2-4x throughput improvement with pacing on these paths</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#how-packet-pacing-works","title":"How Packet Pacing Works","text":"<p>Packet pacing solves this problem by controlling the rate at which packets leave the source host, ensuring the receiver is never overwhelmed and can process packets at a sustainable rate.</p>"},{"location":"perfsonar/packet-pacing/#mechanism-fair-queuing-fq-qdisc","title":"Mechanism: Fair Queuing (FQ) Qdisc","text":"<p>Modern Linux (kernel 3.11+) includes the Fair Queuing (FQ) scheduler, which implements sophisticated packet pacing. The FQ qdisc:</p> <ul> <li> <p>Maintains separate queues for different flows (distinguishing between different streams)</p> </li> <li> <p>Ensures fair bandwidth sharing \u2014 each flow gets an equal share of available bandwidth</p> </li> <li> <p>Paces packets intelligently \u2014 spreads packets out over time instead of sending them in bursts</p> </li> <li> <p>Reduces buffer pressure \u2014 keeps receiving end from being overwhelmed</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#fq-vs-fq_codel","title":"FQ vs FQ_CODEL","text":"<ul> <li> <p>FQ (Fair Queuing): Excellent for high-throughput TCP and data transfer</p> </li> <li> <p>FQ_CODEL: Default in modern kernels (4.12+), but less optimal for sustained high-throughput transfers</p> </li> </ul> <p>For DTN and high-speed data transfer, FQ is recommended over FQ_CODEL.</p>"},{"location":"perfsonar/packet-pacing/#rate-limiting-with-token-bucket-filter-tbf","title":"Rate Limiting with Token Bucket Filter (TBF)","text":"<p>The Token Bucket Filter (TBF) qdisc enforces a maximum rate limit by:</p> <ul> <li> <p>Accumulating tokens at a configured rate (e.g., 2 Gbps)</p> </li> <li> <p>Requiring tokens to send packets \u2014 each packet consumes tokens equal to its size</p> </li> <li> <p>Queuing packets when no tokens are available</p> </li> <li> <p>Smoothing traffic into a predictable, controlled rate</p> </li> </ul> <p>The burst size determines how many back-to-back packets can be sent before rate limiting kicks in. Typically, we calculate burst as 1-2ms worth of packets at the target rate.</p>"},{"location":"perfsonar/packet-pacing/#why-packet-pacing-works-esnet-research-results","title":"Why Packet Pacing Works: ESnet Research Results","text":"<p>ESnet's performance testing with Berkeley Lab and others has demonstrated significant improvements from packet pacing:</p>"},{"location":"perfsonar/packet-pacing/#key-findings","title":"Key Findings","text":"<ol> <li> <p>Reduced Packet Loss: By preventing receiver buffer overflow</p> </li> <li> <p>Higher Sustained Throughput: 2-4x improvements on long paths (50-80ms RTT)</p> </li> <li> <p>Better Resource Utilization: Prevents wasted retransmissions and TCP back-off</p> </li> <li> <p>Predictable Performance: More consistent results across different network conditions</p> </li> <li> <p>Multi-Stream Benefits: Especially effective with 4-8 parallel streams (common with GridFTP)</p> </li> </ol>"},{"location":"perfsonar/packet-pacing/#real-world-dtn-scenario","title":"Real-World DTN Scenario","text":"<p>Configuration: 10G DTN with 4 parallel GridFTP streams to a 10G receiver</p> <p>Without Pacing:</p> <ul> <li> <p>Bursts of packets overwhelm receiver</p> </li> <li> <p>Packet loss triggers TCP retransmissions</p> </li> <li> <p>TCP congestion control backs off aggressively</p> </li> <li> <p>Throughput: ~3-5 Gbps (underutilizing available 10G)</p> </li> </ul> <p>With Pacing at 2 Gbps per stream (8 Gbps total from 4 streams):</p> <ul> <li> <p>Smooth traffic reduces packet loss</p> </li> <li> <p>TCP can maintain higher congestion window</p> </li> <li> <p>Better resource utilization at receiver</p> </li> <li> <p>Throughput: ~8-9 Gbps (near line rate)</p> </li> </ul> <p>Result: 2-3x throughput improvement</p>"},{"location":"perfsonar/packet-pacing/#recommended-pacing-rates-for-common-scenarios","title":"Recommended Pacing Rates for Common Scenarios","text":""},{"location":"perfsonar/packet-pacing/#rule-of-thumb-80-90-of-nic-speed","title":"Rule of Thumb: 80-90% of NIC Speed","text":"<p>For a DTN with N parallel streams, divide available bandwidth accordingly:</p> <p>| Host NIC Speed | Parallel Streams | Recommended Per-Stream Rate | Example Command | | --- | --- | --- | --- | | 10G | 4 | 2 Gbps | <code>tc qdisc add dev eth0 root fq maxrate 2gbit</code> | | 10G | 8 | 1 Gbps | <code>tc qdisc add dev eth0 root fq maxrate 1gbit</code> | | 40G | 4 | 8 Gbps | <code>tc qdisc add dev eth0 root fq maxrate 8gbit</code> | | 40G | 8 | 5 Gbps | <code>tc qdisc add dev eth0 root fq maxrate 5gbit</code> | | 100G | 8 | 10-12 Gbps | <code>tc qdisc add dev eth0 root fq maxrate 10gbit</code> | | 100G (to 10G paths) | Any | 2 Gbps | <code>tc qdisc add dev eth0 root fq maxrate 2gbit</code> |</p>"},{"location":"perfsonar/packet-pacing/#rationale","title":"Rationale","text":"<ul> <li> <p>Conservative default: 2 Gbps \u2014 Works for most 10G-to-10G transfers, prevents overwhelming typical receivers</p> </li> <li> <p>Adjust based on RTT: Longer paths benefit from slightly lower rates</p> </li> <li> <p>Divide bandwidth: With 4 parallel streams on 10G NIC, 2 Gbps/stream = 8 Gbps total (80% utilization)</p> </li> <li> <p>Monitor and tune: Use <code>iperf3 --fq-rate</code> or perfSONAR pscheduler to test your specific path</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#implementation-using-fasterdata-tuningsh","title":"Implementation: Using fasterdata-tuning.sh","text":"<p>The <code>fasterdata-tuning.sh</code> script includes automated packet pacing configuration for DTN nodes.</p>"},{"location":"perfsonar/packet-pacing/#audit-current-state","title":"Audit Current State","text":"<p>Check what pacing rates are recommended for your DTN:</p> <pre><code>fasterdata-tuning.sh --mode audit --target dtn\n</code></pre> <p>Output shows:</p> <ul> <li> <p>Whether packet pacing is currently applied</p> </li> <li> <p>Recommended default rate: 2 Gbps (2000mbps)</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#apply-packet-pacing","title":"Apply Packet Pacing","text":"<p>Apply packet pacing with default 2 Gbps rate:</p> <pre><code>sudo fasterdata-tuning.sh --mode apply --target dtn --apply-packet-pacing\n</code></pre> <p>Apply with custom rate (e.g., for 100G host with 8 streams):</p> <pre><code>sudo fasterdata-tuning.sh --mode apply --target dtn --apply-packet-pacing --packet-pacing-rate 10gbps\n</code></pre> <p>Supported rate units: <code>kbps</code>, <code>mbps</code>, <code>gbps</code>, <code>tbps</code></p> <p>Examples:</p> <ul> <li> <p><code>2gbps</code> \u2014 2 Gigabits per second</p> </li> <li> <p><code>10000mbps</code> \u2014 10 Gigabits per second (equivalent to 10gbps)</p> </li> <li> <p><code>2000mbps</code> \u2014 2 Gigabits per second (default)</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#dry-run-preview","title":"Dry-Run Preview","text":"<p>Preview what would be applied without making changes:</p> <pre><code>sudo fasterdata-tuning.sh --mode apply --target dtn --apply-packet-pacing --dry-run\n</code></pre> <p>Output shows the exact <code>tc</code> commands that would be executed on each interface.</p>"},{"location":"perfsonar/packet-pacing/#burst-size-calculation","title":"Burst Size Calculation","text":"<p>The script automatically calculates burst size as 1 millisecond worth of packets:</p> <ul> <li> <p>2 Gbps \u2192 250 KB burst</p> </li> <li> <p>5 Gbps \u2192 625 KB burst</p> </li> <li> <p>10 Gbps \u2192 1.25 MB burst</p> </li> </ul> <p>Burst is clamped to safe bounds:</p> <ul> <li> <p>Minimum: 1,500 bytes (typical MTU size)</p> </li> <li> <p>Maximum: 10 MB (prevents excessive buffering)</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#manual-configuration-with-tc-command","title":"Manual Configuration with <code>tc</code> Command","text":"<p>If you prefer to configure packet pacing manually, use the <code>tc</code> command directly:</p>"},{"location":"perfsonar/packet-pacing/#check-current-qdisc","title":"Check Current Qdisc","text":"<pre><code>tc qdisc show dev eth0\n</code></pre>"},{"location":"perfsonar/packet-pacing/#set-fair-queuing-with-pacing","title":"Set Fair Queuing with Pacing","text":"<p>Replace <code>eth0</code> with your actual interface name:</p> <pre><code>sudo tc qdisc replace dev eth0 root fq maxrate 2gbit\n</code></pre>"},{"location":"perfsonar/packet-pacing/#verify-configuration","title":"Verify Configuration","text":"<pre><code>tc qdisc show dev eth0\ntc qdisc stat dev eth0\n</code></pre>"},{"location":"perfsonar/packet-pacing/#delete-pacing-revert-to-default","title":"Delete Pacing (Revert to Default)","text":"<pre><code>sudo tc qdisc del dev eth0 root\n</code></pre>"},{"location":"perfsonar/packet-pacing/#using-token-bucket-filter-tbf-instead","title":"Using Token Bucket Filter (TBF) Instead","text":"<p>For more granular control, use TBF instead of FQ:</p> <pre><code>sudo tc qdisc replace dev eth0 root tbf rate 2gbit burst 250000 latency 100ms\n</code></pre> <p>Where:</p> <ul> <li> <p><code>rate</code> = packet pacing rate (e.g., 2gbit, 5gbit)</p> </li> <li> <p><code>burst</code> = maximum burst size in bytes</p> </li> <li> <p><code>latency</code> = maximum queuing latency before dropping packets</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"perfsonar/packet-pacing/#verify-pacing-is-active","title":"Verify Pacing is Active","text":"<pre><code>sysctl net.core.default_qdisc\n# Should show: net.core.default_qdisc = fq\n\ntc qdisc show dev eth0\n# Should show: qdisc fq 8001: root refcnt 2 limit 10000p flows 1024 ...\n</code></pre>"},{"location":"perfsonar/packet-pacing/#test-with-iperf3","title":"Test with iperf3","text":"<p>iperf3 supports FQ-based pacing via the <code>--fq-rate</code> option:</p> <pre><code># Test WITH pacing (recommended)\niperf3 -c &lt;receiver&gt; -P 4 --time 60 --fq-rate 2gbps\n\n# Compare to WITHOUT pacing\niperf3 -c &lt;receiver&gt; -P 4 --time 60\n</code></pre> <p>Expected improvement: 10-50% higher throughput with pacing on long paths.</p>"},{"location":"perfsonar/packet-pacing/#test-with-perfsonar-pscheduler","title":"Test with perfSONAR pscheduler","text":"<p>perfSONAR's pscheduler also supports pacing. Check your perfSONAR configuration for pacing-aware tests.</p>"},{"location":"perfsonar/packet-pacing/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":""},{"location":"perfsonar/packet-pacing/#issue-pacing-not-applied","title":"Issue: Pacing Not Applied","text":"<p>Symptom: <code>tc qdisc show</code> shows <code>qdisc mq</code> instead of <code>qdisc fq</code></p> <p>Solution: Ensure <code>/etc/sysctl.conf</code> contains:</p> <pre><code>net.core.default_qdisc = fq\nsysctl -p\n</code></pre> <p>Then reapply pacing with <code>fasterdata-tuning.sh</code> or <code>tc</code> command.</p>"},{"location":"perfsonar/packet-pacing/#issue-throughput-still-low-after-pacing","title":"Issue: Throughput Still Low After Pacing","text":"<p>Causes:</p> <ul> <li> <p>Pacing rate too conservative \u2014 try increasing by 10-20%</p> </li> <li> <p>Receiver still bottlenecked \u2014 verify receiver can sustain higher rates</p> </li> <li> <p>Network path issue \u2014 check for packet loss with <code>mtr</code> or <code>iperf3</code></p> </li> </ul> <p>Debug Steps:</p> <ol> <li> <p>Test between same hosts in reverse direction (verify it's not sender-specific)</p> </li> <li> <p>Gradually increase pacing rate in 1-2 Gbps increments</p> </li> <li> <p>Monitor <code>tc -s qdisc show dev eth0</code> for dropped/delayed packets</p> </li> </ol>"},{"location":"perfsonar/packet-pacing/#issue-pacing-configuration-lost-after-reboot","title":"Issue: Pacing Configuration Lost After Reboot","text":"<p>Solution: The <code>fasterdata-tuning.sh</code> apply mode creates a systemd service for persistence. Enable it:</p> <pre><code>sudo systemctl enable ethtool-persist.service\nsudo systemctl start ethtool-persist.service\n</code></pre> <p>Verify:</p> <pre><code>sudo systemctl status ethtool-persist.service\ntc qdisc show dev eth0\n</code></pre>"},{"location":"perfsonar/packet-pacing/#when-not-to-use-packet-pacing","title":"When NOT to Use Packet Pacing","text":"<ul> <li> <p>Low-latency, low-throughput applications \u2014 Pacing adds latency</p> </li> <li> <p>Latency-sensitive protocols (HFT, gaming, VoIP) \u2014 Avoid pacing</p> </li> <li> <p>Measurement hosts \u2014 Pacing should not be applied to measurement/monitor hosts</p> </li> <li> <p>Low-bandwidth transfers \u2014 Pacing provides little benefit below 1G</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#advanced-per-application-pacing","title":"Advanced: Per-Application Pacing","text":"<p>If you need finer control than host-level pacing, applications can set pacing rates using the <code>SO_MAX_PACING_RATE</code> socket option:</p> <pre><code>#include &lt;sys/socket.h&gt;\n\n// In your application code:\nint pacing_rate = 2000000000;  // 2 Gbps in bytes per second\nsetsockopt(sockfd, SOL_SOCKET, SO_MAX_PACING_RATE,\n           &amp;pacing_rate, sizeof(pacing_rate));\n</code></pre> <p>Requirements:</p> <ul> <li> <p>Kernel 4.13+</p> </li> <li> <p>Host configured with <code>net.core.default_qdisc = fq</code> or <code>fq_codel</code></p> </li> <li> <p>Application code changes required</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#references_1","title":"References","text":""},{"location":"perfsonar/packet-pacing/#esnet-fasterdata-documentation","title":"ESnet Fasterdata Documentation","text":"<ul> <li> <p>DTN Tuning Guide: https://fasterdata.es.net/DTN/tuning/</p> </li> <li> <p>Packet Pacing Guide: https://fasterdata.es.net/host-tuning/linux/packet-pacing/</p> </li> <li> <p>FQ Pacing Research Results: https://fasterdata.es.net/assets/fasterdata/FQ-pacing-results.pdf</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#linux-kernel-documentation","title":"Linux Kernel Documentation","text":"<ul> <li> <p>tc-fq man page: <code>man 8 tc-fq</code></p> </li> <li> <p>tc-tbf man page: <code>man 8 tc-tbf</code></p> </li> <li> <p>tc man page: <code>man 8 tc</code></p> </li> <li> <p>LWN Article on FQ: https://lwn.net/Articles/564978/</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#tools-and-testing","title":"Tools and Testing","text":"<ul> <li> <p>iperf3: https://iperf.fr/ (with <code>--fq-rate</code> support)</p> </li> <li> <p>perfSONAR: https://www.perfsonar.net/ (pscheduler with pacing)</p> </li> <li> <p>mtr (traceroute tool): <code>mtr &lt;destination&gt;</code></p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#related-tuning","title":"Related Tuning","text":"<ul> <li> <p>SYSCTL Tuning: See <code>fasterdata-tuning.sh</code> for buffer sizing recommendations</p> </li> <li> <p>100G+ Tuning: https://fasterdata.es.net/host-tuning/linux/100g-tuning/</p> </li> <li> <p>BBR Congestion Control: https://fasterdata.es.net/host-tuning/linux/recent-tcp-enhancements/bbr-tcp/</p> </li> </ul>"},{"location":"perfsonar/packet-pacing/#summary_1","title":"Summary","text":"<p>Packet pacing is essential for DTN nodes and high-performance data transfer because it:</p> <ol> <li> <p>\u2705 Prevents receiver buffer overflow \u2014 Smooth traffic instead of bursts</p> </li> <li> <p>\u2705 Reduces packet loss \u2014 Eliminates TCP back-off and retransmission overhead</p> </li> <li> <p>\u2705 Improves throughput 2-4x \u2014 Especially on long paths with high latency</p> </li> <li> <p>\u2705 Fair bandwidth sharing \u2014 Each flow gets equal treatment</p> </li> <li> <p>\u2705 Easy to implement \u2014 Single command or script invocation</p> </li> </ol> <p>Recommended Configuration for 10G DTN with 4 parallel streams:</p> <pre><code>sudo fasterdata-tuning.sh --mode apply --target dtn --apply-packet-pacing --packet-pacing-rate 2gbps\n</code></pre> <p>Expected Result: Near-line-rate throughput with minimal packet loss.</p> <p>Last Updated: December 2025 References: ESnet Fasterdata, Linux kernel tc documentation</p>"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/","title":"perfSONAR use of Policy Based Routing and rp_filter on EL9","text":"<p>In a multihome setup on EL9, <code>sysctl rp_filter</code> and policy-based routing (PBR) address two different layers of network traffic handling and can conflict with each other. PBR determines the outbound path for traffic based on criteria, while <code>rp_filter</code> is a security feature that validates the source address of inbound traffic. If not configured properly, strict <code>rp_filter</code> can block legitimate traffic in a PBR setup.</p>"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#sysctl-rp_filter-overview","title":"<code>sysctl rp_filter</code> overview","text":"<p>The Reverse Path Filtering (<code>rp_filter</code>) kernel parameter is a security measure designed to prevent IP spoofing, often associated with Denial of Service (DoS) attacks. When enabled, it checks the source IP address of an incoming packet to ensure the return path for a response would exit through the same interface that received the packet. It can be configured with three values:</p> <ul> <li> <p><code>0</code>: Disabled. No source validation is performed. This is required for asymmetric routing setups.</p> </li> <li> <p><code>1</code>: Strict mode. The kernel performs a reverse-path lookup. If the best return path for the packet's source address is not the interface on which the packet was received, the packet is dropped.</p> </li> <li> <p><code>2</code>: Loose mode. The kernel only validates that the source IP is routable via any interface, not necessarily the one it arrived on.</p> </li> </ul>"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#policy-based-routing-pbr-overview","title":"Policy-based routing (PBR) overview","text":"<p>PBR is a technique for overriding the standard Linux routing behavior, which is typically based solely on the destination IP address. It allows administrators to route traffic based on other criteria, such as the source IP address, application, protocol, or firewall marks (<code>fwmark</code>). A PBR setup involves these key steps:</p> <ol> <li> <p>Create custom routing tables. Additional tables beyond the main routing table are defined, often in <code>/etc/iproute2/rt_tables</code>.</p> </li> <li> <p>Define rules. The <code>ip rule</code> command is used to add rules that select a specific routing table. Rules are matched against packets based on criteria like source address or firewall mark.</p> </li> <li> <p>Add routes to custom tables. Use the <code>ip route</code> command to add routes to the new tables.</p> </li> </ol>"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#the-conflict-and-solution","title":"The conflict and solution","text":"<p>The conflict arises when using strict <code>rp_filter</code> (value 1) in a multihomed network configured with PBR.</p> <ol> <li> <p>A server is configured with PBR to send traffic from a specific service (e.g., source IP A1) out through network interface <code>eth0</code>.</p> </li> <li> <p>The server's PBR rules also dictate that another service (e.g., source IP A2) sends traffic out through interface <code>eth1</code>.</p> </li> <li> <p>An external client sends a packet to the service on source IP A2.</p> </li> <li> <p>The server's PBR rules correctly select the route on <code>eth1</code> to send the response.</p> </li> <li> <p>However, if strict <code>rp_filter</code> is enabled, the kernel checks the incoming packet's source address and performs a reverse-path lookup. This lookup would likely find the best return path is through <code>eth0</code> (the primary default gateway) rather than the <code>eth1</code> interface where the packet arrived.</p> </li> <li> <p>The kernel drops the packet because of the <code>rp_filter</code> check, even though the PBR configuration would have correctly handled the outbound response.</p> </li> </ol>"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#solution-for-el9-multihome","title":"Solution for EL9 multihome:","text":"<p>To enable asymmetric routing with PBR, you must relax the <code>rp_filter</code> setting, as strict mode will cause legitimate packets to be dropped.</p> <p>The best practice is to use a value of <code>2</code> (loose mode) or <code>0</code> (disabled). The loose mode is safer as it still provides some protection against spoofing, while <code>0</code> completely disables the check.</p> <p>You can configure this persistently by editing a file in <code>/etc/sysctl.d/</code>, for example <code>/etc/sysctl.d/99-network.conf</code>:</p> <p># Disable strict reverse path filtering for asymmetric routing net.ipv4.conf.all.rp_filter=2</p> <p>After saving the file, apply the changes with <code>sysctl -p</code>.</p>"},{"location":"perfsonar/perfSONAR-and-rp_filter-EL9/#summary-of-differences","title":"Summary of differences","text":"<p>| Feature  | <code>sysctl rp_filter</code> | Policy-Based Routing (PBR) | | ----- | ----- | ----- | | Purpose | Security mechanism to prevent IP spoofing by checking inbound packet source addresses. | Advanced routing method to control outbound traffic path based on policies. | | Traffic direction | Checks incoming traffic. | Explicitly directs outgoing traffic. | | Mechanism | A single kernel parameter with three states (<code>0</code>, <code>1</code>, <code>2</code>) that applies globally or per-interface. | Uses multiple routing tables and rules (<code>ip rule</code>) to select the appropriate table for a given packet. | | Compatibility | Strict mode (<code>1</code>) conflicts with multihome setups involving asymmetric routing. | Is designed for multihome setups but requires loosening or disabling <code>rp_filter</code> to function correctly |</p>"},{"location":"perfsonar/perfSONAR-sysctl-info/","title":"Key sysctl settings for perfSONAR","text":"<p>For optimal performance, the perfSONAR Toolkit applies system tuning settings by default upon installation by using the <code>perfsonar-toolkit-sysctl</code> package. These settings are based on the ESnet \"fasterdata\" knowledge base for high- performance test and measurement hosts and are sufficient for most use cases. However, you can manually verify or adjust the settings in <code>/etc/sysctl.conf</code> for specific needs.</p>"},{"location":"perfsonar/perfSONAR-sysctl-info/#tcp-buffer-sizing","title":"TCP buffer sizing","text":"<p>These settings increase the maximum TCP buffer sizes to support high throughput, especially on high-speed (e.g., 10 Gbps and faster) networks over long distances.</p> <ul> <li> <p><code>net.core.rmem_max</code>: The maximum receive socket buffer size in bytes.</p> </li> <li> <p><code>net.core.wmem_max</code>: The maximum send socket buffer size in bytes.</p> </li> <li> <p><code>net.ipv4.tcp_rmem</code>: The minimum, default, and maximum receive buffer sizes for TCP.</p> </li> <li> <p><code>net.ipv4.tcp_wmem</code>: The minimum, default, and maximum send buffer sizes for TCP.</p> </li> </ul> <p>Example settings for a 10G or 40G host with up to 100ms round-trip time (RTT):</p> <p>net.core.rmem_max \\= 67108864 net.core.wmem_max \\= 67108864 net.ipv4.tcp_rmem \\= 4096 87380 33554432 net.ipv4.tcp_wmem \\= 4096 65536 33554432</p> <p>For 100G or higher speeds, you may need to increase these values even further.</p>"},{"location":"perfsonar/perfSONAR-sysctl-info/#queue-management-and-packet-processing","title":"Queue management and packet processing","text":"<p>These settings help prevent packet loss and improve network efficiency.</p> <ul> <li> <p><code>net.core.netdev_max_backlog</code>: Increases the maximum length of the input packet queue for the network device. A higher value can prevent dropped packets during bursts of traffic.</p> </li> <li> <p><code>net.core.default_qdisc</code>: Sets the default queuing discipline. Fair Queuing (fq) or Fair Queuing with CoDel (fq_codel) is recommended for its \"fairness\" in distributing bandwidth and kernel-level packet pacing.</p> </li> </ul>"},{"location":"perfsonar/perfSONAR-sysctl-info/#multipath-and-routing","title":"Multipath and routing","text":"<p>If you are running perfSONAR on a host with multiple network interfaces on the same subnet, specific <code>sysctl</code> settings are needed to prevent Address Resolution Protocol (ARP) conflicts.</p> <p>net.ipv4.conf.all.arp_ignore=1 net.ipv4.conf.all.arp_announce=2 net.ipv4.conf.default.arp_filter=1 net.ipv4.conf.all.arp_filter=1</p>"},{"location":"perfsonar/perfSONAR-sysctl-info/#tcp-optimizations","title":"TCP optimizations","text":"<p>For specific use cases, you might adjust other TCP settings:</p> <ul> <li><code>net.ipv4.tcp_timestamps</code>: While often suggested for improving CPU utilization, be aware of the implications. The <code>fasterdata</code> guide suggests disabling it for older kernels, but modern kernels handle timestamps more efficiently.</li> </ul>"},{"location":"perfsonar/perfSONAR-sysctl-info/#how-to-apply-the-settings","title":"How to apply the settings","text":"<ol> <li> <p>Edit the configuration file: Add the desired settings to <code>/etc/sysctl.conf</code> or a new file in <code>/etc/sysctl.d/</code> (e.g., <code>/etc/sysctl.d/99-perfsonar.conf</code>).</p> </li> <li> <p>Load the settings: Run <code>sysctl -p</code> to load the changes from the configuration file.</p> </li> <li> <p>Verify the settings: Use <code>sysctl [parameter]</code> to confirm the new values are in effect.</p> </li> </ol> <p>Automatic tuning with the perfSONAR Toolkit</p> <p>For installations using the perfSONAR Toolkit, the included <code>perfsonar-toolkit-sysctl</code> package handles most tuning automatically. If you change an interface speed, you can re-run the tuning script to apply appropriate settings:</p> <p>/usr/lib/perfsonar/scripts/configure_sysctl</p>"},{"location":"perfsonar/psetf/","title":"Infrastructure Monitoring","text":"<p>WLCG/OSG is operating more than 200 perfSONAR agents world-wide. A typical perfSONAR deployment has many services that need to function correctly for the the system to work.  As we scale-up to many perfSONAR deployments across many sites it can be difficult to verify everything is working correctly.</p> <p>perfSONAR monitoring instance [BROKEN-LINK: https://psetf.opensciencegrid.org/etf/check_mk/index.py?start_url=%2Fetf%2Fcheck_mk%2Fdashboard.py] actively monitors the state of the infrastructure for both remote perfSONAR installation as well as central services. The instance is based on ETF [BROKEN-LINK: http://etf.cern.ch/docs/latest/], which is an open source measurement middleware for functional/availability testing of the resources. In order to access the page you'll need to have x509 grid certificate loaded in the browser.</p> <p>A sample initial dashboard is shown below:</p> <p></p> <p>You can use quicksearch in the left pane to search for hostnames, domains or tests. The tests performed can be divided into four categories:</p> <ol> <li> <p>Configuration tests (<code>perfSONAR configuration:</code>) tests if the contact, organisation and meshes were set following our installation guide.</p> </li> <li> <p>Service tests (<code>perfSONAR services:</code>) check if different perfSONAR toolkit services are up and running correctly as well as if ports are reachable from OSG subnets.</p> </li> <li> <p>Hardware test (<code>perfSONAR hardware</code>) checks if the node conforms to the minimal hardware requirements (see Requirements for details)</p> </li> <li> <p>Freshness tests (<code>perfSONAR freshness</code>) is a high level test that checks what tests are available in the local measurement archive and compares this with the tests configured.</p> </li> </ol> <p>There can be many different reasons why certain tests are stale, such as nonfunctional remote perfSONAR nodes, network connectivity issues, or local measurement archive or scheduling problems. This test is informative and intentionally does not reach a critical state.</p> <p>A special kind of freshness tests are OSG datastore freshness tests, which account for what fraction of test results are stored centrally as compared to the local measurement archive. It mainly reflects the efficiency of the central OSG collector and does not provide information about the local services.</p> <p>This is sample snapshost showing all metrics for particular perfSONAR instance (latency node in this case): </p> <p>For any issues/questions concerning the monitoring pages and tests, please consult the FAQ</p> <p>Central services are also monitored with the same tool and their status can be seen by following Business Intelligence/All Aggregations on the left pane. It shows the aggregated status of both production and pre-production services including mesh configuration interface, central datastore and infrastructure monitoring.</p>"},{"location":"perfsonar/register-ps-in-gocdb/","title":"Register perfSONAR Service in GOCDB","text":""},{"location":"perfsonar/register-ps-in-gocdb/#this-section-describes-how-to-register-the-perfsonar-service-in-gocdb","title":"This section describes how to register the perfSONAR service in GOCDB","text":"<p>In order to register you perfSONAR services in GOCDB, you should access the proper section of GOC for adding a Service Endpoint</p> <ul> <li>https://goc.egi.eu/portal/index.php?Page_Type=New_Service_Endpoint</li> </ul> <p>You might not be able to access the page if you are not properly registered in GOC, so a snapshot can be found below. In filling the information please follow those simple guidelines:</p> <ul> <li> <p>There are two service types for perfSONAR: <code>net.perfSONAR.Bandwidth</code> and <code>net.perfSONAR.Latency</code>. This is because we suggest installing two perfSONAR boxes at the site (one for latency tests and one for bandwidth tests) and therefore two distinct service endpoints should be published with two distinct service types. If the site cannot afford sufficient hardware for this setup, you may install a single perfSONAR node, but you should still publish both service types (use the same hostname in the \"host name\" field).</p> </li> <li> <p>For each form (i.e. for each service type) fill at least the important informations:</p> </li> </ul> <pre><code>- Hosting Site (drop-down menu, mandatory)\n- Service Type (drop-down menu, mandatory)\n- Host Name (free text, mandatory)\n- Host IP (free text, optional)\n- Description: (free text, optional) This field has a default value of your site name. It is used to \"Label\" your host in our MaDDash GUI. If you want to use this field please use something as short as possible uniquely identifying this instance.\n- Check \"N\" when asked \"Is it a beta service\"\n- Check \"Y\" when asked \"Is this service in production\"\n- Check \"Y\" when asked \"Is this service monitored\"\n</code></pre> <ul> <li> <p>GOCDB screen shot for creating a Service Endpoint:</p> <p></p> </li> </ul> <p>-- Main.ShawnMcKee - 21 Oct 2014</p>"},{"location":"perfsonar/tools_scripts/","title":"perfSONAR Tools &amp; Scripts","text":"<p>This directory contains helper scripts for perfSONAR deployment, configuration, and tuning.</p>"},{"location":"perfsonar/tools_scripts/#available-tools","title":"Available tools","text":"Tool Version Purpose Documentation fasterdata-tuning.sh v1.3.1 Host &amp; NIC tuning (ESnet Fasterdata) Fasterdata Tuning Guide perfSONAR-pbr-nm.sh \u2014 Multi-NIC policy-based routing Multiple NIC Guidance perfSONAR-update-lsregistration.sh \u2014 LS registration management LS Registration Tools perfSONAR-auto-enroll-psconfig.sh \u2014 Automatic pSConfig enrollment Installation Guides install_tools_scripts.sh \u2014 Bulk installer for all scripts Installation install-systemd-service.sh \u2014 Container auto-start on boot Container Management <p>Latest Updates: v1.2.0 (Dec 2025) adds save/restore state management to fasterdata-tuning.sh</p>"},{"location":"perfsonar/tools_scripts/#quick-start","title":"Quick Start","text":""},{"location":"perfsonar/tools_scripts/#download-all-scripts","title":"Download All Scripts","text":"<p>Install all scripts to a target directory (default: <code>/opt/perfsonar-tp/tools_scripts</code>):</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh | sudo bash -s -- /opt/perfsonar-tp\n</code></pre>"},{"location":"perfsonar/tools_scripts/#download-individual-scripts","title":"Download Individual Scripts","text":"<pre><code># Example: fasterdata-tuning.sh\nsudo curl -fsSL -o /usr/local/bin/fasterdata-tuning.sh \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\nsudo chmod +x /usr/local/bin/fasterdata-tuning.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/#host-tuning","title":"Host Tuning","text":""},{"location":"perfsonar/tools_scripts/#fasterdata-tuning-script","title":"Fasterdata Tuning Script","text":"<p>Audit and apply ESnet Fasterdata-inspired host and NIC tuning for EL9 systems.</p> <p>NEW in v1.2.0: Save/restore state functionality for testing different tuning configurations.</p> <p>Quick Usage:</p> <pre><code># Audit mode (no changes)\n/usr/local/bin/fasterdata-tuning.sh --mode audit --target measurement\n\n# Apply tuning (requires root)\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target dtn\n\n# Save current state before testing\nsudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline\n\n# Restore saved state\nsudo /usr/local/bin/fasterdata-tuning.sh --restore-state baseline --yes\n\n# List all saved states\n/usr/local/bin/fasterdata-tuning.sh --list-states\n</code></pre> <p>State Management: Save and restore system configurations for before/after performance testing. Captures sysctl parameters, interface settings, configuration files, CPU governor, and tuned profile. See Fasterdata Tuning Guide for complete workflow examples.</p> <p>Full Documentation: Fasterdata Tuning Guide</p>"},{"location":"perfsonar/tools_scripts/#multi-nic-configuration","title":"Multi-NIC Configuration","text":""},{"location":"perfsonar/tools_scripts/#policy-based-routing-pbr","title":"Policy-Based Routing (PBR)","text":"<p>Configure static IPv4/IPv6 addressing and per-NIC source-based routing via NetworkManager.</p> <p>Script: <code>perfSONAR-pbr-nm.sh</code></p> <p>Config file: <code>/etc/perfSONAR-multi-nic-config.conf</code></p> <p>Log file: <code>/var/log/perfSONAR-multi-nic-config.log</code></p> <p>Quick Usage:</p> <pre><code># Generate config automatically\n/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-auto\n\n# Preview changes (dry-run)\n/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --dry-run --debug\n\n# Apply configuration\nsudo /opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --yes\n</code></pre> <p>Full Documentation: Multiple NIC Guidance</p>"},{"location":"perfsonar/tools_scripts/#ls-registration","title":"LS Registration","text":""},{"location":"perfsonar/tools_scripts/#lookup-service-registration-tools","title":"Lookup Service Registration Tools","text":"<p>Manage perfSONAR Lookup Service registration configuration.</p> <p>Script: <code>perfSONAR-update-lsregistration.sh</code></p> <p>Quick Usage:</p> <pre><code># For RPM Toolkit (local mode)\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh update --local \\\n  --site-name \"My Site\" --admin-email admin@example.org\n\n# For container-based testpoint\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh update \\\n  --container perfsonar-testpoint \\\n  --site-name \"My Site\" --admin-email admin@example.org\n</code></pre> <p>Full Documentation: LS Registration Tools</p>"},{"location":"perfsonar/tools_scripts/#auto-enrollment","title":"Auto-Enrollment","text":""},{"location":"perfsonar/tools_scripts/#psconfig-auto-enrollment","title":"pSConfig Auto-Enrollment","text":"<p>Automatically enroll perfSONAR instances in OSG/WLCG pSConfig meshes by deriving FQDNs from configured IPs.</p> <p>Script: <code>perfSONAR-auto-enroll-psconfig.sh</code></p> <p>Quick Usage:</p> <pre><code># For RPM Toolkit (local mode)\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-auto-enroll-psconfig.sh --local -v\n\n# For container-based testpoint\n/opt/perfsonar-tp/tools_scripts/perfSONAR-auto-enroll-psconfig.sh -v\n</code></pre> <p>Full Documentation: See Testpoint Installation or Toolkit Installation</p>"},{"location":"perfsonar/tools_scripts/#installation","title":"Installation","text":""},{"location":"perfsonar/tools_scripts/#bulk-script-installer","title":"Bulk Script Installer","text":"<p>Install all helper scripts to a target directory.</p> <p>Script: <code>install_tools_scripts.sh</code></p> <p>Usage:</p> <pre><code># Preview (dry-run)\nbash /opt/perfsonar-tp/tools_scripts/install_tools_scripts.sh --dry-run\n\n# Install to default location (/opt/perfsonar-tp/tools_scripts)\nbash /opt/perfsonar-tp/tools_scripts/install_tools_scripts.sh\n\n# Install to custom location\nbash /opt/perfsonar-tp/tools_scripts/install_tools_scripts.sh /custom/path\n</code></pre> <p>Options: - <code>--dry-run</code>: Preview changes without modifying files - <code>--skip-testpoint</code>: Skip cloning testpoint repo if already present</p>"},{"location":"perfsonar/tools_scripts/#container-management","title":"Container Management","text":""},{"location":"perfsonar/tools_scripts/#systemd-service-for-container-auto-start","title":"Systemd Service for Container Auto-Start","text":"<p>Install and enable a systemd service for automatic container restart on boot.</p> <p>Script: <code>install-systemd-service.sh</code></p> <p>Usage:</p> <pre><code># Install with default path (/opt/perfsonar-tp)\nsudo /opt/perfsonar-tp/tools_scripts/install-systemd-service.sh\n\n# Install with custom path\nsudo /opt/perfsonar-tp/tools_scripts/install-systemd-service.sh /custom/path\n</code></pre> <p>Service Management:</p> <pre><code># Start/stop/restart containers\nsystemctl start perfsonar-testpoint\nsystemctl stop perfsonar-testpoint\nsystemctl restart perfsonar-testpoint\n\n# View status and logs\nsystemctl status perfsonar-testpoint\njournalctl -u perfsonar-testpoint -f\n</code></pre> <p>Integration:</p> <p>After deploying containers with <code>podman-compose up -d</code>, install the service to ensure containers start automatically on boot.</p>"},{"location":"perfsonar/tools_scripts/#dependencies","title":"Dependencies","text":"<p>Essential packages (install before using these scripts):</p> <pre><code># RHEL/AlmaLinux/Rocky Linux\ndnf install -y bash coreutils iproute NetworkManager rsync curl openssl \\\n  nftables podman podman-compose fail2ban policycoreutils python3\n\n# Debian/Ubuntu\napt-get install -y bash coreutils iproute2 network-manager rsync curl openssl \\\n  nftables podman podman-compose docker.io fail2ban policycoreutils python3\n</code></pre>"},{"location":"perfsonar/tools_scripts/#support-additional-resources","title":"Support &amp; Additional Resources","text":"<ul> <li>Installation Guides: Quick Deploy Landing</li> <li>Troubleshooting: Troubleshooter Guide</li> <li>Network Issues: Network Troubleshooting Guide</li> <li>perfSONAR FAQ: FAQ</li> <li>Community Support: perfSONAR User Mailing List</li> </ul>"},{"location":"perfsonar/tools_scripts/#contact","title":"Contact","text":"<p>Script Author: Shawn McKee \u2014 smckee@umich.edu</p> <p>OSG Networking Team: networking-team@osg-htc.org</p>"},{"location":"perfsonar/tools_scripts/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"perfsonar/tools_scripts/CHANGELOG/#132-2025-12-16","title":"[1.3.2] - 2025-12-16","text":""},{"location":"perfsonar/tools_scripts/CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Critical JSON state save corruption fix: Fixed invalid JSON generation in <code>--save-state</code> that caused <code>--restore-state</code> to fail</li> <li>Properly quote non-numeric ring buffer values (e.g., <code>Mini:</code>, <code>push</code>, <code>n/a</code>)</li> <li>Properly quote non-numeric <code>nm_mtu</code> values (e.g., <code>auto</code>)</li> <li>Sanitize ring buffer values to ensure numeric-only or properly quoted strings</li> <li>Strip embedded newlines from qdisc strings that corrupted JSON structure</li> <li>Fixed packet pacing audit detection: <code>--mode audit</code> now correctly detects if packet pacing is already applied by checking actual qdisc state instead of relying on command-line flags</li> <li>Added <code>repair-state-json.sh</code> utility script to repair existing corrupted JSON state files</li> </ul>"},{"location":"perfsonar/tools_scripts/CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Enhanced JSON generation in <code>capture_interface_state()</code> with proper type validation</li> <li>Ring buffer values now validated as numeric before embedding in JSON</li> <li>Non-numeric values are quoted as strings</li> <li>Summary now checks system state for packet pacing instead of flag state</li> </ul>"},{"location":"perfsonar/tools_scripts/CHANGELOG/#notes","title":"Notes","text":"<ul> <li>Users with existing corrupted state files can repair them using <code>repair-state-json.sh &lt;file.json&gt;</code></li> <li>The script creates backups before repair and validates output with <code>jq</code> or Python</li> </ul>"},{"location":"perfsonar/tools_scripts/CHANGELOG/#unreleased-2025-12-16","title":"[Unreleased] - 2025-12-16","text":""},{"location":"perfsonar/tools_scripts/CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Installer: <code>install_tools_scripts.sh</code> bumped to <code>1.0.1</code> and updated to fetch a fuller set of helper scripts and docs, and attempt to fetch accompanying <code>.sha256</code> checksum files when available.</li> <li>lsregistration helper: <code>perfSONAR-update-lsregistration.sh</code> now supports non-container (RPM) installs more robustly by attempting to restart the <code>perfsonar-lsregistrationdaemon</code> service name where present and falling back to <code>lsregistrationdaemon</code>, and will prefer the <code>perfsonar-</code> prefixed unit when restarting.</li> <li>lsregistration helper: Clarified <code>save</code> vs <code>extract</code> usage (save writes a raw <code>.conf</code> file; extract produces an executable restore script), and the <code>extract</code> output now attempts to apply <code>restorecon</code> to fix SELinux labels when run on hosts. The updater will attempt a <code>restorecon</code> after writing configuration locally or into a container when <code>restorecon</code> is available.</li> </ul>"},{"location":"perfsonar/tools_scripts/CHANGELOG/#notes_1","title":"Notes","text":"<ul> <li>The <code>docs/perfsonar/tools_scripts</code> directory now includes updated <code>.sha256</code> checksum files for modified scripts. See the PR for details.</li> </ul>"},{"location":"perfsonar/tools_scripts/CHANGELOG/#113-2025-12-06","title":"[1.1.3] - 2025-12-06","text":""},{"location":"perfsonar/tools_scripts/CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>Corrected IOMMU audit messaging to suggest <code>grubby</code> for BLS systems (EL9+) and <code>grub2-mkconfig</code>/<code>update-grub</code> for legacy systems. Also bumped <code>fasterdata-tuning.sh</code> to v1.1.3 and updated site copy + checksums.</li> </ul>"},{"location":"perfsonar/tools_scripts/CHANGELOG/#notes_2","title":"Notes","text":"<ul> <li>This is a minor documentation &amp; diagnostic improvement; no new behavioral changes beyond clearer messaging and version bump.</li> </ul>"},{"location":"perfsonar/tools_scripts/DEPRECATION/","title":"DEPRECATED: perfSONAR-extract-lsregistration.sh","text":"<p>This helper script, <code>perfSONAR-extract-lsregistration.sh</code>, is deprecated and has been removed from active maintenance as of the quick-deploy docs release v1.0.1.</p>"},{"location":"perfsonar/tools_scripts/DEPRECATION/#why-deprecated","title":"Why deprecated","text":"<ul> <li>The functionality provided by this script is better covered by <code>perfSONAR-update-lsregistration.sh</code> and by using the explicit configuration and restore workflow described in the quick-deploy documentation and release notes.</li> </ul>"},{"location":"perfsonar/tools_scripts/DEPRECATION/#what-to-use-instead","title":"What to use instead","text":"<ul> <li> <p>For programmatic restores or to re-apply settings, use <code>perfSONAR-update-lsregistration.sh</code> directly. See its <code>--help</code> output for supported flags and examples.</p> </li> <li> <p>The Quick Deploy release notes describe the recommended workflows and the location of the updater script in the tools bundle: <code>docs/release-notes/quick-deploy-1.0.1.md</code> and <code>CHANGELOG.md</code> in the repo root.</p> </li> </ul> <p>If you still depend on the old extractor script, please migrate to the updater-based workflow or open an issue in the repository to discuss use cases that aren't covered.</p>"},{"location":"perfsonar/tools_scripts/DEPRECATION/#contact","title":"Contact","text":"<p>If you need assistance migrating, open an issue on the repository or contact the maintainers listed in the project's <code>CHANGELOG.md</code> and release notes.</p>"},{"location":"perfsonar/tools_scripts/README-lsregistration/","title":"perfSONAR lsregistration helpers","text":"<p>This directory includes a helper for managing the perfSONAR Lookup Service (LS) registration configuration in <code>lsregistrationdaemon.conf</code>.</p> <ul> <li><code>perfSONAR-update-lsregistration.sh</code> \u2014 a combined helper that can update, save, restore, create, and extract a <code>lsregistrationdaemon.conf</code>. Use the commands <code>update</code>, <code>save</code>, <code>restore</code>, <code>create</code>, and <code>extract</code> (see examples below).</li> </ul>"},{"location":"perfsonar/tools_scripts/README-lsregistration/#update-existing-configuration-container-or-local","title":"Update existing configuration (container or local)","text":"<p>Script: <code>perfSONAR-update-lsregistration.sh</code></p> <ul> <li> <p>Container mode (default): copies <code>/etc/perfsonar/lsregistrationdaemon.conf</code> into a temp area, applies requested changes, writes it back into the container, and restarts <code>lsregistrationdaemon</code> inside the container.</p> </li> <li> <p>Key flags: <code>--container NAME</code> (default: <code>perfsonar-testpoint</code>),       <code>--engine auto|docker|podman</code> (default: <code>auto</code>).</p> </li> <li> <p>Local mode: operates directly on the host filesystem without a container.</p> </li> <li> <p>Key flags: <code>--local</code>, <code>--conf PATH</code> (default: <code>/etc/perfsonar/lsregistrationdaemon.conf</code>).</p> </li> <li> <p>Attempts a best-effort restart of <code>lsregistrationdaemon</code> on the host.</p> </li> <li> <p>Restart behavior: the script now attempts to restart the <code>perfsonar-lsregistrationdaemon</code> unit first (common in RPM installs), falls back to <code>lsregistrationdaemon</code> if that unit is not present, and finally falls back to signalling the process via <code>pkill -HUP</code> when <code>systemctl</code> is not available.</p> </li> <li> <p>SELinux: when writing configuration to a host or into a container the updater will attempt to apply <code>restorecon</code> (if available) to the target path to ensure SELinux labels are usable after an automated restore. For manual restores that fail due to SELinux, run: <code>sudo /sbin/restorecon -v /etc/perfsonar/lsregistrationdaemon.conf</code>.</p> </li> <li> <p>Save vs Extract: <code>save --output FILE</code> writes the raw <code>lsregistrationdaemon.conf</code> to <code>FILE</code> (recommended suffix <code>.conf</code>). <code>extract --output SCRIPT</code> produces a self-contained, executable restore script that writes the conf into <code>/etc/perfsonar/lsregistrationdaemon.conf</code> and tries to apply <code>restorecon</code> when executed on a host (recommended suffix <code>.sh</code>).</p> </li> </ul> <p>Examples:</p> <pre><code># Update a few fields inside the container (from installed tools path)\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh \\\n  --container perfsonar-testpoint \\\n  --site-name \"Acme Co.\" --domain example.org \\\n  --project WLCG --project OSG \\\n  --admin-name \"pS Admin\" --admin-email admin@example.org\n\n# Update the host file directly (non-container use)\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh --local \\\n  --conf /etc/perfsonar/lsregistrationdaemon.conf \\\n  --city Berkeley --region CA --country US\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-lsregistration/#generate-a-restore-script-from-an-existing-conf","title":"Generate a restore script from an existing conf","text":"<p>Script: <code>perfSONAR-update-lsregistration.sh</code> (see above)</p> <p>The combined helper contains an <code>extract</code> command that produces a self-contained restore script. Note the distinction:</p> <ul> <li><code>save --output FILE</code> writes the raw <code>lsregistrationdaemon.conf</code> content to <code>FILE</code> (recommended suffix: <code>.conf</code>).</li> <li><code>extract --output FILE</code> produces an executable script that will write the conf to <code>/etc/perfsonar/lsregistrationdaemon.conf</code> and attempt to fix SELinux labels (recommended suffix: <code>.sh</code>).</li> </ul> <p>Examples:</p> <pre><code># Save the raw conf file\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh save --output /tmp/lsreg.conf\n\n# Produce a self-contained restore script suitable for host restore\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh extract --output /tmp/restore-lsreg.sh\n/tmp/restore-lsreg.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-lsregistration/#notes","title":"Notes","text":"<ul> <li> <p>Both scripts are Bash and require a modern Bash (4+). Use <code>shellcheck</code> for   linting if making changes.</p> </li> <li> <p>In container mode, the updater restarts services inside the container; in   local mode, it attempts to restart the host service.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/","title":"perfSONAR Tools &amp; Scripts","text":"<p>This directory contains helper scripts for perfSONAR deployment, configuration, and tuning.</p>"},{"location":"perfsonar/tools_scripts/README-old/#available-tools","title":"Available Tools","text":"Tool Purpose Documentation fasterdata-tuning.sh Host &amp; NIC tuning (ESnet Fasterdata) Fasterdata Tuning Guide perfSONAR-pbr-nm.sh Multi-NIC policy-based routing PBR Configuration perfSONAR-update-lsregistration.sh LS registration management LS Registration Tools perfSONAR-auto-enroll-psconfig.sh Automatic pSConfig enrollment Auto-Enrollment install_tools_scripts.sh Bulk installer for all scripts Installation"},{"location":"perfsonar/tools_scripts/README-old/#quick-start","title":"Quick Start","text":""},{"location":"perfsonar/tools_scripts/README-old/#download-all-scripts","title":"Download All Scripts","text":"<pre><code># Install all scripts to /opt/perfsonar-tp/tools_scripts\ncurl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh | sudo bash -s -- /opt/perfsonar-tp\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#download-individual-scripts","title":"Download Individual Scripts","text":"<pre><code># Example: fasterdata-tuning.sh\nsudo curl -fsSL -o /usr/local/bin/fasterdata-tuning.sh \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\nsudo chmod +x /usr/local/bin/fasterdata-tuning.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#host-tuning","title":"Host Tuning","text":""},{"location":"perfsonar/tools_scripts/README-old/#fasterdata-tuning-script","title":"Fasterdata Tuning Script","text":"<p>Audit and apply ESnet Fasterdata-inspired host and NIC tuning for EL9 systems.</p> <p>Quick Usage:</p> <pre><code># Audit mode (no changes)\nfasterdata-tuning.sh --mode audit --target measurement\n\n# Apply tuning (requires root)\nsudo fasterdata-tuning.sh --mode apply --target dtn\n</code></pre> <p>Full Documentation: Fasterdata Tuning Guide</p>"},{"location":"perfsonar/tools_scripts/README-old/#installation","title":"Installation","text":"<p>This legacy page groups older installer notes. For current guidance, see the Install helper and Systemd service installer sections below, or refer to the Quick Deploy docs.</p>"},{"location":"perfsonar/tools_scripts/README-old/#pbr-configuration","title":"PBR Configuration","text":""},{"location":"perfsonar/tools_scripts/README-old/#install-helper","title":"Install helper","text":"<p>A small helper is provided to populate <code>/opt/perfsonar-tp/tools_scripts</code> from this repository using a shallow sparse checkout. It copies only the <code>docs/perfsonar/tools_scripts</code> directory and preserves executable bits.</p> <ul> <li> <p>Script: <code>install_tools_scripts.sh</code> (path: <code>docs/perfsonar/tools_scripts/install_tools_scripts.sh</code>)</p> </li> <li> <p>Purpose: idempotent installer for <code>/opt/perfsonar-tp/tools_scripts</code></p> </li> <li> <p>Options: <code>--dry-run</code> (preview), <code>--skip-testpoint</code> (don't clone testpoint repo)</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#systemd-service-installer","title":"Systemd service installer","text":"<p>A helper script is provided to install and enable a systemd service for automatic container restart on boot. This ensures perfSONAR testpoint containers managed by podman-compose restart automatically after a host reboot.</p> <ul> <li> <p>Script: <code>install-systemd-service.sh</code></p> </li> <li> <p>Purpose: Creates and enables systemd service for perfsonar-testpoint containers</p> </li> <li> <p>Service file: <code>/etc/systemd/system/perfsonar-testpoint.service</code></p> </li> <li> <p>Must be run as root</p> </li> </ul> <p>Usage:</p> <pre><code># Install with default path (/opt/perfsonar-tp)\nsudo bash install-systemd-service.sh\n\n# Install with custom path\nsudo bash install-systemd-service.sh /custom/path/to/perfsonar-tp\n</code></pre> <p>After installation:</p> <ul> <li> <p>Containers will automatically start on boot</p> </li> <li> <p>Use <code>systemctl start|stop|restart|status perfsonar-testpoint</code> to manage</p> </li> <li> <p>View logs with <code>journalctl -u perfsonar-testpoint -f</code></p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#integration-tips","title":"Integration tips","text":"<ul> <li>Orchestrated installs: If you use <code>perfSONAR-orchestrator.sh</code>, you can run the systemd installer after the compose stack is up so containers start on boot. Example:</li> </ul> <pre><code>/opt/perfsonar-tp/tools_scripts/install-systemd-service.sh /opt/perfsonar-tp\nsystemctl enable --now perfsonar-testpoint.service\n</code></pre> <ul> <li> <p>Manual installs: After <code>podman-compose up -d</code>, install and enable the service as shown above.</p> </li> <li> <p>Updating compose files: Edit <code>/opt/perfsonar-tp/docker-compose.yml</code> and run <code>systemctl restart perfsonar-testpoint.service</code> to apply changes cleanly.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#usage-examples","title":"Usage examples","text":"<p>Preview what would happen (safe):</p> <pre><code>bash docs/perfsonar/tools_scripts/install_tools_scripts.sh --dry-run\n</code></pre> <p>Install into <code>/opt/perfsonar-tp/tools_scripts</code> (creates the directory if missing):</p> <pre><code>bash docs/perfsonar/tools_scripts/install_tools_scripts.sh\n</code></pre> <p>If you already have the perfSONAR testpoint repo checked out in <code>/opt/perfsonar-tp</code>, skip cloning with:</p> <pre><code>bash docs/perfsonar/tools_scripts/install_tools_scripts.sh --skip-testpoint\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#fasterdata-host-tuning-script","title":"Fasterdata host tuning script","text":"<ul> <li> <p>Script: <code>fasterdata-tuning.sh</code> \u2014 audit/apply host &amp; NIC tuning (ESnet Fasterdata-aligned) for EL9 systems</p> </li> <li> <p>Path: <code>docs/perfsonar/tools_scripts/fasterdata-tuning.sh</code></p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#download","title":"Download","text":"<p>You can download the raw script from the GitHub repo (master branch):</p> <pre><code>https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\n</code></pre> <p>Or once the site is built, from the site URL:</p> <pre><code>https://osg-htc.org/networking/perfsonar/tools_scripts/fasterdata-tuning.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#quick-install","title":"Quick install","text":"<pre><code># Download and install in /usr/local/bin\nsudo curl -L -o /usr/local/bin/fasterdata-tuning.sh https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\nsudo chmod +x /usr/local/bin/fasterdata-tuning.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#verify-checksum","title":"Verify checksum","text":"<p>You can verify the script integrity with the provided SHA256 file:</p> <pre><code>curl -L -o /tmp/fasterdata-tuning.sh.sha256 https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh.sha256\nsha256sum -c /tmp/fasterdata-tuning.sh.sha256 --status &amp;&amp; echo \"OK\" || echo \"Checksum mismatch\"\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#optional-flags-apply-mode-only","title":"Optional flags (apply mode only)","text":"<ul> <li> <p><code>--apply-iommu</code>: Update GRUB <code>GRUB_CMDLINE_LINUX</code> to add vendor + <code>iommu=pt</code> and regenerate grub (requires <code>--mode apply</code> and root).</p> </li> <li> <p><code>--apply-smt on|off</code>: Change SMT state at runtime; use <code>--persist-smt</code> to also persist via GRUB edits.</p> </li> <li> <p><code>--persist-smt</code>: Persist SMT change by adding/removing <code>nosmt</code> in the kernel cmdline.</p> </li> <li> <p><code>--yes</code>: Non-interactive confirmation for apply flags.</p> </li> <li> <p><code>--dry-run</code>: Preview the changes that would be made (GRUB edits and sysfs writes) without applying them. Use for validation and audits.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#usage-examples-fasterdata-tuning","title":"Usage examples (fasterdata tuning)","text":"<p>Audit (default) a measurement host:</p> <pre><code>bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode audit --target measurement\n</code></pre> <p>Apply tuning (requires root):</p> <pre><code>sudo bash docs/perfsonar/tools_scripts/fasterdata-tuning.sh --mode apply --target dtn\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#notes","title":"Notes","text":"<ul> <li> <p>Shows a <code>Host Info</code> summary and performs various checks (sysctl, ethtool, drivers, SMT, IOMMU). IOMMU changes require GRUB edits and reboot; SMT toggles can be done via <code>/sys/devices/system/cpu/smt/control</code>.</p> </li> <li> <p>In apply mode the script writes <code>/etc/sysctl.d/90-fasterdata.conf</code> and writes/enables a systemd <code>ethtool-persist.service</code> to persist NIC settings.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#requirements","title":"Requirements","text":"<ul> <li> <p>Must be run as root. The script now enforces running as root early in execution and will exit if run as a non-privileged user. Run it with sudo or from a root shell.</p> </li> <li> <p>NetworkManager (<code>nmcli</code>) is required. The script checks for the presence of <code>nmcli</code> and will abort if it is not installed. Install NetworkManager via your distribution's package manager before running.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#dependencies-and-package-install-hints","title":"Dependencies and package install hints","text":"<p>The scripts in this directory call a number of external commands. Install these packages (or their distro equivalents) before using the tools below.</p> <p>Essential packages</p> <ul> <li> <p>bash (Bash 4.3+)</p> </li> <li> <p>coreutils (cp/mv/rm/mkdir/chmod/chown)</p> </li> <li> <p>iproute2 (provides <code>ip</code> and <code>ip route</code>)</p> </li> <li> <p>NetworkManager (provides <code>nmcli</code>)</p> </li> <li> <p>rsync (recommended for safe backups; scripts fall back to <code>cp</code>)</p> </li> <li> <p>curl</p> </li> <li> <p>openssl</p> </li> </ul> <p>Optional / feature packages</p> <ul> <li> <p>nftables (provides <code>nft</code>) \u2014 required for <code>perfSONAR-install-nftables.sh</code></p> </li> <li> <p>fail2ban (provides <code>fail2ban-client</code>) \u2014 optional; only used if present</p> </li> <li> <p>SELinux user tools (provides <code>getenforce</code>, <code>setenforce</code>, <code>restorecon</code>) \u2014   used by SELinux-related operations</p> </li> <li> <p>A container engine: <code>podman</code> or <code>docker</code> \u2014 required for the lsregistration   updater/extractor when operating against the running testpoint container</p> </li> <li> <p>podman-compose or docker-compose \u2014 useful for running the testpoint   compose bundle locally</p> </li> </ul> <p>Note: the <code>check-deps.sh</code> helper accepts <code>podman-compose</code> as an alternative provider to <code>docker-compose</code> and will report the dependency as satisfied if either binary is present.</p> <p>Example install commands</p> <p>Note: package names vary slightly across distributions. Adapt as needed.</p> <p>Fedora / RHEL / CentOS (dnf):</p> <pre><code>dnf install -y bash coreutils iproute NetworkManager rsync curl openssl nftables podman podman-compose docker-compose fail2ban policycoreutils\n</code></pre> <p>Debian / Ubuntu (apt):</p> <pre><code>apt-get update\napt-get install -y bash coreutils iproute2 network-manager rsync curl openssl nftables podman podman-compose docker.io docker-compose fail2ban policycoreutils\n</code></pre> <p>If you intend to use the lsregistration container helpers, ensure either <code>podman</code> or <code>docker</code> is installed and that the service can list and access containers (e.g., <code>podman ps</code> or <code>docker ps</code> works as root).</p> <p>If <code>rsync</code> is not available the scripts will attempt a <code>cp -a</code> fallback, but installing <code>rsync</code> provides safer, more robust backups.</p>"},{"location":"perfsonar/tools_scripts/README-old/#safety-first","title":"Safety first","text":"<p>This script will REMOVE ALL existing NetworkManager connections when run. Always test in a VM or console-attached host and use <code>--dry-run</code> to preview changes. The script creates a timestamped backup of existing connections before modifying anything.</p>"},{"location":"perfsonar/tools_scripts/README-old/#compatibility-and-fallbacks","title":"Compatibility and fallbacks","text":"<ul> <li> <p>The script prefers to configure routing and policy rules via NetworkManager (<code>nmcli</code>). However, <code>nmcli</code> support for advanced <code>routes</code> entries and <code>routing-rules</code> varies across versions and distributions. If <code>nmcli</code> cannot apply a given route or routing-rule, the script will attempt a compatibility fallback using the <code>ip route</code> and <code>ip rule</code> commands directly.</p> </li> <li> <p>Because the script now requires root, it no longer invokes <code>sudo</code> internally (the caller should run it with root privileges). This makes behavior deterministic in automation and avoids interactive sudo prompts.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#how-to-run-dry-run-debug","title":"How to run (dry-run / debug)","text":"<p>Preview what the script would do without changing the system:</p> <pre><code>bash perfSONAR-pbr-nm.sh --dry-run --debug\n</code></pre> <p>Generate an example or auto-detected config (preview, dry-run only):</p> <pre><code>bash perfSONAR-pbr-nm.sh --generate-config-debug\n</code></pre> <p>Write the auto-detected config to /etc (does not apply changes):</p> <pre><code>bash perfSONAR-pbr-nm.sh --generate-config-auto\n</code></pre> <p>Run for real (be careful):</p> <pre><code>bash perfSONAR-pbr-nm.sh\n# or non-interactive\nbash perfSONAR-pbr-nm.sh --yes\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#gateway-requirement-inference-and-generator-warnings","title":"Gateway requirement, inference, and generator warnings","text":"<ul> <li> <p>Any NIC with an IPv4 address should have a corresponding IPv4 gateway; likewise for IPv6. If a NIC lacks a   gateway, the generator will attempt conservative inference (below). If a device has no IPv4 or IPv6 gateway   (e.g., a management-only NIC), the generator will intentionally skip that NIC when creating an   auto-generated config to avoid generating unusable NetworkManager profiles unless you explicitly set the   device as <code>DEFAULT_ROUTE_NIC</code>.</p> </li> <li> <p>Conservative gateway inference: if a NIC has an address/prefix but no gateway, the tool will try to reuse a gateway from another NIC on the SAME subnet.</p> </li> <li> <p>IPv4: subnets are checked in bash; one unambiguous match is required.</p> </li> <li> <p>IPv6: requires <code>python3</code> (<code>ipaddress</code> module) to verify the gateway is in the same prefix; link-local gateways (fe80::/10) are not reused; one unambiguous match is required.</p> </li> <li> <p>If multiple gateways match, no guess is made; a warning is logged and validation will require you to set it explicitly.</p> </li> <li> <p>This inference runs in two places:</p> </li> <li> <p>During auto-generation (<code>--generate-config-auto</code> or <code>--generate-config-debug</code>) so the written config can be immediately useful.</p> </li> <li> <p>During normal execution after loading the config but before validation, so missing gateways may be filled automatically.</p> </li> </ul> <p>Example: generated config with inferred gateways</p> <pre><code>NIC_NAMES=(\n  \"eth0\"\n  \"eth1\"\n)\n\nNIC_IPV4_ADDRS=(\n  \"192.0.2.10\"\n  \"192.0.2.20\"\n)\nNIC_IPV4_PREFIXES=(\n  \"/24\"\n  \"/24\"\n)\nNIC_IPV4_GWS=(\n  \"192.0.2.1\"  # guessed from eth0\n  \"192.0.2.1\"  # guessed (reused gateway)\n)\n\nNIC_IPV6_ADDRS=(\n  \"2001:db8::10\"\n  \"2001:db8::20\"\n)\nNIC_IPV6_PREFIXES=(\n  \"/64\"\n  \"/64\"\n)\nNIC_IPV6_GWS=(\n  \"2001:db8::1\"  # guessed from eth0\n  \"2001:db8::1\"  # guessed (reused gateway)\n)\n</code></pre> <p>When gateways are inferred, a NOTE section is added near the bottom of the generated file listing each guess. The script will also print a NOTICE to the console/log. Review and edit the guessed values if needed before applying changes.</p> <p>If gateways remain missing after inference, the generator writes a WARNING block listing the affected NICs and the script will refuse to proceed until you set the gateways.</p>"},{"location":"perfsonar/tools_scripts/README-old/#backups-and-safety","title":"Backups and safety","text":"<ul> <li>Before applying changes, the script creates a timestamped backup of existing NetworkManager connections. It prefers <code>rsync</code> when available and falls back to <code>cp -a</code>. If the backup fails, the script aborts without removing existing configurations.</li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#tests","title":"Tests","text":"<p>A small set of unit-style tests is provided under <code>tests/</code>. These are designed to exercise pure validation and sanitization helpers without modifying system configuration. They source the script (functions only) and run checks in a non-destructive way.</p> <p>Run the tests:</p> <pre><code>cd docs/perfsonar\n./tests/run_tests.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/README-old/#notes-script-details","title":"Notes (script details)","text":"<ul> <li> <p>The script requires Bash (uses <code>local -n</code> namerefs). Run tests on a system   with Bash 4.3+.</p> </li> <li> <p>For more extensive validation, run <code>shellcheck -x perfSONAR-pbr-nm.sh</code> and   address any issues reported.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/README-old/#contact","title":"Contact","text":""},{"location":"perfsonar/tools_scripts/README-old/#auto-enrollment","title":"Auto-Enrollment","text":"<p>This page previously linked to auto-enrollment details. For current instructions, see the Quick Deploy Install Guides:</p> <ul> <li>Testpoint Installation: ../../personas/quick-deploy/install-perfsonar-testpoint.md</li> <li>Toolkit Installation: ../../personas/quick-deploy/install-perfsonar-toolkit.md</li> </ul> <p>Shawn McKee (script author) \u2014 smckee@umich.edu</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/","title":"Fasterdata-Tuning.sh Save/Restore Design Document","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#executive-summary","title":"Executive Summary","text":"<p>This document outlines the design for adding <code>--save-state</code> and <code>--restore-state</code> functionality to the fasterdata-tuning.sh script to enable before/after performance testing with the ability to return to a known configuration state.</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#problem-statement","title":"Problem Statement","text":"<p>Users need to test performance before and after applying fasterdata tuning configurations. To conduct meaningful tests, they must be able to: 1. Save the current system state before applying changes 2. Apply tuning changes 3. Test performance 4. Restore the original state 5. Repeat with different tuning configurations</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#system-state-inventory","title":"System State Inventory","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#what-the-script-currently-modifies","title":"What the Script Currently Modifies","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#1-sysctl-parameters-persistent-runtime","title":"1. Sysctl Parameters (Persistent &amp; Runtime)","text":"<ul> <li>File: <code>/etc/sysctl.d/90-fasterdata.conf</code></li> <li>Runtime: Applied via <code>sysctl -w</code></li> <li>Parameters Modified:</li> <li><code>net.core.rmem_max</code></li> <li><code>net.core.wmem_max</code></li> <li><code>net.core.rmem_default</code></li> <li><code>net.core.wmem_default</code></li> <li><code>net.ipv4.tcp_rmem</code> (min, default, max)</li> <li><code>net.ipv4.tcp_wmem</code> (min, default, max)</li> <li><code>net.core.netdev_max_backlog</code></li> <li><code>net.ipv4.tcp_congestion_control</code></li> <li><code>net.ipv4.tcp_mtu_probing</code></li> <li><code>net.core.default_qdisc</code></li> </ul> <p>Restoration Concerns:  - \u2705 Trackable: Current values can be read via <code>sysctl -n &lt;key&gt;</code> - \u2705 Restorable: Can be written back via <code>sysctl -w</code> and file deletion/modification - \u26a0\ufe0f Complexity: Need to distinguish between system defaults and previously modified values</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#2-per-interface-settings-runtime-persistent-via-systemd","title":"2. Per-Interface Settings (Runtime &amp; Persistent via systemd)","text":"<ul> <li>File: <code>/etc/systemd/system/ethtool-persist.service</code></li> <li>Settings Modified Per Interface:</li> <li>txqueuelen: <code>ip link set dev &lt;iface&gt; txqueuelen &lt;value&gt;</code></li> <li>Ring buffers: <code>ethtool -G &lt;iface&gt; rx &lt;max&gt; tx &lt;max&gt;</code></li> <li>Offload features: <code>ethtool -K &lt;iface&gt; gro on/off gso on/off tso on/off lro on/off rx on/off tx on/off</code></li> <li>QDisc: <code>tc qdisc replace dev &lt;iface&gt; root fq</code> or <code>tc qdisc replace dev &lt;iface&gt; root tbf ...</code></li> <li>MTU (optional): <code>ip link set dev &lt;iface&gt; mtu &lt;value&gt;</code></li> </ul> <p>Restoration Concerns: - \u2705 Trackable: All current values readable via ethtool, ip, tc commands - \u2705 Restorable: Can be reapplied - \u26a0\ufe0f Complexity: Ring buffer changes may not be fully reversible if hardware doesn't support original values - \u26a0\ufe0f State: Need to capture current qdisc parameters, not just type</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#3-mtu-settings-persistent-via-networkmanager-or-ifcfg","title":"3. MTU Settings (Persistent via NetworkManager or ifcfg)","text":"<ul> <li>NetworkManager: <code>nmcli connection modify &lt;conn&gt; 802-3-ethernet.mtu &lt;value&gt;</code></li> <li>ifcfg files: <code>/etc/sysconfig/network-scripts/ifcfg-&lt;iface&gt;</code> (legacy)</li> </ul> <p>Restoration Concerns: - \u2705 Trackable: Current MTU readable via <code>ip link show &lt;iface&gt;</code> - \u2705 Restorable: Can be reapplied via nmcli or ip link - \u26a0\ufe0f Persistence Mechanism: Need to track which method was used (NM vs ifcfg)</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#4-grub-kernel-command-line-persistent-requires-reboot","title":"4. GRUB Kernel Command Line (Persistent, requires reboot)","text":"<ul> <li>File: <code>/etc/default/grub</code></li> <li>BLS Entries: <code>/boot/loader/entries/*.conf</code> (modified via grubby)</li> <li>Parameters Modified:</li> <li>IOMMU: <code>intel_iommu=on iommu=pt</code> or <code>amd_iommu=on iommu=pt</code></li> <li>SMT: <code>nosmt</code> (when disabling)</li> </ul> <p>Restoration Concerns: - \u2705 Trackable: Current cmdline in <code>/proc/cmdline</code>, GRUB config in <code>/etc/default/grub</code> - \u2705 Restorable: Can modify GRUB config and regenerate - \u26a0\ufe0f Reboot Required: Changes require reboot to take effect - \u26a0\ufe0f Safety: Backup required before modifying boot configuration - \u26a0\ufe0f Complexity: Need to handle both traditional GRUB and BLS systems</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#5-smt-simultaneous-multithreading-runtime-optional-persistent","title":"5. SMT (Simultaneous Multithreading) (Runtime &amp; Optional Persistent)","text":"<ul> <li>Runtime: <code>/sys/devices/system/cpu/smt/control</code></li> <li>Persistent: Via GRUB (see #4)</li> </ul> <p>Restoration Concerns: - \u2705 Trackable: Current state readable from <code>/sys/devices/system/cpu/smt/control</code> - \u2705 Restorable: Runtime changes immediate, persistent requires GRUB modification - \u26a0\ufe0f Two Layers: Runtime and boot-time configuration may differ</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#6-cpu-governor-runtime-only-not-persistent-by-script","title":"6. CPU Governor (Runtime only, not persistent by script)","text":"<ul> <li>Runtime: <code>/sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</code> or via <code>cpupower</code></li> </ul> <p>Restoration Concerns: - \u2705 Trackable: Current governor readable per CPU - \u2705 Restorable: Can be set via sysfs or cpupower - \u26a0\ufe0f Not Persistent: Script doesn't persist this, so restoration only needs runtime</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#7-tuned-profile-persistent","title":"7. Tuned Profile (Persistent)","text":"<ul> <li>Command: <code>tuned-adm profile &lt;profile&gt;</code></li> </ul> <p>Restoration Concerns: - \u2705 Trackable: Current profile via <code>tuned-adm active</code> - \u2705 Restorable: Can be set via <code>tuned-adm profile</code> - \u26a0\ufe0f Side Effects: Changing tuned profile may modify other sysctl/system settings not tracked by this script</p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#untracked-changes-and-side-effects","title":"Untracked Changes and Side Effects","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#potential-issues","title":"Potential Issues:","text":"<ol> <li>Tuned Profile Side Effects: The <code>network-throughput</code> tuned profile may modify sysctls beyond what we track</li> <li>NetworkManager Modifications: Changes to NM connections may trigger network restarts</li> <li>Kernel Module Parameters: If drivers are updated/reloaded, module parameters may change</li> <li>systemd Service State: The <code>ethtool-persist.service</code> is created and enabled</li> <li>File Permissions: Created files have specific permissions that should be tracked</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#design-state-management-system","title":"Design: State Management System","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#state-file-format","title":"State File Format","text":"<p>Use JSON for structured, readable, version-controlled state files.</p> <p>State File Location: <code>/var/lib/fasterdata-tuning/saved-states/&lt;timestamp&gt;-&lt;label&gt;.json</code></p> <p>State File Structure: <pre><code>{\n  \"metadata\": {\n    \"version\": \"1.0\",\n    \"timestamp\": \"2025-12-10T14:30:00Z\",\n    \"hostname\": \"perfsonar.example.org\",\n    \"kernel\": \"5.14.0-362.el9.x86_64\",\n    \"label\": \"baseline\",\n    \"created_by\": \"fasterdata-tuning.sh v1.2.0\"\n  },\n  \"sysctl\": {\n    \"net.core.rmem_max\": \"134217728\",\n    \"net.core.wmem_max\": \"134217728\",\n    \"net.ipv4.tcp_congestion_control\": \"cubic\",\n    ...\n  },\n  \"sysctl_file\": {\n    \"path\": \"/etc/sysctl.d/90-fasterdata.conf\",\n    \"exists\": true,\n    \"content\": \"# base64 encoded content or null if doesn't exist\",\n    \"backup_path\": \"/var/lib/fasterdata-tuning/backups/90-fasterdata.conf.20251210143000\"\n  },\n  \"interfaces\": {\n    \"ens1f0\": {\n      \"state\": \"UP\",\n      \"mtu\": 1500,\n      \"txqueuelen\": 1000,\n      \"speed\": 10000,\n      \"qdisc\": {\n        \"type\": \"mq\",\n        \"full_output\": \"qdisc mq 0: root...\"\n      },\n      \"ethtool\": {\n        \"features\": {\n          \"rx-checksumming\": \"on\",\n          \"tx-checksumming\": \"on\",\n          \"scatter-gather\": \"on\",\n          \"tcp-segmentation-offload\": \"on\",\n          \"generic-segmentation-offload\": \"on\",\n          \"generic-receive-offload\": \"on\",\n          \"large-receive-offload\": \"off\"\n        },\n        \"ring\": {\n          \"rx\": 4096,\n          \"rx_max\": 4096,\n          \"tx\": 4096,\n          \"tx_max\": 4096\n        }\n      },\n      \"nm_connection\": \"System ens1f0\",\n      \"nm_mtu\": 1500\n    }\n  },\n  \"ethtool_persist_service\": {\n    \"exists\": false,\n    \"enabled\": false,\n    \"content\": null,\n    \"backup_path\": null\n  },\n  \"grub\": {\n    \"cmdline_current\": \"BOOT_IMAGE=... root=... intel_iommu=on iommu=pt\",\n    \"grub_file\": \"/etc/default/grub\",\n    \"grub_cmdline_linux\": \"root=... quiet\",\n    \"uses_bls\": true,\n    \"backup_path\": \"/var/lib/fasterdata-tuning/backups/grub.20251210143000\"\n  },\n  \"cpu\": {\n    \"governor\": {\n      \"cpu0\": \"powersave\",\n      \"cpu1\": \"powersave\",\n      \"unique_governors\": [\"powersave\"]\n    },\n    \"smt\": {\n      \"control\": \"on\",\n      \"supported\": true\n    }\n  },\n  \"tuned\": {\n    \"active_profile\": \"virtual-guest\",\n    \"available\": true\n  },\n  \"warnings\": [\n    \"NIC ens1f0: Ring buffer settings may not be fully restorable if hardware limits change\",\n    \"GRUB modifications require reboot to take effect\"\n  ]\n}\n</code></pre></p>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#implementation-plan","title":"Implementation Plan","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#phase-1-state-capture-functions","title":"Phase 1: State Capture Functions","text":"<p>Create modular functions to capture each configuration component:</p> <pre><code>capture_sysctl_state() {\n  # Returns JSON object with all relevant sysctl values\n}\n\ncapture_interface_state() {\n  # Per-interface: MTU, txqueuelen, qdisc, ethtool settings, NM config\n}\n\ncapture_grub_state() {\n  # GRUB config, current cmdline, BLS detection\n}\n\ncapture_cpu_state() {\n  # Governor per CPU, SMT state\n}\n\ncapture_tuned_state() {\n  # Active tuned profile\n}\n\ncapture_files_state() {\n  # Track existence and content of: \n  #   - /etc/sysctl.d/90-fasterdata.conf\n  #   - /etc/systemd/system/ethtool-persist.service\n  #   - /etc/default/grub\n}\n</code></pre>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#phase-2-state-restoration-functions","title":"Phase 2: State Restoration Functions","text":"<p>Create restoration functions with validation:</p> <pre><code>restore_sysctl_state() {\n  # Restore sysctl values from state file\n  # Delete /etc/sysctl.d/90-fasterdata.conf if it didn't exist\n  # Restore original content if it did exist\n}\n\nrestore_interface_state() {\n  # Per-interface restoration with validation\n  # Check if interface still exists\n  # Validate values are within hardware limits\n}\n\nrestore_grub_state() {\n  # Restore GRUB configuration\n  # Warn about reboot requirement\n  # Create backup before modification\n}\n\nrestore_cpu_state() {\n  # Restore CPU governor and SMT\n}\n\nrestore_tuned_state() {\n  # Restore tuned profile\n}\n\nrestore_files_state() {\n  # Restore or remove configuration files\n  # Disable/remove systemd services if needed\n}\n</code></pre>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#phase-3-command-line-interface","title":"Phase 3: Command-Line Interface","text":"<pre><code># Save current state with label\n./fasterdata-tuning.sh --save-state [--label &lt;name&gt;]\n\n# List saved states\n./fasterdata-tuning.sh --list-states\n\n# Restore specific state\n./fasterdata-tuning.sh --restore-state &lt;state-file-or-label&gt;\n\n# Show diff between current and saved state\n./fasterdata-tuning.sh --diff-state &lt;state-file-or-label&gt;\n\n# Delete saved state\n./fasterdata-tuning.sh --delete-state &lt;state-file-or-label&gt;\n\n# Apply and auto-save state before\n./fasterdata-tuning.sh --mode apply --auto-save-before [--label pre-tuning]\n</code></pre>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#safety-mechanisms","title":"Safety Mechanisms","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#1-pre-flight-checks","title":"1. Pre-Flight Checks","text":"<ul> <li>Verify sufficient disk space in <code>/var/lib/fasterdata-tuning/</code></li> <li>Check write permissions</li> <li>Validate JSON state file format before restoration</li> </ul>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#2-atomic-operations","title":"2. Atomic Operations","text":"<ul> <li>Create backups before any modifications</li> <li>Use temporary files with atomic moves</li> <li>Transaction log for multi-step operations</li> </ul>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#3-validation","title":"3. Validation","text":"<ul> <li>Verify interface still exists before restoration</li> <li>Check hardware capabilities before applying settings</li> <li>Validate sysctl keys exist before writing</li> <li>Confirm kernel module support for features</li> </ul>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#4-rollback-on-failure","title":"4. Rollback on Failure","text":"<ul> <li>If restoration fails mid-way, log error and stop</li> <li>Provide manual recovery instructions</li> <li>Keep backup files until successful restoration</li> </ul>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#5-warnings-and-prompts","title":"5. Warnings and Prompts","text":"<ul> <li>Warn if restoring state from different kernel version</li> <li>Warn if restoring state from different hardware</li> <li>Prompt before GRUB modifications (unless --yes)</li> <li>Show diff before restoration (unless --yes)</li> </ul>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#file-organization","title":"File Organization","text":"<pre><code>/var/lib/fasterdata-tuning/\n\u251c\u2500\u2500 saved-states/\n\u2502   \u251c\u2500\u2500 20251210-143000-baseline.json\n\u2502   \u251c\u2500\u2500 20251210-150000-tuned-dtn.json\n\u2502   \u2514\u2500\u2500 20251210-153000-tuned-measurement.json\n\u251c\u2500\u2500 backups/\n\u2502   \u251c\u2500\u2500 90-fasterdata.conf.20251210143000\n\u2502   \u251c\u2500\u2500 grub.20251210143000\n\u2502   \u2514\u2500\u2500 ethtool-persist.service.20251210143000\n\u251c\u2500\u2500 logs/\n\u2502   \u251c\u2500\u2500 save-20251210-143000.log\n\u2502   \u2514\u2500\u2500 restore-20251210-150000.log\n\u2514\u2500\u2500 lock\n</code></pre>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#limitations-and-caveats","title":"Limitations and Caveats","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#cannot-be-restored-without-reboot","title":"Cannot Be Restored Without Reboot:","text":"<ol> <li>GRUB kernel command-line changes (IOMMU, nosmt)</li> <li>Kernel module parameter changes</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#hardware-dependent-settings","title":"Hardware-Dependent Settings:","text":"<ol> <li>Ring buffer sizes - limited by NIC hardware</li> <li>Ethtool offload features - depends on NIC capabilities</li> <li>Link speed - physical limitation</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#side-effects-not-tracked","title":"Side Effects Not Tracked:","text":"<ol> <li>Tuned profile may modify additional sysctls</li> <li>NetworkManager connection changes may trigger network interruptions</li> <li>Driver updates/reloads</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#race-conditions","title":"Race Conditions:","text":"<ol> <li>If network interfaces are added/removed between save and restore</li> <li>If NetworkManager is actively reconfiguring interfaces</li> <li>If other tools modify the same settings concurrently</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#testing-strategy","title":"Testing Strategy","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#unit-tests","title":"Unit Tests:","text":"<ol> <li>State capture functions return valid JSON</li> <li>State restoration functions handle missing interfaces</li> <li>Backup/restore of configuration files</li> <li>JSON schema validation</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#integration-tests","title":"Integration Tests:","text":"<ol> <li>Save state \u2192 Apply tuning \u2192 Restore state \u2192 Verify identical</li> <li>Save state \u2192 Modify manually \u2192 Restore \u2192 Verify restored</li> <li>Save state on system A \u2192 Attempt restore on system B \u2192 Proper warnings</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#validation-tests","title":"Validation Tests:","text":"<ol> <li>Verify sysctl values match after restoration</li> <li>Verify interface settings match after restoration</li> <li>Verify file contents match after restoration</li> <li>Verify systemd service state matches</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#documentation-requirements","title":"Documentation Requirements","text":"<ol> <li>User Guide: How to use save/restore for performance testing workflows</li> <li>Limitations: What cannot be restored, what requires reboot</li> <li>Troubleshooting: Common issues and manual recovery procedures</li> <li>State File Format: Document JSON schema for advanced users</li> <li>Migration Guide: How to use with existing installations</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#implementation-priorities","title":"Implementation Priorities","text":""},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#p0-must-have-mvp","title":"P0 - Must Have (MVP):","text":"<ol> <li>\u2705 Save/restore sysctl parameters</li> <li>\u2705 Save/restore per-interface runtime settings (txqueuelen, qdisc, ethtool)</li> <li>\u2705 Save/restore configuration files (90-fasterdata.conf, ethtool-persist.service)</li> <li>\u2705 Basic state file management (save, restore, list)</li> <li>\u2705 Validation and warnings</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#p1-should-have","title":"P1 - Should Have:","text":"<ol> <li>Save/restore GRUB configuration (with reboot warnings)</li> <li>Save/restore CPU governor and SMT</li> <li>Save/restore tuned profile</li> <li>Diff functionality</li> <li>Auto-save before apply</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#p2-nice-to-have","title":"P2 - Nice to Have:","text":"<ol> <li>State migration/compatibility between script versions</li> <li>Compressed state files</li> <li>Remote state storage</li> <li>State comparison reports (detailed HTML/markdown)</li> <li>Integration with perfSONAR testing frameworks</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#open-questions","title":"Open Questions","text":"<ol> <li>Should we track process-level state (e.g., running services, firewall rules)?</li> <li> <p>Decision: No, out of scope. Focus on network tuning only.</p> </li> <li> <p>How to handle partial restoration failures?</p> </li> <li> <p>Decision: Stop on first error, log what was restored, provide manual steps.</p> </li> <li> <p>Should state files be portable between hosts?</p> </li> <li> <p>Decision: No, but provide warnings if hostname/hardware differs.</p> </li> <li> <p>Should we integrate with git for state versioning?</p> </li> <li> <p>Decision: Phase 2 feature, not MVP.</p> </li> <li> <p>How to handle NetworkManager vs legacy ifcfg systems?</p> </li> <li>Decision: Detect which is in use, save that info in state, restore using same method.</li> </ol>"},{"location":"perfsonar/tools_scripts/SAVE_RESTORE_DESIGN/#success-criteria","title":"Success Criteria","text":"<ol> <li>User can save current state with one command</li> <li>User can restore saved state and verify all tracked settings match</li> <li>System behaves identically after save/restore cycle (for tracked settings)</li> <li>Clear warnings about settings requiring reboot</li> <li>Comprehensive error messages and recovery guidance</li> <li>Zero data loss: all backups preserved even if restoration fails</li> </ol>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/","title":"Fasterdata Host &amp; Network Tuning (EL9)","text":"<p>This page documents <code>fasterdata-tuning.sh</code>, a script that audits and optionally applies ESnet Fasterdata-inspired host and NIC tuning recommendations for Enterprise Linux 9.</p> <p>Script: <code>docs/perfsonar/tools_scripts/fasterdata-tuning.sh</code></p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#purpose","title":"Purpose","text":"<p>The <code>fasterdata-tuning.sh</code> script helps network administrators optimize Enterprise Linux 9 systems for high-throughput data transfers by:</p> <ul> <li>Auditing current system configuration against ESnet Fasterdata best practices</li> <li>Applying recommended tuning automatically with safe defaults</li> <li>Testing different configurations via save/restore state management (v1.2.0+)</li> <li>Persisting settings across reboots via systemd and sysctl.d</li> </ul> <p>Recommended for perfSONAR testpoints, Data Transfer Nodes (DTNs), and dedicated high-performance networking hosts.</p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#download-install","title":"Download &amp; Install","text":"<p>You can download the script directly from the website or GitHub raw URL and install it locally for repeated use:</p> <pre><code># Download via curl to a system location and make it executable\nsudo curl -L -o /usr/local/bin/fasterdata-tuning.sh https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\nsudo chmod +x /usr/local/bin/fasterdata-tuning.sh\n\n# Or download directly from the site (if published):\nsudo curl -L -o /usr/local/bin/fasterdata-tuning.sh https://osg-htc.org/networking/perfsonar/tools_scripts/fasterdata-tuning.sh\nsudo chmod +x /usr/local/bin/fasterdata-tuning.sh\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#verify-the-checksum-optional","title":"Verify the checksum (optional)","text":"<p>To verify script integrity, compare the downloaded file with the provided SHA256 checksum:</p> <pre><code># Download script and checksum\ncurl -L -o /tmp/fasterdata-tuning.sh \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh\ncurl -L -o /tmp/fasterdata-tuning.sh.sha256 \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/fasterdata-tuning.sh.sha256\n\n# Verify checksum\nsha256sum -c /tmp/fasterdata-tuning.sh.sha256\n</code></pre> <p>Expected output: <code>fasterdata-tuning.sh: OK</code></p> <p>Note: The checksum file is manually maintained and updated with each script release.</p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#why-use-this-script","title":"Why use this script?","text":"<p>This script packages ESnet Fasterdata best practices into an audit/apply helper that:</p> <ul> <li> <p>Provides a non-invasive audit mode to compare current host settings against Fasterdata recommendations tailored by NIC speed and host role (measurement vs DTN).</p> </li> <li> <p>Centralizes recommended sysctl tuning for high-throughput, long-distance transfers (buffer sizing, qdisc, congestion control), reducing guesswork and manual errors.</p> </li> <li> <p>Applies and persists sysctl settings in <code>/etc/sysctl.d/90-fasterdata.conf</code> and helps persist per-NIC settings (ethtool) via a <code>systemd</code> oneshot service; it also checks for problematic driver versions and provides vendor-specific guidance.</p> </li> <li> <p>For DTN nodes: Supports TCP pacing via <code>fq</code> by default, with an optional interface cap via <code>tbf</code> when policy or constraints require it.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#who-should-use-it","title":"Who should use it?","text":"<ul> <li> <p>perfSONAR testpoints, dedicated DTNs and other throughput-focused hosts on EL9 where you control the host configuration.</p> </li> <li> <p>NOT for multi-tenant or general-purpose interactive servers without prior review \u2014 these sysctl changes can affect other services.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#verification-basic-checks","title":"Verification &amp; Basic checks","text":"<p>After running the script (audit or apply), verify key settings:</p> <pre><code># Sysctl\nsysctl net.core.rmem_max net.core.wmem_max net.core.netdev_max_backlog net.core.default_qdisc\n# Tuned active profile\ntuned-adm active || echo \"tuned-adm not present\"\n# Per NIC checks\nethtool -k &lt;iface&gt; # offload features\nethtool -g &lt;iface&gt; # ring buffer sizes\ntc qdisc show dev &lt;iface&gt;\n# Verify IOMMU in kernel cmdline\ncat /proc/cmdline | grep -E \"iommu=pt|intel_iommu=on|amd_iommu=on\"\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#security-safety","title":"Security &amp; safety","text":"<ul> <li> <p>Always test in a staging environment first. Use <code>--mode audit</code> to review before applying.</p> </li> <li> <p>The <code>iommu</code> and <code>SMT</code> settings are environment-sensitive: IOMMU changes require GRUB kernel cmdline edits and a reboot. The script only suggests GRUB edits and does not automatically change the bootloader.</p> </li> <li> <p>If you require automated GRUB edits or SMT toggles, those should be opt-in with thorough confirmation prompts and recovery steps.</p> </li> </ul>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#usage","title":"Usage","text":"<p>Quick Usage:</p> <pre><code># Audit mode (no changes)\n/usr/local/bin/fasterdata-tuning.sh --mode audit --target measurement\n\n# Apply tuning (requires root)\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target dtn\n\n# Apply with specific interfaces\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target measurement --ifaces \"ens1f0np0,ens1f1np1\"\n\n# Apply with packet pacing (fq by default)\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target dtn --apply-packet-pacing\n\n# Apply with explicit interface cap using tbf\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target dtn --apply-packet-pacing --use-tbf-cap --tbf-cap-rate 5gbps\n\n# Audit packet pacing settings\n/usr/local/bin/fasterdata-tuning.sh --mode audit --target dtn\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#notes","title":"Notes","text":"<ul> <li>IOMMU: The script checks whether <code>iommu=pt</code> plus vendor-specific flags (<code>intel_iommu=on</code> or <code>amd_iommu=on</code>) are present. When you run with <code>--apply-iommu</code> and <code>--mode apply</code>, the script will optionally back up <code>/etc/default/grub</code>, append the appropriate IOMMU flags (or use values provided via <code>--iommu-args</code>) and regenerate GRUB (using <code>grubby</code> or <code>grub2-mkconfig</code> where available). Use <code>--dry-run</code> to preview the GRUB changes. You may also set <code>--iommu-args \"intel_iommu=on iommu=pt\"</code> to provide custom boot args.</li> <li>SMT: The script detects SMT status and suggests commands to toggle runtime SMT; persistence requires GRUB edits (kernel cmdline). It does not toggle SMT by default.</li> <li>Apply mode writes to <code>/etc/sysctl.d/90-fasterdata.conf</code> and creates <code>/etc/systemd/system/ethtool-persist.service</code> when necessary.</li> <li>Packet Pacing (DTN only): For Data Transfer Node targets, the script can apply token bucket filter (tbf) qdisc to pace outgoing traffic. This is recommended when a DTN node handles multiple simultaneous transfers where the effective transfer rate is limited by the minimum of: source read rate, network bandwidth, and destination write rate. See the <code>--apply-packet-pacing</code> and <code>--packet-pacing-rate</code> flags below. For detailed information on why packet pacing is important and how it works, see the separate Packet Pacing guide.</li> </ul> <p>Optional apply flags (use with <code>--mode apply</code>):</p> <ul> <li><code>--apply-packet-pacing</code>: Enable packet pacing for DTN interfaces. By default, uses <code>fq</code> (Linux TCP pacing) \u2014 preferred.</li> <li><code>--use-tbf-cap</code>: Use <code>tbf</code> to apply an interface rate cap instead of <code>fq</code>. If no <code>--tbf-cap-rate</code> is provided, a default of ~90% of link speed is used.</li> <li><code>--tbf-cap-rate RATE</code>: Explicit <code>tbf</code> cap (e.g., <code>2000mbps</code>). Implies <code>--use-tbf-cap</code>. Deprecated alias: <code>--packet-pacing-rate</code>.</li> <li><code>--apply-iommu</code>: Edit GRUB to add <code>iommu=pt</code> and vendor-specific flags (e.g., <code>intel_iommu=on iommu=pt</code>) to the kernel cmdline and regenerate GRUB. On EL9/BLS systems the script will use <code>grubby</code> to update kernel entries; otherwise it falls back to <code>grub2-mkconfig -o /boot/grub2/grub.cfg</code> or <code>update-grub</code> as available. Requires confirmation or <code>--yes</code> to skip the interactive prompt.</li> <li><code>--iommu-args ARGS</code>: Provide custom kernel cmdline arguments to apply for IOMMU (e.g., <code>intel_iommu=on iommu=pt</code>). When set, these args override vendor-appropriate defaults.</li> <li><code>--apply-smt on|off</code>: Toggle SMT state at runtime. Requires <code>--mode apply</code>. Example: <code>--apply-smt off</code>.</li> <li><code>--persist-smt</code>: If set along with <code>--apply-smt</code>, also persist the change via GRUB edits (<code>nosmt</code> applied/removed).</li> <li><code>--yes</code>: Skip interactive confirmations; use with caution.</li> <li><code>--dry-run</code>: Preview the exact GRUB, sysctl, tc, and sysfs commands that would be run without actually applying them. Useful for audits and CI checks.</li> </ul> <p>Examples:</p> <pre><code># Preview IOMMU changes (dry-run)\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --apply-iommu --dry-run\n\n# Apply with custom IOMMU args\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --apply-iommu --iommu-args \"intel_iommu=on iommu=pt\" --yes\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#state-management-save--restore-configurations","title":"State Management: Save &amp; Restore Configurations","text":"<p>NEW in v1.2.0: The script now supports saving and restoring system state for testing different tuning configurations.</p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#quick-testing-workflow","title":"Quick testing workflow","text":"<p>TL;DR: Save baseline \u2192 Apply tuning \u2192 Test \u2192 Restore \u2192 Compare</p> <pre><code># Save baseline state\nsudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline\n\n# Apply tuning\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target measurement --yes\n\n# Run your performance tests here (iperf3, perfSONAR tests, etc.)\n\n# Restore baseline\nsudo /usr/local/bin/fasterdata-tuning.sh --restore-state baseline --yes\n\n# Compare configurations\n/usr/local/bin/fasterdata-tuning.sh --diff-state baseline\n</code></pre> <p>For detailed step-by-step workflow with multiple tuning profiles, see Example Performance Testing Workflow below.</p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#why-use-saverestore","title":"Why use save/restore?","text":"<p>When testing performance with different tuning configurations, you need to:</p> <ol> <li>Save your baseline (pre-tuning) configuration</li> <li>Apply tuning changes</li> <li>Test performance</li> <li>Restore the baseline to test alternative configurations</li> <li>Compare results</li> </ol> <p>The save/restore functionality captures all settings that <code>--mode apply</code> modifies, including:</p> <ul> <li>Sysctl parameters (TCP buffers, congestion control, etc.)</li> <li>Per-interface settings (txqueuelen, MTU, ring buffers, offload features, qdisc)</li> <li>Configuration files (<code>/etc/sysctl.d/90-fasterdata.conf</code>, <code>/etc/systemd/system/ethtool-persist.service</code>)</li> <li>CPU governor settings</li> <li>Tuned profile</li> <li>SMT (Simultaneous Multithreading) state</li> </ul> <p>Note: GRUB/boot configuration (IOMMU, persistent SMT) is not saved/restored as it requires a reboot to take effect.</p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#save-current-state","title":"Save Current State","text":"<p>Save the current system configuration before making changes:</p> <pre><code># Save state with a descriptive label (requires root)\nsudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline\n\n# Save state with automatic timestamp\nsudo /usr/local/bin/fasterdata-tuning.sh --save-state\n</code></pre> <p>State files are stored in: <code>/var/lib/fasterdata-tuning/saved-states/</code></p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#auto-save-before-apply","title":"Auto-save Before Apply","text":"<p>Automatically save the current state before applying changes:</p> <pre><code># Apply tuning and auto-save the pre-apply state\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target measurement --auto-save-before --label pre-tuning\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#list-saved-states","title":"List Saved States","text":"<p>View all saved configuration states:</p> <pre><code>/usr/local/bin/fasterdata-tuning.sh --list-states\n</code></pre> <p>Example output:</p> <pre><code>Saved States:\n=============\n\nFile: 20251210-143000-baseline.json\n  Timestamp: 2025-12-10T14:30:00Z\n  Label: baseline\n  Hostname: perfsonar.example.org\n  Path: /var/lib/fasterdata-tuning/saved-states/20251210-143000-baseline.json\n\nFile: 20251210-150000-tuned-measurement.json\n  Timestamp: 2025-12-10T15:00:00Z\n  Label: tuned-measurement\n  Hostname: perfsonar.example.org\n  Path: /var/lib/fasterdata-tuning/saved-states/20251210-150000-tuned-measurement.json\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#compare-current-vs-saved-state","title":"Compare Current vs Saved State","text":"<p>Show differences between current configuration and a saved state:</p> <pre><code># Compare using label\n/usr/local/bin/fasterdata-tuning.sh --diff-state baseline\n\n# Compare using filename\n/usr/local/bin/fasterdata-tuning.sh --diff-state 20251210-143000-baseline.json\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#restore-state","title":"Restore State","text":"<p>Restore a previously saved configuration:</p> <pre><code># Restore using label (requires root)\nsudo /usr/local/bin/fasterdata-tuning.sh --restore-state baseline --yes\n\n# Restore using filename (with interactive confirmation)\nsudo /usr/local/bin/fasterdata-tuning.sh --restore-state 20251210-143000-baseline.json\n</code></pre> <p>The restore process:</p> <ol> <li>Validates the state file exists and is valid JSON</li> <li>Warns if restoring from a different hostname</li> <li>Prompts for confirmation (unless <code>--yes</code> specified)</li> <li>Restores all saved settings:</li> <li>Sysctl parameters (runtime)</li> <li>Configuration files</li> <li>Per-interface settings</li> <li>CPU governor</li> <li>Tuned profile</li> <li>Reports success/failure for each component</li> </ol>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#delete-saved-state","title":"Delete Saved State","text":"<p>Remove a saved state file:</p> <pre><code># Delete using label (requires root)\nsudo /usr/local/bin/fasterdata-tuning.sh --delete-state baseline --yes\n\n# Delete using filename (with interactive confirmation)\nsudo /usr/local/bin/fasterdata-tuning.sh --delete-state 20251210-143000-baseline.json\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#example-performance-testing-workflow","title":"Example Performance Testing Workflow","text":"<p>Complete workflow for testing before/after tuning:</p> <pre><code># 1. Save baseline configuration\nsudo /usr/local/bin/fasterdata-tuning.sh --save-state --label baseline\n\n# 2. Run baseline performance tests\n# ... run your perfSONAR tests, iperf3, etc ...\n\n# 3. Apply tuning with auto-save\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target measurement --auto-save-before --label pre-measurement-tuning\n\n# 4. Run tests with tuned configuration\n# ... run your perfSONAR tests, iperf3, etc ...\n\n# 5. Compare configurations\n/usr/local/bin/fasterdata-tuning.sh --diff-state baseline\n\n# 6. Try alternative tuning (e.g., DTN profile)\nsudo /usr/local/bin/fasterdata-tuning.sh --restore-state baseline --yes\nsudo /usr/local/bin/fasterdata-tuning.sh --mode apply --target dtn --auto-save-before --label pre-dtn-tuning\n\n# 7. Run tests with DTN tuning\n# ... run your perfSONAR tests, iperf3, etc ...\n\n# 8. Restore to baseline when done\nsudo /usr/local/bin/fasterdata-tuning.sh --restore-state baseline --yes\n\n# 9. Verify restoration\n/usr/local/bin/fasterdata-tuning.sh --mode audit\n</code></pre>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#state-management-caveats","title":"State management caveats","text":"<p>What is and is not saved/restored:</p> Component Saved/Restored Notes Sysctl parameters \u2705 Yes Runtime values (TCP buffers, congestion control, etc.) Configuration files \u2705 Yes <code>/etc/sysctl.d/90-fasterdata.conf</code>, systemd services Per-interface settings \u2705 Yes txqueuelen, MTU, ring buffers, offload features, qdisc CPU governor \u2705 Yes Runtime setting SMT state \u2705 Yes Runtime setting Tuned profile \u2705 Yes Active profile GRUB kernel cmdline \u274c No Requires reboot (IOMMU, persistent nosmt); not suitable for testing cycles Kernel module parameters \u274c No Out of scope Firewall rules \u274c No Not modified by this script Network interfaces \u274c No Interface creation/deletion not supported <p>Important limitations:</p> <ol> <li>Hardware-dependent: Ring buffer sizes are limited by NIC hardware; restoration may fail if hardware doesn't support saved values</li> <li>Hostname-specific: Restoring a state from a different hostname will trigger a warning but proceed</li> <li>NetworkManager: Connection modifications may cause brief network interruptions</li> <li>Side effects: Changing tuned profile may modify additional sysctls not tracked by this script</li> <li>Requires python3: State save/restore operations require python3 for JSON processing</li> </ol>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#state-file-format","title":"State file format","text":"<p>State files are stored as JSON in <code>/var/lib/fasterdata-tuning/saved-states/</code> with the following structure:</p> <ul> <li>Metadata: Timestamp, hostname, kernel version, label, creator version</li> <li>Sysctl values: All tracked network tuning parameters</li> <li>Configuration files: Base64-encoded content with backup paths</li> <li>Per-interface settings: MTU, txqueuelen, qdisc, ethtool features, ring buffers, NetworkManager connection info</li> <li>System settings: CPU governor, SMT state, tuned profile</li> <li>Warnings: List of restoration limitations</li> </ul> <p>Backup copies of modified files are stored in <code>/var/lib/fasterdata-tuning/backups/</code>.</p>"},{"location":"perfsonar/tools_scripts/fasterdata-tuning/#reference-and-source","title":"Reference and source","text":"<ul> <li>Source script: <code>docs/perfsonar/tools_scripts/fasterdata-tuning.sh</code></li> <li>Fasterdata docs: https://fasterdata.es.net/host-tuning/</li> <li>DTN tuning and packet pacing guidance: https://fasterdata.es.net/DTN/tuning/</li> </ul> <p>If you use this script as part of a host onboarding flow, ensure you test it in a VM or staging host before applying to production hosts.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/","title":"Installing a perfSONAR Testpoint for WLCG/OSG","text":"<p>This guide walks WLCG/OSG site administrators through end-to-end installation, configuration, and validation of a perfSONAR testpoint on Enterprise Linux 9 (EL9). It uses automated tooling from this repository to streamline the process while accommodating site-specific requirements.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#prerequisites-and-planning","title":"Prerequisites and Planning","text":"<p>Before you begin, it may be helpful to gather the following information:</p> <ul> <li> <p>Hardware details: hostname, BMC/iLO/iDRAC credentials (if used), interface names, available storage locations.</p> </li> <li> <p>Network data: IPv4/IPv6 assignments for each NIC, default gateway, internal/external VLAN   information.</p> </li> <li> <p>Operational contacts: site admin email, OSG facility/site name, latitude/longitude.</p> </li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#existing-perfsonar-configuration","title":"Existing perfSONAR configuration","text":"<p>If replacing an existing instance, you may want to back up <code>/etc/perfsonar/</code> files, especially <code>lsregistrationdaemon.conf</code>, and any container volumes. We have a script named<code>perfSONAR-update-lsregistration.sh</code> to extract/save/restore registration config that you may want to use.</p> Quick capture of existing lsregistration config (if you have a src) <p>Download a temp copy:  <pre><code>curl -fsSL \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-update-lsregistration.sh \\\n  -o /tmp/update-lsreg.sh\nchmod 0755 /tmp/update-lsreg.sh\n</code></pre> Use the downloaded tool to extract a restore script: <pre><code>/tmp/update-lsreg.sh extract --output /root/restore-lsreg.sh\n</code></pre> Note: Repository clone instructions are in Step 2. Note: All shell commands assume an interactive root shell.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-1-install-and-harden-el9","title":"Step 1 \u2013 Install and Harden EL9","text":"<ol> <li> <p>Provision EL9: Install AlmaLinux, Rocky Linux, or RHEL 9 with the Minimal profile.</p> </li> <li> <p>Set the hostname and time sync: Pick the NIC that will own the default route for the hostname.</p> <pre><code>hostnamectl set-hostname &lt;testpoint-hostname&gt;\nsystemctl enable --now chronyd\ntimedatectl set-timezone &lt;Region/City&gt;\n</code></pre> </li> <li> <p>Disable unused services:</p> <pre><code>systemctl disable --now firewalld NetworkManager-wait-online\ndnf remove -y rsyslog\n</code></pre> Why disable unused services? <p>We recommend disabling unused services during initial provisioning to reduce complexity and avoid unexpected interference with network and container setup. Services such as <code>firewalld</code>, <code>NetworkManager-wait-online</code>, and <code>rsyslog</code> can alter networking state, hold boot or network events, or conflict with the automated nftables/NetworkManager changes performed by the helper scripts. Disabling non-essential services makes the install deterministic, reduces the host attack surface, and avoids delays or race conditions while configuring policy-based routing, nftables rules, and       container networking.</p> </li> <li> <p>Update the system:</p> <p><code>bash dnf -y update</code></p> </li> <li> <p>Record NIC names: Document interface mappings for later PBR configuration.</p> <pre><code>nmcli device status\nip -br addr\n</code></pre> </li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-2-choose-your-deployment-path","title":"Step 2 \u2013 Choose Your Deployment Path","text":"<p>After completing Step 1 (minimal OS hardening), you can proceed in one of two ways:</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#path-a-orchestrated-guided-install-recommended-for-new-deployments","title":"Path A: Orchestrated Guided Install (Recommended for New Deployments)","text":"<p>The orchestrator automates package installation, bootstrap, PBR configuration, security hardening, container deployment, certificate issuance, and pSConfig enrollment with interactive pauses (or non-interactive batch mode).</p> <p>Download and run the orchestrator:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-orchestrator.sh \\\n    -o /tmp/perfSONAR-orchestrator.sh\nchmod 0755 /tmp/perfSONAR-orchestrator.sh\n</code></pre> <p>Interactive mode (pause at each step, confirm/skip/quit):</p> <pre><code>/tmp/perfSONAR-orchestrator.sh\n</code></pre> <p>Non-interactive mode (auto-confirm all steps):</p> <pre><code>/tmp/perfSONAR-orchestrator.sh --non-interactive --option A\n</code></pre> <p>With Let's Encrypt (Option B):</p> <pre><code>/tmp/perfSONAR-orchestrator.sh --option B --fqdn &lt;FQDN&gt; --email &lt;EMAIL&gt;\n</code></pre> <p>Flags:</p> <ul> <li><code>--option {A|B}</code> \u2014 A = testpoint only; B = testpoint + Let's Encrypt</li> <li><code>--fqdn NAME</code> \u2014 primary FQDN for certificates (Option B)</li> <li><code>--email ADDRESS</code> \u2014 email for Let's Encrypt (Option B)</li> <li><code>--non-interactive</code> \u2014 skip pauses, auto-confirm</li> <li><code>--yes</code> \u2014 auto-confirm internal script prompts</li> <li><code>--dry-run</code> \u2014 preview steps without executing</li> <li><code>--auto-update</code> \u2014 install and enable a systemd timer that pulls container images daily and restarts containers only if updated (creates <code>/usr/local/bin/perfsonar-auto-update.sh</code>, a systemd service and timer)</li> </ul> <p>If you choose this path, skip to Step 7 (the orchestrator completes Steps 2\u20136 for you).</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#path-b-manual-step-by-step","title":"Path B: Manual Step-by-Step","text":"<p>For users who prefer granular control or need to customize each stage, continue with manual package installation, bootstrap, and configuration.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-21-install-base-packages","title":"Step 2.1 \u2013 Install Base Packages","text":"<p>On minimal hosts several required tools (e.g. <code>dig</code>, <code>nft</code>, <code>podman-compose</code>) are missing. Install all recommended prerequisites in one command:</p> <pre><code>dnf -y install podman podman-docker podman-compose \\\n    jq curl tar gzip rsync bind-utils \\\n    nftables fail2ban policycoreutils-python-utils \\\n    python3 iproute iputils procps-ng sed grep gawk\n</code></pre> <p>This ensures all subsequent steps (PBR generation, DNS checks, firewall hardening, container deployment) have their dependencies available.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-22-bootstrap-helper-scripts","title":"Step 2.2 \u2013 Bootstrap Helper Scripts","text":"<p>Use the bootstrap script to install helper scripts under <code>/opt/perfsonar-tp/tools_scripts</code>:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh \\\n    -o /tmp/install_tools_scripts.sh\n\nchmod 0755 /tmp/install_tools_scripts.sh\n\n/tmp/install_tools_scripts.sh /opt/perfsonar-tp\n</code></pre> <p>Verify bootstrap completed successfully:</p> <pre><code># Check that all helper scripts were downloaded\nls -1 /opt/perfsonar-tp/tools_scripts/*.sh | wc -l\n# Should show 11 shell scripts\n\n# Verify key scripts are present and executable\nls -l /opt/perfsonar-tp/tools_scripts/{perfSONAR-pbr-nm.sh,perfSONAR-install-nftables.sh,perfSONAR-orchestrator.sh}\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-3-configure-policy-based-routing-pbr","title":"Step 3 \u2013 Configure Policy-Based Routing (PBR)","text":"<p>Skip this step if you used the orchestrator (Path A)</p> <p>The orchestrator automates PBR configuration. If you ran it in Step 2, skip to Step 4.</p> <p>The script <code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh</code> automates NetworkManager profiles and routing rule setup. It fills out and consumes the network configuration in <code>/etc/perfSONAR-multi-nic-config.conf</code>.</p> Modes of operation <p>By default the script now performs an in-place apply that adjusts routes, rules, and NetworkManager connection properties without deleting existing connections or flushing all system routes. This minimizes disruption and usually avoids the need for a reboot.</p> <p>An optional destructive mode <code>--rebuild-all</code> performs the original full workflow: backup existing profiles, flush all routes and rules, remove every NetworkManager connection, then recreate connections from scratch. Use this only for initial deployments or when you must completely reset inconsistent legacy state.</p> Mode Flag Disruption When to use In-place (default) (none) or <code>--apply-inplace</code> Low (interfaces stay up; rules adjusted) Routine updates, gateway changes, add routes Full rebuild <code>--rebuild-all</code> High (connections removed; brief connectivity drop) First-time setup, severe misconfiguration"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#safety-enhancements","title":"Safety Enhancements","text":"<ul> <li>Detects active SSH session interface and avoids extra disruption to that NIC in in-place mode.</li> <li>Prompts are still skipped with <code>--yes</code>.</li> <li>Dry-run preview supported via <code>--dry-run</code> (combine with <code>--debug</code> for verbose output).</li> <li>Reboot is no longer generally required; only consider one if NetworkManager fails to apply the new rules cleanly.</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#generate-config-file-automatically-or-preview","title":"Generate config file automatically (or preview)","text":"<p>Gateways required for addresses</p> <p>Any NIC with an IPv4 address must also have an IPv4 gateway, and any NIC with an IPv6 address must have an IPv6 gateway. If the generator cannot detect a gateway, it adds a WARNING block to the generated file listing affected NICs. Edit <code>NIC_IPV4_GWS</code>/<code>NIC_IPV6_GWS</code> accordingly before applying changes.</p> Gateway prompts <p>During generation, the script attempts to detect gateways per-NIC. If a NIC has an IP address but no gateway could be determined, it will prompt you interactively to enter an IPv4 and/or IPv6 gateway (or <code>-</code> to skip). Prompts are skipped in non-interactive sessions or when you use <code>--yes</code>. Note, NICs without gateways are assumed to NOT be used for perfSONAR.</p> <p>Preview generation (no changes):</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-debug    \n</code></pre> <p>Generate and write the config file:</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-auto \n</code></pre> <p>The script writes the config file to <code>/etc/perfSONAR-multi-nic-config.conf</code>. Edit to adjust site-specific values (e.g., confirm <code>DEFAULT_ROUTE_NIC</code>, add <code>NIC_IPV4_ADDROUTE</code> entries) and verify the entries.  Next step is to apply the network changes...</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#apply-changes-in-place-default","title":"Apply changes (in-place default)","text":"<p>Connect via console for network changes</p> <p>When applying network changes across an ssh connection, your session may be interrupted.   Please try to run the perfSONAR-pbr-nm.sh script when connected either directly to the console or by using 'nohup' in front of the script invocation.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#in-place-apply-recommended","title":"In-place apply (recommended)","text":"<pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --yes\n</code></pre> If SSH connection drops during network reconfiguration: <ol> <li>Access via BMC/iLO/iDRAC console or physical console</li> <li>Review <code>/var/log/perfSONAR-multi-nic-config.log</code> for errors</li> <li>Check network state with <code>nmcli connection show</code> and <code>ip addr</code></li> <li>Restore from backup if needed: backups are in <code>/var/backups/nm-connections-&lt;timestamp&gt;/</code></li> <li>Reapply config after corrections: <code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --yes</code></li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#full-rebuild-destructive-removes-all-nm-connections-first","title":"Full rebuild (destructive \u2013 removes all NM connections first)","text":"<pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --rebuild-all --yes\n</code></pre> <p>The policy based routing script logs to <code>/var/log/perfSONAR-multi-nic-config.log</code>. After an in-place apply, a reboot is typically unnecessary. If connectivity or rules appear inconsistent (<code>ip rule show</code> / <code>ip route</code> mismatch), consider a manual NetworkManager restart:</p> <pre><code>systemctl restart NetworkManager\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#dns-forward-and-reverse-entries-required","title":"DNS: forward and reverse entries (required)","text":"<p>All IP addresses that will be used for perfSONAR testing MUST have DNS entries: a forward (A/AAAA) record and a matching reverse (PTR) record. This is required so remote test tools and site operators can reliably reach and identify your host, and because some measurement infrastructure and registration systems perform forward/reverse consistency checks.</p> <ul> <li>For single-stack IPv4-only hosts: ensure A and PTR are present and consistent.</li> <li>For single-stack IPv6-only hosts: ensure AAAA and PTR are present and consistent.</li> <li>For dual-stack hosts: both IPv4 and IPv6 addresses used for testing must have matching forward and reverse records (A+PTR and AAAA+PTR).</li> </ul> Run the DNS checker <p>To validate forward/reverse DNS for addresses in <code>/etc/perfSONAR-multi-nic-config.conf</code> you can run a script: <pre><code>/opt/perfsonar-tp/tools_scripts/check-perfsonar-dns.sh\n</code></pre> Notes and automation tips:</p> <ul> <li>The script above uses <code>dig</code> (bind-utils package) which is commonly available; you can adapt it   to use <code>host</code> if preferred.</li> <li>Run the check as part of your provisioning CI or as a pre-flight check before enabling measurement registration.</li> <li>For large sites or many addresses, parallelize the checks (xargs -P) or use a small Python   script that leverages <code>dns.resolver</code> for async checks.</li> <li>If your PTR returns a hostname with a trailing dot, the script strips it before the forward check.</li> </ul> <p>If any addresses fail these checks, correct the DNS zone (forward and/or reverse) and allow DNS propagation before proceeding with registration and testing.</p> <p>Verify the routing policy:</p> <pre><code>nmcli connection show\nip rule show\nip route show table &lt;table-id&gt;\n</code></pre> <p>Confirm that non-default interfaces have their own routing tables and that the default interface owns the system default route.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-4-configure-nftables-selinux-and-fail2ban","title":"Step 4 \u2013 Configure nftables, SELinux, and Fail2Ban","text":"<p>Skip this step if you used the orchestrator (Path A)</p> <p>The orchestrator automates security hardening. If you ran it in Step 2, skip to Step 5.</p> <p>Use <code>/opt/perfsonar-tp/tools_scripts/perfSONAR-install-nftables.sh</code> to configure a hardened nftables profile with optional SELinux and Fail2Ban support. No staging or copy step is required.</p> <p>Prerequisites (not installed by the script and should have been installed when check-deps.sh was run above):</p> <ul> <li> <p><code>nftables</code> must already be installed and available (<code>nft</code> binary) for firewall configuration.</p> </li> <li> <p><code>fail2ban</code> must be installed if you want the optional jail configuration.</p> </li> <li> <p>SELinux tools (e.g., <code>getenforce</code>, <code>policycoreutils</code>) must be present to attempt SELinux configuration.</p> </li> </ul> <p>If any prerequisite is missing, the script skips that component and continues.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#installconfigure-the-desired-options","title":"Install/configure the desired options","text":"<p>You can use the install script to install the options you want (selinux, fail2ban) <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-install-nftables.sh --selinux --fail2ban --yes\n</code></pre></p> <pre><code>- Use `--yes` to skip the interactive confirmation prompt (omit it if you prefer to review the\n  summary and answer manually).\n\n- Add `--dry-run` for a rehearsal that only prints the planned actions.\n</code></pre> <p>The script writes nftables rules for perfSONAR services, derives SSH allow-lists from <code>/etc/perfSONAR-multi-nic- config.conf</code>, optionally adjusts SELinux, and enables Fail2ban jails\u2014only if those components are already installed.</p> SSH allow-lists and validation <ul> <li>Derives SSH allow-lists from <code>/etc/perfSONAR-multi-nic-config.conf</code> (CIDR prefixes and addresses).</li> <li>Validates nftables rules before writing.</li> <li>Outputs: rules to <code>/etc/nftables.d/perfsonar.nft</code>, log to <code>/var/log/perfSONAR-install-nftables.log</code>, backups to <code>/var/backups/</code>.</li> </ul> Preview nftables rules before applying <p>You can preview the fully rendered nftables rules (no changes are made):</p> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-install-nftables.sh --print-rules\n</code></pre> Manually add extra management hosts/subnets <p>If you need to allow additional SSH sources not represented by your NIC-derived prefixes, edit <code>/etc/nftables.d/perfsonar.nft</code> and add entries to the appropriate sets. Example:</p> <pre><code>set ssh_access_ip4_subnets {\n    type ipv4_addr\n    flags interval\n    elements = { 192.0.2.0/24, 198.51.100.0/25 }\n}\n\nset ssh_access_ip4_hosts {\n    type ipv4_addr\n    elements = { 203.0.113.10, 203.0.113.11 }\n}\n\nset ssh_access_ip6_subnets {\n    type ipv6_addr\n    flags interval\n    elements = { 2001:db8:1::/64 }\n}\n\nset ssh_access_ip6_hosts {\n    type ipv6_addr\n    elements = { 2001:db8::10 }\n}\n</code></pre> <p>Then validate and reload (root shell):</p> <pre><code>nft -c -f /etc/nftables.d/perfsonar.nft\nsystemctl reload nftables || systemctl restart nftables\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#confirm-nftables-state-and-security-services","title":"Confirm nftables state and security services","text":"Verification commands <pre><code>nft list ruleset\nsestatus\nsystemctl status fail2ban\n</code></pre> <p>You may want to document any site-specific exceptions (e.g., additional allowed management hosts) in your change log.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-5-deploy-the-containerized-perfsonar-testpoint","title":"Step 5 \u2013 Deploy the Containerized perfSONAR Testpoint","text":"<p>Skip this step if you used the orchestrator (Path A)</p> <p>The orchestrator automates container deployment and certificate issuance. If you ran it in Step 2, skip to Step 6.</p> <p>Run the official testpoint image using Podman (or Docker). Choose one of the two deployment modes:</p> <ul> <li> <p>Option A: Testpoint only (simplest) \u2014 only bind-mount <code>/opt/perfsonar-tp/psconfig</code> for pSConfig.</p> </li> <li> <p>Option B: Testpoint + Let\u2019s Encrypt \u2014 two containers that share Apache files and certs via host bind mounts.</p> </li> </ul> <p>Use <code>podman-compose</code> (or <code>docker-compose</code>) in the examples below.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#option-a-testpoint-only-simplest","title":"Option A \u2014 Testpoint only (simplest)","text":"<p>Prepare the pSConfig directory and a minimal compose file. No other host bind-mounts are required.</p> <pre><code>mkdir -p /opt/perfsonar-tp/psconfig\n</code></pre> <p>Download a ready-made compose file (or copy it manually): Browse: repo view</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/docker-compose.testpoint.yml \\\n    -o /opt/perfsonar-tp/docker-compose.yml\n</code></pre> <p>Edit the <code>docker-compose.yml</code> as desired.</p> <p>Bring it up:</p> <pre><code>(cd /opt/perfsonar-tp; podman-compose up -d)\n</code></pre> <p>Verify the container is running and healthy:</p> <pre><code>podman ps\n</code></pre> <p>The container should show <code>healthy</code> status. The healthcheck monitors Apache HTTPS availability.</p> <p>Manage pSConfig files under <code>/opt/perfsonar-tp/psconfig</code> on the host; they are consumed by the container at <code>/etc/perfsonar/psconfig</code>. </p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#ensure-containers-restart-automatically-on-reboot-systemd-unit-for-testpoint-required","title":"Ensure containers restart automatically on reboot (systemd unit for testpoint - REQUIRED)","text":"<p>podman-compose limitation with systemd containers</p> <p>The perfSONAR testpoint image runs systemd internally and requires the <code>--systemd=always</code> flag to function correctly. podman-compose does not support this flag, which causes containers to crash-loop after reboot with exit code 255.</p> <p>You must use the systemd unit approach below instead of relying on compose alone.</p> <p>Install the provided systemd units to manage containers with proper systemd support:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-units.sh \\\n    -o /tmp/install-systemd-units.sh\nchmod 0755 /tmp/install-systemd-units.sh\n\n# Install testpoint-only systemd unit\n/tmp/install-systemd-units.sh --install-dir /opt/perfsonar-tp\n\n# Enable and start now\nsystemctl enable --now perfsonar-testpoint.service\n\n# Verify service and containers\nsystemctl status perfsonar-testpoint.service --no-pager\npodman ps\n</code></pre> Notes on podman/systemd <ul> <li>The service uses <code>podman run --systemd=always</code> to enable proper systemd operation inside the container</li> <li>The compose file is kept for reference but not used by the systemd units</li> <li>If you need to update container configuration, edit the systemd unit file directly: <code>/etc/systemd/system/perfsonar-testpoint.service</code></li> <li>After editing the unit file, reload and restart: <code>systemctl daemon-reload &amp;&amp; systemctl restart perfsonar-testpoint.service</code></li> </ul> <p>Jump to Step 6 below.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#option-b-testpoint-lets-encrypt-shared-apache-and-certs","title":"Option B \u2014 Testpoint + Let's Encrypt (shared Apache and certs)","text":"<p>This mode runs two containers (<code>perfsonar-testpoint</code> and <code>certbot</code>) and bind-mounts the following host paths so Apache content and certificates persist on the host and are shared between containers:</p> <ul> <li> <p><code>/opt/perfsonar-tp/psconfig</code> \u2192 <code>/etc/perfsonar/psconfig</code> \u2014 perfSONAR configuration</p> </li> <li> <p><code>/var/www/html</code> \u2192 <code>/var/www/html</code> \u2014 Apache webroot (shared for HTTP-01 challenges)</p> </li> <li> <p><code>/etc/apache2</code> \u2192 <code>/etc/apache2</code> \u2014 Apache configuration (for SSL certificate patching)</p> </li> <li> <p><code>/etc/letsencrypt</code> \u2192 <code>/etc/letsencrypt</code> \u2014 Let's Encrypt certificates and state</p> </li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#1-seed-required-host-directories-required-before-first-compose-up","title":"1) Seed required host directories (REQUIRED before first compose up)","text":"<p>Why seed? The perfsonar-testpoint container requires baseline configuration files from the image to be present on the host filesystem. Without seeding, the bind-mounted directories would be empty, causing Apache and perfSONAR services to fail.</p> <p>What's seeded:</p> <ul> <li> <p><code>/opt/perfsonar-tp/psconfig</code> \u2014 perfSONAR pSConfig files (baseline remotes and archives)</p> </li> <li> <p><code>/var/www/html</code> \u2014 Apache webroot with index.html (required for healthcheck)</p> </li> <li> <p><code>/etc/apache2</code> \u2014 Apache config including <code>sites-available/default-ssl.conf</code> (patched by entrypoint wrapper)</p> </li> </ul> <p>What's NOT seeded:</p> <ul> <li><code>/etc/letsencrypt</code> \u2014 Certbot creates this automatically; no pre-seeding needed</li> </ul> <p>Run the bundled seeding helper script (automatically installed in Step 2):</p> <pre><code>/opt/perfsonar-tp/tools_scripts/seed_testpoint_host_dirs.sh\n</code></pre> Seed script details <ul> <li>Pulls the latest perfSONAR testpoint image</li> <li>Creates temporary containers to extract baseline files</li> <li>Copies content to host directories</li> <li>Verifies seeding was successful</li> <li>Skips seeding if directories already have content (idempotent)</li> </ul> <p>Verify seeding succeeded:</p> <pre><code># Should show config files\nls -la /opt/perfsonar-tp/psconfig\n\n# Should show index.html and perfsonar/ directory\nls -la /var/www/html\n\n# Should show sites-available/, sites-enabled/, etc.\nls -la /etc/apache2\n</code></pre> SELinux labeling handled automatically <p>If SELinux is enforcing, the <code>:Z</code> and <code>:z</code> options in the compose files will cause Podman to relabel the host paths when containers start. No manual <code>chcon</code> commands are required.</p> <p>SELinux Volume Labels:</p> <ul> <li><code>:Z</code> (uppercase) - Exclusive access. Podman creates a unique SELinux label for this volume that only this specific container can access. Use for volumes that should not be shared between containers.</li> <li><code>:z</code> (lowercase) - Shared access. Podman uses a shared SELinux label that multiple containers can access. Use for volumes that need to be accessed by multiple containers. In our compose files:</li> <li><code>/etc/letsencrypt:/etc/letsencrypt:Z</code> - Exclusive to testpoint container</li> <li><code>/var/www/html:/var/www/html:z</code> - Shared between testpoint and certbot containers</li> <li><code>/etc/apache2:/etc/apache2:Z</code> - Exclusive to testpoint container</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#2-deploy-the-testpoint-with-automatic-ssl-patching-recommended","title":"2) Deploy the testpoint with automatic SSL patching (recommended)","text":"<p>Prerequisites: Ensure <code>/opt/perfsonar-tp/tools_scripts</code> exists from Step 2 (bootstrap):</p> <pre><code>ls -la /opt/perfsonar-tp/tools_scripts/testpoint-entrypoint-wrapper.sh\n</code></pre> <p>If the file is missing, run the Step 2 bootstrap first:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh \\\n    | bash -s -- /opt/perfsonar-tp\n</code></pre> <p>Deploy using the compose file with automatic Apache SSL certificate patching. This approach uses an entrypoint wrapper that auto-discovers Let's Encrypt certificates on container startup and automatically patches the Apache configuration.</p> <p>Download the auto-patching compose file:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/docker-compose.testpoint-le-auto.yml \\\n    -o /opt/perfsonar-tp/docker-compose.yml\n</code></pre> The SERVER_FQDN use is optional <p>Note: The <code>SERVER_FQDN</code> environment variable is optional. The entrypoint wrapper will auto-discover certificates in <code>/etc/letsencrypt/live</code>  and use the first one found. Only set <code>SERVER_FQDN</code> if you have multiple certificates and need to specify which one to use.</p> <p>If you want to explicitly set the FQDN (optional):</p> <pre><code># Optional: only needed if you have multiple certificates\nsed -i 's/# - SERVER_FQDN=.*/- SERVER_FQDN=&lt;YOUR_FQDN&gt;/' /opt/perfsonar-tp/docker-compose.yml\n</code></pre> <p>Start the containers:</p> <pre><code>cd /opt/perfsonar-tp\npodman-compose up -d\n</code></pre> <p>At this point, the testpoint is running with self-signed certificates. The certbot container is also running but won't renew anything until you obtain the initial certificates.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#ensure-containers-restart-automatically-on-reboot-systemd-units-for-testpoint-certbot-required","title":"Ensure containers restart automatically on reboot (systemd units for testpoint &amp; certbot - REQUIRED)","text":"<p>podman-compose limitation with systemd containers</p> <p>The perfSONAR testpoint image runs systemd internally and requires the <code>--systemd=always</code> flag to function correctly. podman-compose does not support this flag, which causes containers to crash-loop after reboot with exit code 255.</p> <p>You must use the systemd unit approach below instead of relying on compose alone.</p> <p>Install and enable the systemd units so containers start on boot with proper systemd support:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-units.sh \\\n    -o /tmp/install-systemd-units.sh\nchmod 0755 /tmp/install-systemd-units.sh\n\n# Install both testpoint and certbot systemd units\n/tmp/install-systemd-units.sh --install-dir /opt/perfsonar-tp --with-certbot\n\n# Enable and start services\nsystemctl enable --now perfsonar-testpoint.service perfsonar-certbot.service\n\n# Verify services\nsystemctl status perfsonar-testpoint.service perfsonar-certbot.service --no-pager\npodman ps\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#3-obtain-your-first-lets-encrypt-certificate-one-time","title":"3) Obtain your first Let's Encrypt certificate (one-time)","text":"<p>Use Certbot in standalone mode to obtain the initial certificates. The perfsonar-testpoint image is patched to NOT listen on port 80, so port 80 is available for Certbot's HTTP-01 challenge.</p> <p>Important: Stop the certbot sidecar temporarily to free port 80:</p> <pre><code>podman stop certbot\n</code></pre> <p>Now run Certbot standalone with host networking to bind port 80:</p> <pre><code>podman run --rm --net=host \\\n    -v /etc/letsencrypt:/etc/letsencrypt:Z \\\n    -v /var/www/html:/var/www/html:Z \\\n    docker.io/certbot/certbot:latest certonly \\\n    --standalone --agree-tos --non-interactive \\\n    -d &lt;SERVER_FQDN&gt; -d &lt;ALT_FQDN&gt; \\\n    -m &lt;LETSENCRYPT_EMAIL&gt;\n</code></pre> Certbot command explained <p>Podman options:</p> <ul> <li><code>--rm</code> - Remove container after it exits</li> <li><code>--net=host</code> - Use host network (allows binding port 80)</li> <li><code>-v /etc/letsencrypt:/etc/letsencrypt:Z</code> - Mount certificate storage with exclusive SELinux label</li> <li><code>-v /var/www/html:/var/www/html:Z</code> - Mount webroot for HTTP-01 challenge</li> </ul> <p>Certbot options:</p> <ul> <li><code>certonly</code> - Obtain certificate only, don't install it</li> <li><code>--standalone</code> - Run standalone HTTP server on port 80 for ACME HTTP-01 challenge</li> <li><code>--agree-tos</code> - Agree to Let's Encrypt Terms of Service</li> <li><code>--non-interactive</code> - Don't prompt for input (required for automation)</li> <li><code>-d &lt;FQDN&gt;</code> - Domain name(s) for the certificate (repeat for each domain/SAN)</li> <li><code>-m &lt;EMAIL&gt;</code> - Email for renewal notifications and account recovery</li> </ul> <p>Replace:</p> <ul> <li><code>&lt;SERVER_FQDN&gt;</code> with your primary hostname (e.g., <code>psum05.aglt2.org</code>)</li> <li><code>&lt;ALT_FQDN&gt;</code> with additional FQDNs if needed (one <code>-d</code> flag per FQDN)</li> <li><code>&lt;LETSENCRYPT_EMAIL&gt;</code> with your email for certificate notifications</li> </ul> <p>After successful issuance, restart the perfsonar-testpoint container to trigger the automatic patching:</p> <pre><code>podman restart perfsonar-testpoint\n</code></pre> <p>Check the logs to verify the SSL config was patched:</p> <pre><code>podman logs perfsonar-testpoint 2&gt;&amp;1 | grep -A5 \"Patching Apache\"\n</code></pre> <p>You should see output confirming the certificate paths were updated.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#4-restart-the-certbot-sidecar-for-automatic-renewals","title":"4) Restart the certbot sidecar for automatic renewals","text":"<p>Now that certificates are in place, restart the certbot sidecar to enable automatic renewals:</p> <pre><code>podman start certbot\n</code></pre> <p>The certbot container runs a renewal loop that checks for expiring certificates every 12 hours.</p> <p>Automatic Container Restart: After each successful certificate renewal, certbot automatically runs a deploy hook script (<code>certbot-deploy-hook.sh</code>) that gracefully restarts the <code>perfsonar-testpoint</code> container. This ensures the new certificates are loaded without manual intervention. The deploy hook uses the mounted Podman socket</p> <p>(<code>/run/podman/podman.sock</code>) to communicate with the host's container runtime.</p> <p>Note: The certbot container in this setup uses host networking mode (via <code>network_mode: host</code> in the compose file) so it can bind directly to port 80 for HTTP-01 challenges during renewals. This works because the perfsonar- testpoint Apache is patched to NOT listen on port 80. Both containers share the host network namespace without conflict.</p> <p>Test renewal with a dry-run:</p> <pre><code>podman exec certbot certbot renew --dry-run\n</code></pre> <p>If successful, certificates will auto-renew before expiry, and the testpoint will be automatically restarted to load the new certificates. You can verify this behavior by checking the certbot logs after a renewal:</p> <pre><code>podman logs certbot 2&gt;&amp;1 | grep -A5 \"deploy hook\"\n</code></pre> Alternative: Manual SSL Patching (without automatic entrypoint wrapper) <p>If you prefer not to use the automatic patching entrypoint wrapper, you can use the standard compose file and manually patch the Apache SSL configuration after obtaining certificates.</p> <ol> <li>Use <code>docker-compose.testpoint-le.yml</code> instead of <code>docker-compose.testpoint-le-auto.yml</code></li> <li>After obtaining Let's Encrypt certificates, run: <pre><code>/opt/perfsonar-tp/tools_scripts/patch_apache_ssl_for_letsencrypt.sh &lt;SERVER_FQDN&gt;\n</code></pre></li> <li>Reload Apache in the running container: <pre><code>podman exec perfsonar-testpoint apachectl -k graceful\n</code></pre></li> </ol> <p>This approach requires manual intervention after initial certificate issuance and any time the container is recreated. The automatic approach (using the entrypoint wrapper) eliminates this manual step.</p> Troubleshooting: Container fails with 'executable file not found' error <p>Error: <code>Error: unable to start container: crun: executable file /opt/perfsonar-tp/tools_scripts/testpoint- entrypoint-wrapper.sh not found</code></p> <p>Cause: The <code>/opt/perfsonar-tp/tools_scripts</code> directory doesn't exist or the entrypoint wrapper wasn't downloaded.</p> <p>Fix: Run the Step 2 bootstrap script to fetch all helper scripts:</p> <pre><code>curl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh \\\n    | bash -s -- /opt/perfsonar-tp\n</code></pre> <p>Then verify the entrypoint wrapper exists:</p> <pre><code>ls -la /opt/perfsonar-tp/tools_scripts/testpoint-entrypoint-wrapper.sh\n</code></pre> <p>If you've already started the container and it failed, remove it before retrying:</p> <pre><code>podman-compose down\npodman-compose up -d\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-6-configure-and-enroll-in-psconfig","title":"Step 6 \u2013 Configure and Enroll in pSConfig","text":"<p>Skip this step if you used the orchestrator (Path A)</p> <p>The orchestrator automates pSConfig enrollment. If you ran it in Step 2, skip to Step 7.</p> <p>We need to enroll your testpoint with the OSG/WLCG pSConfig service so tests are auto-configured. Use the \"auto URL\" for each FQDN you expose for  perfSONAR (one or two depending on whether you split latency/throughput by hostname).</p> <p>Basic enroll (interactive root on the host; runs inside the container) if you have only one entry to make (automation alternative below):</p> <pre><code># Add auto URLs (configures archives too) and show configured remotes\npodman exec -it perfsonar-testpoint psconfig remote --configure-archives add \\\n    \"https://psconfig.opensciencegrid.org/pub/auto/ps-lat-example.my.edu\"\n\npodman exec -it perfsonar-testpoint psconfig remote list\n</code></pre> <p>If there are any stale/old/incorrect entries, you can remove them:</p> <pre><code>podman exec -it perfsonar-testpoint psconfig remote delete \"&lt;old-url&gt;\"\n</code></pre> <p>Automation tip: derive FQDNs from your configured IPs (PTR lookup) and enroll automatically. Review the list before applying.</p> <pre><code># Dry run only (show planned URLs):\n/opt/perfsonar-tp/tools_scripts/perfSONAR-auto-enroll-psconfig.sh -n\n\n# Typical usage (podman):\n/opt/perfsonar-tp/tools_scripts/perfSONAR-auto-enroll-psconfig.sh -v\n\npodman exec -it perfsonar-testpoint psconfig remote list\n</code></pre> The auto enroll psconfig script details <ul> <li>Parses IP lists from <code>/etc/perfSONAR-multi-nic-config.conf</code>  (<code>NIC_IPV4_ADDRS</code> / <code>NIC_IPV6_ADDRS</code>).</li> <li>Performs reverse DNS lookups (getent/dig) to derive FQDNs.</li> <li>Deduplicates while preserving discovery order.</li> <li>Adds each <code>https://psconfig.opensciencegrid.org/pub/auto/&lt;FQDN&gt;</code> with <code>--configure-archives</code>.</li> <li>Lists configured remotes and returns non-zero if any enrollment fails.</li> </ul> <p>Integrate into provisioning CI by running with <code>-n</code> (dry-run) for approval and then <code>-y</code> once approved.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-7-register-and-configure-with-wlcgosg","title":"Step 7 \u2013 Register and Configure with WLCG/OSG","text":"<ol> <li> <p>OSG/WLCG registration workflow:</p> Registration steps and portals <ul> <li>Register the host in OSG topology.</li> <li>Create or update a GGUS ticket announcing the new measurement point.</li> <li>In GOCDB, add the service endpoint         <code>org.opensciencegrid.crc.perfsonar-testpoint</code> bound to this host.</li> </ul> </li> <li> <p>Document memberships:</p> <p>Update your site wiki or change log with assigned mesh names, feed  URLs, and support contacts.</p> </li> <li> <p>Update Lookup Service registration inside the container:</p> <p>Use the helper script to edit <code>/etc/perfsonar/lsregistrationdaemon.conf</code> inside the running <code>perfsonar-testpoint</code> container and restart the daemon only if needed. Install and run examples below, pick which type you want (root shell):</p> <pre><code># Preview changes only (uses the copy from /opt/perfsonar-tp/tools_scripts)\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh update \\\n    --dry-run --site-name \"Acme Co.\" --project WLCG \\\n    --admin-email admin@example.org --admin-name \"pS Admin\"\n\n# Restore previously saved settings from the Prerequisites extract (if you saved a restore script earlier)\nbash /root/restore-lsreg.sh\n\n# Apply new settings and restart the daemon inside the container\n/opt/perfsonar-tp/tools_scripts/perfSONAR-update-lsregistration.sh create \\\n    --site-name \"Acme Co.\" --domain example.org --project WLCG --project OSG \\\n    --city Berkeley --region CA --country US --zip 94720 \\\n    --latitude 37.5 --longitude -121.7469 \\\n    --admin-name \"pS Admin\" --admin-email admin@example.org\n</code></pre> </li> <li> <p>Automatic image updates and safe restarts</p> <p>Keep containers current and only restart them when their image actually changes.</p> Auto-update for compose-managed containers <p>Since these containers are managed by <code>podman-compose</code>, we use a different approach than systemd-managed containers. Create a simple script and systemd timer to periodically pull new images and restart containers if updates are available.</p> <ol> <li> <p>Create an update script:</p> <pre><code>cat &gt; /usr/local/bin/perfsonar-auto-update.sh &lt;&lt; 'EOF'\n#!/bin/bash\n# perfsonar-auto-update.sh - Check for and apply container image updates\nset -e\n\nCOMPOSE_DIR=\"/opt/perfsonar-tp\"\nLOGFILE=\"/var/log/perfsonar-auto-update.log\"\n\nlog() {\n    echo \"$(date -Iseconds) $*\" | tee -a \"$LOGFILE\"\n}\n\ncd \"$COMPOSE_DIR\"\n\nlog \"Checking for image updates...\"\n\n# Pull latest images\nif podman-compose pull 2&gt;&amp;1 | tee -a \"$LOGFILE\" | grep -q \"Downloaded newer image\"; then\n    log \"New images found - recreating containers...\"\n    podman-compose up -d\n    log \"Containers updated successfully\"\nelse\n    log \"No updates available\"\nfi\nEOF\n\nchmod +x /usr/local/bin/perfsonar-auto-update.sh\n</code></pre> </li> <li> <p>Create a systemd service:</p> <pre><code>cat &gt; /etc/systemd/system/perfsonar-auto-update.service &lt;&lt; 'EOF'\n[Unit]\nDescription=perfSONAR Container Auto-Update\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/perfsonar-auto-update.sh\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> </li> <li> <p>Create a systemd timer (runs daily at 3 AM):</p> <pre><code>cat &gt; /etc/systemd/system/perfsonar-auto-update.timer &lt;&lt; 'EOF'\n\n[Unit]\nDescription=perfSONAR Container Auto-Update Timer\n\n[Timer]\nOnCalendar=daily\nRandomizedDelaySec=1h\nPersistent=true\n\n[Install]\nWantedBy=timers.target\nEOF\n</code></pre> </li> <li> <p>Enable and start the timer:</p> <pre><code>systemctl daemon-reload\nsystemctl enable --now perfsonar-auto-update.timer\n</code></pre> </li> <li> <p>Verify the timer is active:</p> <pre><code>systemctl list-timers perfsonar-auto-update.timer\n</code></pre> </li> <li> <p>Test manually (optional):</p> <pre><code>systemctl start perfsonar-auto-update.service\njournalctl -u perfsonar-auto-update.service -n 50\n</code></pre> </li> <li> <p>Monitor the update log:</p> <pre><code>tail -f /var/log/perfsonar-auto-update.log\n</code></pre> </li> </ol> </li> </ol> <p>This approach ensures containers are updated only when new images are available, minimizing unnecessary restarts while keeping your deployment current.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#step-8-post-install-validation","title":"Step 8 \u2013 Post-Install Validation","text":"<p>Perform these checks before handing the host over to operations:</p> <ol> <li> <p>System services:</p> Verify Podman runtime and containers <pre><code># Check Podman service is available\nsystemctl status podman\n\n# Verify containers are managed by compose\ncd /opt/perfsonar-tp &amp;&amp; podman-compose ps\n\n# Alternative: check containers directly\npodman ps --filter name=perfsonar\n</code></pre> <p>Ensure Podman is active and containers are running.</p> </li> <li> <p>Container health:</p> Check container status and logs <pre><code># Check all containers are running and healthy\npodman ps --format 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}'\n\n# Check perfsonar-testpoint logs for errors\npodman logs perfsonar-testpoint --tail 50\n\n# If using Let's Encrypt, check certbot logs\npodman logs certbot --tail 20 2&gt;/dev/null || echo \"Certbot container not present (testpoint-only mode)\"\n\n# Verify services inside container are running\npodman exec perfsonar-testpoint systemctl status apache2 psconfig-pscheduler-agent --no-pager\n</code></pre> </li> <li> <p>Network path validation:</p> Test network connectivity and routing <p>Test throughput to a remote testpoint (run from inside the container): <pre><code>podman exec -it perfsonar-testpoint pscheduler task throughput --dest &lt;remote-testpoint&gt;\n</code></pre></p> <p>Check routing from the host: <pre><code>tracepath -n &lt;remote-testpoint&gt;\nip route get &lt;remote-testpoint-ip&gt;\n</code></pre></p> <p>Confirm traffic uses the intended policy-based routes (check <code>ip route get &lt;dest&gt;</code>).</p> </li> <li> <p>Security posture:</p> Check firewall, fail2ban, and SELinux <pre><code># Check nftables firewall rules\nnft list ruleset | grep perfsonar\n\n# Check fail2ban status (if installed in Step 4)\nif command -v fail2ban-client &gt;/dev/null 2&gt;&amp;1; then\n    fail2ban-client status\nelse\n    echo \"fail2ban not installed (optional)\"\nfi\n\n# Check for recent SELinux denials\nif command -v ausearch &gt;/dev/null 2&gt;&amp;1; then\n    ausearch --message AVC --just-one\nelif [ -f /var/log/audit/audit.log ]; then\n    grep -i \"avc.*denied\" /var/log/audit/audit.log | tail -5\nelse\n    echo \"SELinux audit tools not available\"\nfi\n</code></pre> <p>Investigate any SELinux denials or repeated Fail2Ban bans.</p> </li> <li> <p>LetsEncrypt certificate check:</p> Verify certificate validity <pre><code># Check certificate via HTTPS connection\necho | openssl s_client -connect &lt;SERVER_FQDN&gt;:443 -servername &lt;SERVER_FQDN&gt; 2&gt;/dev/null | openssl x509 -noout -dates -issuer\n\n# Alternative: Check certificate files directly\nsudo openssl x509 -in /etc/letsencrypt/live/&lt;SERVER_FQDN&gt;/cert.pem -noout -dates -issuer\n</code></pre> <p>Ensure the issuer is Let's Encrypt and the validity period is acceptable. This check only applies if you configured Let's Encrypt in Step 3.</p> </li> <li> <p>Reporting:</p> Run perfSONAR diagnostic reports <p>Run the perfSONAR troubleshoot command from inside the container and send outputs to operations: <pre><code>podman exec -it perfsonar-testpoint pscheduler troubleshoot\n</code></pre></p> </li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#ongoing-maintenance","title":"Ongoing Maintenance","text":"<ul> <li> <p>Quarterly or as-needed: Re-validate routing policy and nftables rules after network changes or security audits.</p> </li> <li> <p>Monthly or during maintenance windows: Apply OS updates (<code>dnf update</code>) and reboot during scheduled downtime.</p> </li> <li> <p>Monitor psconfig feeds for changes in mesh participation and test configuration.</p> </li> <li> <p>Track certificate expiry with <code>certbot renew --dry-run</code> if you rely on Let's Encrypt (automatic renewal is configured but monitoring is recommended).</p> </li> <li> <p>Review container logs periodically for errors: <code>podman logs perfsonar-testpoint</code> and <code>podman logs certbot</code>.</p> </li> <li> <p>Verify auto-update timer is active: <code>systemctl list-timers perfsonar-auto-update.timer</code>.</p> </li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#troubleshooting","title":"Troubleshooting","text":""},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#container-issues","title":"Container Issues","text":"Container won't start or exits immediately <p>Symptoms: <code>podman ps</code> shows no running containers, or container exits shortly after starting.</p> <p>Diagnostic steps:</p> <pre><code># Check container logs\npodman logs perfsonar-testpoint\n\n# Check for systemd initialization errors\npodman logs perfsonar-testpoint 2&gt;&amp;1 | grep -i \"failed\\|error\"\n\n# Verify compose file syntax\ncd /opt/perfsonar-tp\npodman-compose config\n</code></pre> <p>Common causes:</p> <ul> <li>Missing entrypoint wrapper: Ensure <code>/opt/perfsonar-tp/tools_scripts/testpoint-entrypoint-wrapper.sh</code> exists</li> <li>SELinux denials: Check <code>ausearch -m avc -ts recent</code> and consider temporarily setting to permissive mode for testing</li> <li>Incorrect bind-mount paths: Verify all host directories exist and have correct permissions</li> <li>Cgroup issues: Ensure <code>cgroupns: private</code> is set and no manual cgroup bind-mounts exist</li> </ul> Container won't start or exits immediately <p>Symptoms: <code>podman ps</code> shows no running containers, or container exits shortly after starting.</p> <p>Diagnostic steps:</p> <pre><code># Check container logs\npodman logs perfsonar-testpoint\n\n# Check for systemd initialization errors\npodman logs perfsonar-testpoint 2&gt;&amp;1 | grep -i \"failed\\|error\"\n\n# Verify compose file syntax\ncd /opt/perfsonar-tp\npodman-compose config\n</code></pre> <p>Common causes:</p> <ul> <li>Missing entrypoint wrapper: Ensure <code>/opt/perfsonar-tp/tools_scripts/testpoint-entrypoint-wrapper.sh</code> exists</li> <li>SELinux denials: Check <code>ausearch -m avc -ts recent</code> and consider temporarily setting to permissive mode for testing</li> <li>Incorrect bind-mount paths: Verify all host directories exist and have correct permissions</li> <li>Cgroup issues: Ensure <code>cgroupns: private</code> is set and no manual cgroup bind-mounts exist</li> </ul> Container crashes after reboot with exit code 255 <p>Symptoms: Containers run fine when started manually but crash-loop after host reboot. Logs show repeated restarts with exit code 255.</p> <p>Cause: The perfSONAR testpoint image runs systemd internally but podman-compose doesn't support the <code>--systemd=always</code> flag required for proper systemd operation in containers.</p> <p>Diagnostic steps:</p> <pre><code># Check container status\npodman ps -a\n\n# Check systemd service status\nsystemctl status perfsonar-testpoint.service\n\n# View recent container logs\npodman logs perfsonar-testpoint --tail 100\n\n# Check if using compose-based service (BAD)\ngrep -A5 \"ExecStart\" /etc/systemd/system/perfsonar-testpoint.service\n</code></pre> <p>Solution:</p> <p>Replace the compose-based systemd service with proper systemd units that use <code>podman run --systemd=always</code>:</p> <pre><code># Stop and disable old service\nsystemctl stop perfsonar-testpoint.service\nsystemctl disable perfsonar-testpoint.service\n\n# Install new systemd units\ncurl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-units.sh \\\n    -o /tmp/install-systemd-units.sh\nchmod 0755 /tmp/install-systemd-units.sh\n\n# For testpoint only:\n/tmp/install-systemd-units.sh --install-dir /opt/perfsonar-tp\n\n# For testpoint + certbot:\n/tmp/install-systemd-units.sh --install-dir /opt/perfsonar-tp --with-certbot\n\n# Enable and start\nsystemctl enable --now perfsonar-testpoint.service\n\n# If using certbot:\nsystemctl enable --now perfsonar-certbot.service\n\n# Verify containers are running\npodman ps\ncurl -kI https://127.0.0.1/\n</code></pre> <p>Verification:</p> <p>After installing the new units, the testpoint should: - Start successfully on boot - Run systemd properly inside the container - Maintain state across reboots - Show \"Up\" status in <code>podman ps</code> (not \"Exited\" or crash-looping)</p> Certbot service fails with 'Unable to open config file' error <p>Symptoms: <code>perfsonar-certbot.service</code> fails immediately after starting with exit code 2. Logs show: <code>certbot: error: Unable to open config file: trap exit TERM; while...</code></p> <p>Cause: The certbot container image has a built-in entrypoint that expects certbot commands directly. When using a shell loop for renewal, the entrypoint tries to parse the shell command as a certbot config file, causing this error.</p> <p>Diagnostic steps:</p> <pre><code># Check certbot service status\nsystemctl status perfsonar-certbot.service\n\n# View detailed logs\njournalctl -u perfsonar-certbot.service -n 50\n\n# Check for the error in logs\njournalctl -u perfsonar-certbot.service | grep \"Unable to open config file\"\n\n# Verify service file configuration\ngrep -A5 \"ExecStart\" /etc/systemd/system/perfsonar-certbot.service\n</code></pre> <p>Solution:</p> <p>The certbot service needs two flags: - <code>--systemd=always</code> for proper systemd integration and reboot persistence - <code>--entrypoint=/bin/sh</code> to override the built-in entrypoint</p> <p>Re-run the installation script to get the fixed version:</p> <pre><code># Stop current service\nsystemctl stop perfsonar-certbot.service\n\n# Download and install updated systemd units\ncurl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-units.sh \\\n    -o /tmp/install-systemd-units.sh\nchmod 0755 /tmp/install-systemd-units.sh\n\n# Install with certbot support\n/tmp/install-systemd-units.sh --install-dir /opt/perfsonar-tp --with-certbot\n\n# Start the fixed service\nsystemctl daemon-reload\nsystemctl start perfsonar-certbot.service\n\n# Verify it's running\nsystemctl status perfsonar-certbot.service\npodman ps | grep certbot\n</code></pre> <p>Expected result: The certbot container should be running (not exiting) and the service should be in \"active (running)\" state.</p> SELinux denials blocking container operations <p>Symptoms: Container starts but services fail, permission denied errors in logs.</p> <p>Diagnostic steps:</p> <pre><code># Check for recent SELinux denials\nausearch -m avc -ts recent\n\n# Temporarily set to permissive for testing\nsetenforce 0\n\n# Test if issue resolves, then check audit log\nausearch -m avc -ts recent &gt; /tmp/selinux-denials.txt\n</code></pre> <p>Solutions:</p> <ul> <li>Verify volume labels are correct (<code>:Z</code> for exclusive, <code>:z</code> for shared)</li> <li>Recreate containers to reapply SELinux labels: <code>podman-compose down &amp;&amp; podman-compose up -d</code></li> <li>If persistent issues, consider creating custom SELinux policy or running in permissive mode</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#networking-issues","title":"Networking Issues","text":"Policy-based routing not working correctly <p>Symptoms: Traffic not using expected interfaces, routing to wrong gateway.</p> <p>Diagnostic steps:</p> <pre><code># Check routing rules\nip rule show\n\n# Check routing tables\nip route show table all\n\n\n# Test specific route lookup\nip route get &lt;destination-ip&gt;\n\n# Check NetworkManager connections\nnmcli connection show\n\n# Review PBR script log\ntail -100 /var/log/perfSONAR-multi-nic-config.log\n</code></pre> <p>Solutions:</p> <ul> <li>Verify <code>/etc/perfSONAR-multi-nic-config.conf</code> has correct IPs and gateways</li> <li>Reapply configuration: <code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --yes</code></li> <li>Reboot if rules are not being applied correctly</li> <li>Check for conflicting NetworkManager or systemd-networkd rules</li> </ul> DNS resolution failing for test endpoints <p>Symptoms: perfSONAR tests fail with \"unknown host\" or DNS errors.</p> <p>Diagnostic steps:</p> <pre><code># Test DNS resolution from container\npodman exec -it perfsonar-testpoint dig &lt;remote-testpoint&gt;\n\n# Check container's resolv.conf\n\npodman exec -it perfsonar-testpoint cat /etc/resolv.conf\n\n# Verify forward and reverse DNS\n/opt/perfsonar-tp/tools_scripts/check-perfsonar-dns.sh\n</code></pre> <p>Solutions:</p> <ul> <li>Ensure DNS servers are correctly configured on host</li> <li>Fix missing PTR records in DNS zones</li> <li>Verify forward A/AAAA records match reverse PTR records</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#certificate-issues","title":"Certificate Issues","text":"Let's Encrypt certificate issuance fails <p>Symptoms: Certbot fails with \"Failed to authenticate\" or \"Connection refused\" errors.</p> <p>Diagnostic steps:</p> <pre><code># Check if port 80 is open\nnft list ruleset | grep \"80\"\n\n# Verify Apache is NOT listening on port 80 in container\npodman exec perfsonar-testpoint netstat -tlnp | grep :80\n\n# Test port 80 accessibility from external host\ncurl -v http://&lt;your-fqdn&gt;/\n\n# Run certbot in verbose mode\n\npodman run --rm --net=host \\\n    -v /etc/letsencrypt:/etc/letsencrypt:Z \\\n    -v /var/www/html:/var/www/html:Z \\\n    docker.io/certbot/certbot:latest certonly \\\n    --standalone -d &lt;SERVER_FQDN&gt; -m &lt;EMAIL&gt; --dry-run -vvv\n</code></pre> <p>Common causes:</p> <ul> <li>Port 80 blocked by firewall: Add with <code>perfSONAR-install-nftables.sh --ports=80,443</code></li> <li>Apache listening on port 80: Verify testpoint-entrypoint-wrapper.sh patched Apache correctly</li> <li>DNS not propagated: Wait for DNS changes to propagate globally</li> <li>Rate limiting: Let's Encrypt has rate limits; wait if you've hit them</li> </ul> Certificate not loaded after renewal <p>Symptoms: Old certificate still in use after automatic renewal.</p> <p>Diagnostic steps:</p> <pre><code># Check certificate files\nls -la /etc/letsencrypt/live/&lt;fqdn&gt;/\n\n# Verify deploy hook is configured\npodman logs certbot 2&gt;&amp;1 | grep \"deploy hook\"\n\n# Check if container restarted\npodman ps --format 'table {{.Names}}\\t{{.Status}}'\n\n# Manually restart testpoint\npodman restart perfsonar-testpoint\n</code></pre> <p>Solutions:</p> <ul> <li>Verify deploy hook script exists and is executable: <code>/opt/perfsonar-tp/tools_scripts/certbot-deploy-hook.sh</code></li> <li>Ensure deploy hook is mounted in container at: <code>/etc/letsencrypt/renewal-hooks/deploy/certbot-deploy-hook.sh</code></li> <li>Verify Podman socket is mounted in certbot container: <code>/run/podman/podman.sock</code></li> <li>Check deploy hook logs: <code>journalctl -u perfsonar-certbot.service | grep deploy</code></li> <li>Manually restart testpoint after renewals if deploy hook fails: <code>podman restart perfsonar-testpoint</code></li> </ul> <p>Note: Certbot automatically executes scripts in <code>/etc/letsencrypt/renewal-hooks/deploy/</code> when certificates are renewed. Do not use <code>--deploy-hook</code> parameter with full paths ending in <code>.sh</code> as certbot will append <code>-hook</code> to the filename.</p>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#perfsonar-service-issues","title":"perfSONAR Service Issues","text":"perfSONAR services not running <p>Symptoms: Web interface not accessible, tests not running.</p> <p>Diagnostic steps:</p> <pre><code># Check service status inside container\npodman exec perfsonar-testpoint systemctl status apache2\npodman exec perfsonar-testpoint systemctl status pscheduler-ticker\npodman exec perfsonar-testpoint systemctl status owamp-server\n\n# Check for errors in service logs\npodman exec perfsonar-testpoint journalctl -u apache2 -n 50\npodman exec perfsonar-testpoint journalctl -u pscheduler-ticker -n 50\n</code></pre> <p>Solutions:</p> <ul> <li>Restart services inside container: <code>podman exec perfsonar-testpoint systemctl restart apache2</code></li> <li>Check Apache SSL configuration was patched correctly</li> <li>Verify certificates are in place: <code>ls -la /etc/letsencrypt/live/</code></li> <li>Restart container: <code>podman restart perfsonar-testpoint</code></li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#auto-update-issues","title":"Auto-Update Issues","text":"Auto-update not working <p>Symptoms: Containers not updating despite new images available.</p> <p>Diagnostic steps:</p> <pre><code># Check timer status\nsystemctl status perfsonar-auto-update.timer\nsystemctl list-timers perfsonar-auto-update.timer\n\n# Check service logs\njournalctl -u perfsonar-auto-update.service -n 100\n\n# Check update log\ntail -50 /var/log/perfsonar-auto-update.log\n\n# Manually test update\nsystemctl start perfsonar-auto-update.service\n</code></pre> <p>Solutions:</p> <ul> <li>Enable timer if not active: <code>systemctl enable --now perfsonar-auto-update.timer</code></li> <li>Verify script exists and is executable: <code>ls -la /usr/local/bin/perfsonar-auto-update.sh</code></li> <li>Check podman-compose is installed and working</li> <li>Review script for errors and update if needed</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-testpoint/#general-debugging-tips","title":"General Debugging Tips","text":"Useful debugging commands <p>Container management:</p> <pre><code># View all containers (running and stopped)\npodman ps -a\n\n# View container resource usage\npodman stats\n\n# Enter container for interactive debugging\npodman exec -it perfsonar-testpoint /bin/bash\n\n# View compose configuration\ncd /opt/perfsonar-tp &amp;&amp; podman-compose config\n</code></pre> <p>Networking:</p> <pre><code># Check which process is listening on a port\nss -tlnp | grep &lt;port&gt;\n\n# Test connectivity to remote testpoint\nping &lt;remote-ip&gt;\ntraceroute &lt;remote-ip&gt;\n\n# Check nftables rules\nnft list ruleset\n</code></pre> <p>Logs:</p> <pre><code># System journal for container runtime\njournalctl -u podman -n 100\n\n# All logs from a container\npodman logs perfsonar-testpoint --tail=100\n\n# Follow logs in real-time\npodman logs -f perfsonar-testpoint\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/","title":"Installing a perfSONAR Toolkit for WLCG/OSG","text":"<p>This guide walks WLCG/OSG site administrators through end-to-end installation, configuration, and validation of a perfSONAR Toolkit on Enterprise Linux 9 (EL9) using RPM packages. The Toolkit provides a full-featured perfSONAR installation with a local web interface for configuration and monitoring, plus a local measurement archive for data storage.</p> <p>For upstream RPM installation documentation, see: https://docs.perfsonar.net/install_el.html</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#choosing-between-toolkit-and-testpoint","title":"Choosing Between Toolkit and Testpoint","text":""},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#use-perfsonar-toolkit-this-guide-if-you-need","title":"Use perfSONAR Toolkit (this guide) if you need","text":"<ul> <li>Local web interface for configuration, monitoring, and viewing measurement results</li> <li>Local measurement archive to store test data on-site with your own retention policies</li> <li>Full-featured installation with all perfSONAR capabilities</li> <li>Site-specific data retention requirements or regulatory compliance needs</li> <li>On-site troubleshooting access to historical measurement data without external dependencies</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#use-perfsonar-testpoint-instead-if-you-prefer","title":"Use perfSONAR Testpoint instead if you prefer","text":"<ul> <li>Lightweight container-based deployment with minimal local resources</li> <li>Central archiving where measurements are stored at a remote archive (WLCG/OSG central infrastructure)</li> <li>Simplified updates via container image pulls rather than RPM package management</li> <li>Reduced local storage requirements (no local measurement archive)</li> </ul> <p>See Installing a perfSONAR Testpoint for the container-based deployment guide.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#prerequisites-and-planning","title":"Prerequisites and Planning","text":"<p>Before you begin, it may be helpful to gather the following information:</p> <ul> <li> <p>Hardware details: hostname, BMC/iLO/iDRAC credentials (if used), interface names, available storage locations.</p> </li> <li> <p>Network data: IPv4/IPv6 assignments for each NIC, default gateway, internal/external VLAN   information.</p> </li> <li> <p>Operational contacts: site admin email, OSG facility/site name, latitude/longitude.</p> </li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#existing-perfsonar-configuration","title":"Existing perfSONAR configuration","text":"<p>If replacing an existing instance, you may want to back up <code>/etc/perfsonar/</code> files, especially <code>lsregistrationdaemon.conf</code>, and any container volumes. We have a script named<code>perfSONAR-update-lsregistration.sh</code> to extract/save/restore registration config that you may want to use.</p> Quick capture of existing lsregistration config (if you have a src) <p>Download a temp copy:  <pre><code>curl -fsSL \\\n  https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/perfSONAR-update-lsregistration.sh \\\n  -o /tmp/update-lsreg.sh\nchmod 0755 /tmp/update-lsreg.sh\n</code></pre> Use the downloaded tool to extract a restore script: <pre><code>/tmp/update-lsreg.sh extract --output /root/restore-lsreg.sh --local\n</code></pre> Note: Repository clone instructions are in Step 2. Note: All shell commands assume an interactive root shell.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-1-install-and-harden-el9","title":"Step 1 \u2013 Install and Harden EL9","text":"<ol> <li> <p>Provision EL9: Install AlmaLinux, Rocky Linux, or RHEL 9 with the Minimal profile.</p> </li> <li> <p>Set the hostname and time sync: Pick the NIC that will own the default route for the hostname.</p> <pre><code>hostnamectl set-hostname &lt;testpoint-hostname&gt;\nsystemctl enable --now chronyd\ntimedatectl set-timezone &lt;Region/City&gt;\n</code></pre> </li> <li> <p>Disable unused services:</p> <pre><code>systemctl disable --now firewalld NetworkManager-wait-online\ndnf remove -y rsyslog\n</code></pre> Why disable unused services? <p>We recommend disabling unused services during initial provisioning to reduce complexity and avoid unexpected interference with network and container setup. Services such as <code>firewalld</code>, <code>NetworkManager-wait-online</code>, and <code>rsyslog</code> can alter networking state, hold boot or network events, or conflict with the automated nftables/NetworkManager changes performed by the helper scripts. Disabling non-essential services makes the install deterministic, reduces the host attack surface, and avoids delays or race conditions while configuring policy-based routing, nftables rules, and       container networking.</p> </li> <li> <p>Update the system:</p> <p><code>bash dnf -y update</code></p> </li> <li> <p>Record NIC names: Document interface mappings for later PBR configuration.</p> <pre><code>nmcli device status\nip -br addr\n</code></pre> </li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-2-install-perfsonar-toolkit-via-rpm","title":"Step 2 \u2013 Install perfSONAR Toolkit via RPM","text":"<p>After completing Step 1 (minimal OS hardening), install the perfSONAR Toolkit bundle using RPM packages.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-21-configure-dnf-repositories","title":"Step 2.1 \u2013 Configure DNF Repositories","text":"<p>Configure DNF to access EPEL, CRB (CodeReady Builder), and perfSONAR repositories:</p> <pre><code># Install EPEL repository\ndnf install -y epel-release\n\n# Non-RHEL Enable CRB (CodeReady Builder) repository\ndnf config-manager --set-enabled crb  \n# --OR--\n# For RHEL Enable access to codeready-builder. \n# NOTE auto-install script from perfSONAR doesn't set this (tries \"crb\" above which fails for RHEL)\nsubscription-manager repos --enable codeready-builder-for-rhel-9-x86_64-rpms\n\n# Install perfSONAR repository for EL9\ndnf install -y http://software.internet2.edu/rpms/el9/x86_64/latest/packages/perfsonar-repo-0.11-1.noarch.rpm\n\n# Refresh DNF cache\ndnf clean all\n</code></pre> What these repositories provide <ul> <li>EPEL (Extra Packages for Enterprise Linux): Community packages not in base EL9</li> <li>CRB (CodeReady Builder): Additional development and build tools</li> <li>perfSONAR repo: Official perfSONAR packages maintained by Internet2</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-22-install-perfsonar-toolkit-bundle","title":"Step 2.2 \u2013 Install perfSONAR Toolkit Bundle","text":"<p>Install the complete toolkit bundle:</p> <pre><code>dnf install -y perfsonar-toolkit\n</code></pre> <p>This bundle automatically includes:</p> <ul> <li>Core perfSONAR measurement tools (pScheduler, OWAMP, traceroute, throughput tests)</li> <li>perfsonar-toolkit-security: Firewall rules (nftables) and fail2ban configuration</li> <li>perfsonar-toolkit-sysctl: Network tuning parameters optimized for measurements</li> <li>perfsonar-toolkit-systemenv-testpoint: Automatic updates and logging configuration</li> <li>Web interface: Local UI at <code>https://&lt;hostname&gt;/toolkit</code></li> <li>Measurement archive: Local OpenSearch and Logstash for storing test results</li> </ul> <p>Installation takes approximately 5-10 minutes depending on network speed.</p> Alternative automated installation <p>perfSONAR provides a one-line automated installer script: <pre><code>curl -s https://downloads.perfsonar.net/install | sh -s - toolkit\n</code></pre></p> <p>This script performs the same steps as above (configure repos + install bundle).</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-23-run-post-install-configuration-scripts","title":"Step 2.3 \u2013 Run Post-Install Configuration Scripts","text":"<p>The toolkit bundle includes configuration scripts that must be run after installation:</p> <pre><code># Configure system tuning parameters (sysctl)\n/usr/lib/perfsonar/scripts/configure_sysctl\n\n# Configure firewall rules\n/usr/lib/perfsonar/scripts/configure_firewall install\n</code></pre> What these scripts configure <p><code>configure_sysctl</code>: - TCP congestion control algorithm (htcp instead of reno) - Maximum TCP buffer sizes for high-bandwidth paths - Network stack tuning for measurement workloads - Creates <code>/etc/sysctl.d/perfsonar-sysctl.conf</code></p> <p><code>configure_firewall</code>: - Opens required ports for perfSONAR services (pScheduler, OWAMP, HTTP/HTTPS) - Configures nftables rules (compatible with existing rules) - Enables fail2ban with perfSONAR jails - Creates <code>/etc/nftables.d/perfsonar.nft</code></p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-24-install-helper-scripts-for-pbr-and-management","title":"Step 2.4 \u2013 Install Helper Scripts for PBR and Management","text":"<p>Install OSG/WLCG helper scripts for policy-based routing and advanced configuration:</p> <pre><code># Install base packages for helper scripts\ndnf -y install jq curl tar gzip rsync bind-utils \\\n    python3 iproute iputils procps-ng sed grep gawk\n\n# Bootstrap helper scripts\ncurl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh \\\n    -o /tmp/install_tools_scripts.sh\n\nchmod 0755 /tmp/install_tools_scripts.sh\n\n/tmp/install_tools_scripts.sh /opt/perfsonar-toolkit\n</code></pre> <p>Verify bootstrap completed successfully:</p> <pre><code># Check that all helper scripts were downloaded\nls -1 /opt/perfsonar-toolkit/tools_scripts/*.sh | wc -l\n# Should show 17 shell scripts\n\n# Verify key scripts are present and executable\nls -l /opt/perfsonar-toolkit/tools_scripts/{perfSONAR-pbr-nm.sh,perfSONAR-install-nftables.sh,check-perfsonar-dns.sh,fasterdata-tuning.sh}\n</code></pre> Why install helper scripts? <p>The OSG/WLCG helper scripts provide automation for: - Multi-NIC policy-based routing configuration - DNS forward/reverse validation - Registration information management - Custom nftables rules integrated with PBR</p> <p>These scripts are optional but highly recommended for sites with multiple network interfaces or complex routing requirements.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-3-configure-policy-based-routing-pbr","title":"Step 3 \u2013 Configure Policy-Based Routing (PBR)","text":"<p>The script <code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh</code> automates NetworkManager profiles and routing rule setup. It fills out and consumes the network configuration in <code>/etc/perfSONAR-multi-nic-config.conf</code>.</p> Modes of operation <p>By default the script now performs an in-place apply that adjusts routes, rules, and NetworkManager connection properties without deleting existing connections or flushing all system routes. This minimizes disruption and usually avoids the need for a reboot.</p> <p>An optional destructive mode <code>--rebuild-all</code> performs the original full workflow: backup existing profiles, flush all routes and rules, remove every NetworkManager connection, then recreate connections from scratch. Use this only for initial deployments or when you must completely reset inconsistent legacy state.</p> Mode Flag Disruption When to use In-place (default) (none) or <code>--apply-inplace</code> Low (interfaces stay up; rules adjusted) Routine updates, gateway changes, add routes Full rebuild <code>--rebuild-all</code> High (connections removed; brief connectivity drop) First-time setup, severe misconfiguration"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#safety-enhancements","title":"Safety Enhancements","text":"<ul> <li>Detects active SSH session interface and avoids extra disruption to that NIC in in-place mode.</li> <li>Prompts are still skipped with <code>--yes</code>.</li> <li>Dry-run preview supported via <code>--dry-run</code> (combine with <code>--debug</code> for verbose output).</li> <li>Reboot is no longer generally required; only consider one if NetworkManager fails to apply the new rules cleanly.</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#generate-config-file-automatically-or-preview","title":"Generate config file automatically (or preview)","text":"<p>Gateways required for addresses</p> <p>Any NIC with an IPv4 address must also have an IPv4 gateway, and any NIC with an IPv6 address must have an IPv6 gateway. If the generator cannot detect a gateway, it adds a WARNING block to the generated file listing affected NICs. Edit <code>NIC_IPV4_GWS</code>/<code>NIC_IPV6_GWS</code> accordingly before applying changes.</p> Gateway prompts <p>During generation, the script attempts to detect gateways per-NIC. If a NIC has an IP address but no gateway could be determined, it will prompt you interactively to enter an IPv4 and/or IPv6 gateway (or <code>-</code> to skip). Prompts are skipped in non-interactive sessions or when you use <code>--yes</code>. Note, NICs without gateways are assumed to NOT be used for perfSONAR.</p> <p>Preview generation (no changes):</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-debug    \n</code></pre> <p>Generate and write the config file:</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh --generate-config-auto \n</code></pre> <p>The script writes the config file to <code>/etc/perfSONAR-multi-nic-config.conf</code>. Edit to adjust site-specific values (e.g., confirm <code>DEFAULT_ROUTE_NIC</code>, add <code>NIC_IPV4_ADDROUTE</code> entries) and verify the entries.  Next step is to apply the network changes...</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#apply-changes-in-place-default","title":"Apply changes (in-place default)","text":"<p>Connect via console for network changes</p> <p>When applying network changes across an ssh connection, your session may be interrupted.   Please try to run the perfSONAR-pbr-nm.sh script when connected either directly to the console or by using 'nohup' in front of the script invocation.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#in-place-apply-recommended","title":"In-place apply (recommended)","text":"<pre><code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh --yes\n</code></pre> If SSH connection drops during network reconfiguration: <ol> <li>Access via BMC/iLO/iDRAC console or physical console</li> <li>Review <code>/var/log/perfSONAR-multi-nic-config.log</code> for errors</li> <li>Check network state with <code>nmcli connection show</code> and <code>ip addr</code></li> <li>Restore from backup if needed: backups are in <code>/var/backups/nm-connections-&lt;timestamp&gt;/</code></li> <li>Reapply config after corrections: <code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh --yes</code></li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#full-rebuild-destructive-removes-all-nm-connections-first","title":"Full rebuild (destructive \u2013 removes all NM connections first)","text":"<pre><code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh --rebuild-all --yes\n</code></pre> <p>The policy based routing script logs to <code>/var/log/perfSONAR-multi-nic-config.log</code>. After an in-place apply, a reboot is typically unnecessary. If connectivity or rules appear inconsistent (<code>ip rule show</code> / <code>ip route</code> mismatch), consider a manual NetworkManager restart:</p> <pre><code>systemctl restart NetworkManager\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#dns-forward-and-reverse-entries-required","title":"DNS: forward and reverse entries (required)","text":"<p>All IP addresses that will be used for perfSONAR testing MUST have DNS entries: a forward (A/AAAA) record and a matching reverse (PTR) record. This is required so remote test tools and site operators can reliably reach and identify your host, and because some measurement infrastructure and registration systems perform forward/reverse consistency checks.</p> <ul> <li>For single-stack IPv4-only hosts: ensure A and PTR are present and consistent.</li> <li>For single-stack IPv6-only hosts: ensure AAAA and PTR are present and consistent.</li> <li>For dual-stack hosts: both IPv4 and IPv6 addresses used for testing must have matching forward and reverse records (A+PTR and AAAA+PTR).</li> </ul> Run the DNS checker <p>To validate forward/reverse DNS for addresses in <code>/etc/perfSONAR-multi-nic-config.conf</code> you can run a script: <pre><code>/opt/perfsonar-toolkit/tools_scripts/check-perfsonar-dns.sh\n</code></pre> Notes and automation tips:</p> <ul> <li>The script above uses <code>dig</code> (bind-utils package) which is commonly available; you can adapt it   to use <code>host</code> if preferred.</li> <li>Run the check as part of your provisioning CI or as a pre-flight check before enabling measurement registration.</li> <li>For large sites or many addresses, parallelize the checks (xargs -P) or use a small Python   script that leverages <code>dns.resolver</code> for async checks.</li> <li>If your PTR returns a hostname with a trailing dot, the script strips it before the forward check.</li> </ul> <p>If any addresses fail these checks, correct the DNS zone (forward and/or reverse) and allow DNS propagation before proceeding with registration and testing.</p> <p>Verify the routing policy:</p> <pre><code>nmcli connection show\nip rule show\nip route show table &lt;table-id&gt;\n</code></pre> <p>Confirm that non-default interfaces have their own routing tables and that the default interface owns the system default route.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-4-configure-nftables-selinux-and-fail2ban","title":"Step 4 \u2013 Configure nftables, SELinux, and Fail2Ban","text":"<p>Toolkit automatic security hardening</p> <p>The perfsonar-toolkit bundle automatically configured security during installation (Step 2):</p> <ul> <li>nftables rules via <code>/usr/lib/perfsonar/scripts/configure_firewall</code></li> <li>fail2ban with perfSONAR jails for SSH and service protection</li> <li>SELinux policies (if enforcing mode is enabled)</li> </ul> <p>This step is optional and only needed if you want to:</p> <ul> <li>Customize firewall rules beyond the toolkit defaults</li> <li>Integrate with OSG helper scripts for PBR-derived SSH access control</li> <li>Add site-specific security policies</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#optional-customize-security-with-helper-scripts","title":"Optional: Customize Security with Helper Scripts","text":"<p>Use <code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-install-nftables.sh</code> to configure additional hardened nftables rules integrated with your PBR configuration. This script can derive SSH allow-lists from your multi-NIC configuration.</p> <p>Prerequisites:</p> <ul> <li>nftables, fail2ban, and SELinux tools are already installed by the perfsonar-toolkit bundle</li> <li>Multi-NIC configuration file at <code>/etc/perfSONAR-multi-nic-config.conf</code> (from Step 3)</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#installconfigure-additional-custom-options","title":"Install/configure additional custom options","text":"<p>You can use the install script to install the options you want (selinux, fail2ban).</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-install-nftables.sh --selinux --fail2ban --yes\n</code></pre> <pre><code>- Use `--yes` to skip the interactive confirmation prompt (omit it if you prefer to review the\n  summary and answer manually).\n\n- Add `--dry-run` for a rehearsal that only prints the planned actions.\n</code></pre> <p>The script writes nftables rules for perfSONAR services, derives SSH allow-lists from <code>/etc/perfSONAR-multi-nic- config.conf</code>, optionally adjusts SELinux, and enables Fail2ban jails\u2014only if those components are already installed.</p> SSH allow-lists and validation <ul> <li>Derives SSH allow-lists from <code>/etc/perfSONAR-multi-nic-config.conf</code> (CIDR prefixes and addresses).</li> <li>Validates nftables rules before writing.</li> <li>Outputs: rules to <code>/etc/nftables.d/perfsonar.nft</code>, log to <code>/var/log/perfSONAR-install-nftables.log</code>, backups to <code>/var/backups/</code>.</li> </ul> Preview nftables rules before applying <p>You can preview the fully rendered nftables rules (no changes are made):</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-install-nftables.sh --print-rules\n</code></pre> Manually add extra management hosts/subnets <p>If you need to allow additional SSH sources not represented by your NIC-derived prefixes, edit <code>/etc/nftables.d/perfsonar.nft</code> and add entries to the appropriate sets. Example:</p> <pre><code>set ssh_access_ip4_subnets {\n    type ipv4_addr\n    flags interval\n    elements = { 192.0.2.0/24, 198.51.100.0/25 }\n}\n\nset ssh_access_ip4_hosts {\n    type ipv4_addr\n    elements = { 203.0.113.10, 203.0.113.11 }\n}\n\nset ssh_access_ip6_subnets {\n    type ipv6_addr\n    flags interval\n    elements = { 2001:db8:1::/64 }\n}\n\nset ssh_access_ip6_hosts {\n    type ipv6_addr\n    elements = { 2001:db8::10 }\n}\n</code></pre> <p>Then validate and reload (root shell):</p> <pre><code>nft -c -f /etc/nftables.d/perfsonar.nft\nsystemctl reload nftables || systemctl restart nftables\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#confirm-nftables-state-and-security-services","title":"Confirm nftables state and security services","text":"Verification commands <pre><code>nft list ruleset\nsestatus\nsystemctl status fail2ban\n</code></pre> <p>You may want to document any site-specific exceptions (e.g., additional allowed management hosts) in your change log.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-5-start-and-configure-perfsonar-services","title":"Step 5 \u2013 Start and Configure perfSONAR Services","text":"<p>The perfSONAR Toolkit installation automatically enables and starts all required services. This step verifies service health and completes first-time web interface configuration.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-51-verify-perfsonar-services","title":"Step 5.1 \u2013 Verify perfSONAR Services","text":"<p>Check that all perfSONAR services are running:</p> <pre><code>systemctl status pscheduler-scheduler\nsystemctl status pscheduler-runner\nsystemctl status pscheduler-archiver\nsystemctl status pscheduler-ticker\nsystemctl status psconfig-pscheduler-agent\nsystemctl status owamp-server\nsystemctl status perfsonar-lsregistrationdaemon\n</code></pre> <p>All services should show <code>active (running)</code> status. If any service is not running, start it:</p> <pre><code>systemctl start &lt;service-name&gt;\n</code></pre> What each service does <ul> <li>pscheduler-scheduler: Schedules measurement tests</li> <li>pscheduler-runner: Executes scheduled tests</li> <li>pscheduler-archiver: Archives measurement results to local and remote stores</li> <li>pscheduler-ticker: Manages periodic tasks and cleanup</li> <li>psconfig-pscheduler-agent: Processes pSConfig templates and creates scheduled tests</li> <li>owamp-server: One-Way Active Measurement Protocol (latency/loss measurements)</li> <li>perfsonar-lsregistrationdaemon: Registers this host with the global Lookup Service</li> </ul> Additional services (measurement archive) <p>The toolkit also runs OpenSearch and Logstash for local measurement archive: <pre><code>systemctl status opensearch\nsystemctl status logstash\n</code></pre></p> <p>These services store measurement results locally for web UI display and historical analysis.</p> <p>All services are configured to start automatically on boot via systemd.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-52-access-the-web-interface","title":"Step 5.2 \u2013 Access the Web Interface","text":"<p>The perfSONAR Toolkit provides a comprehensive web interface for configuration and monitoring.</p> <p>Access the web UI:</p> <ol> <li> <p>Open a browser and navigate to: <code>https://&lt;your-hostname&gt;/toolkit</code></p> </li> <li> <p>First-time setup wizard:</p> <p>On first access, you'll be guided through initial configuration:</p> </li> <li> <p>Create administrator account: Set username and password for web UI access</p> </li> <li>Administrative information: Site name, location, contact details</li> <li>Host information: Verify hostname, addresses, and network interfaces</li> <li>Test configuration: Review default test settings (typically defaults are appropriate)</li> <li> <p>Archive settings: Configure local and/or remote archiving</p> </li> <li> <p>Complete the wizard to enable full functionality</p> </li> </ol> Web UI features <p>The web interface provides:</p> <ul> <li>Dashboard: Real-time and historical measurement results with graphs</li> <li>Test Configuration: Schedule on-demand or regular tests to remote endpoints</li> <li>Administrative Info: Update site information, contacts, and registration details</li> <li>Service Health: Monitor perfSONAR service status and system resources</li> <li>Archive Configuration: Manage local archive retention and remote archive destinations</li> <li>Host Details: View network interfaces, routes, and system information</li> </ul> Accessing web UI remotely <p>If you need to access the web UI from outside your local network:</p> <ul> <li>Ensure firewall allows HTTPS (port 443) from your management networks</li> <li>Consider using SSH port forwarding for secure remote access:   <pre><code>ssh -L 8443:localhost:443 root@&lt;perfsonar-host&gt;\n</code></pre>   Then access: <code>https://localhost:8443/toolkit</code></li> </ul> <p>Web UI URL: <code>https://&lt;your-hostname&gt;/toolkit</code></p> <p>For detailed web UI documentation, see: https://docs.perfsonar.net/manage_admin_info.html</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-53-configure-automatic-updates","title":"Step 5.3 \u2013 Configure Automatic Updates","text":"<p>The perfSONAR Toolkit enables automatic updates by default using <code>dnf-automatic</code>.</p> <p>Verify automatic updates are enabled:</p> <pre><code>systemctl status dnf-automatic.timer\n</code></pre> <p>The timer should show <code>active</code> and run daily to check for and install perfSONAR package updates.</p> <p>Manual update check:</p> <pre><code>dnf check-update perfsonar\\*\n</code></pre> <p>Apply updates manually (if needed):</p> <pre><code>dnf update perfsonar\\*\n\n# Restart affected services after updates\nsystemctl restart pscheduler-scheduler pscheduler-runner pscheduler-archiver pscheduler-ticker psconfig-pscheduler-agent\n</code></pre> How automatic updates work <ul> <li>dnf-automatic runs daily (configured in <code>/etc/dnf/automatic.conf</code>)</li> <li>Updates are downloaded and installed automatically</li> <li>Security updates are prioritized</li> <li>Services are restarted as needed by RPM post-install scripts</li> <li>Update logs: <code>/var/log/dnf.log</code> and <code>journalctl -u dnf-automatic</code></li> </ul> Update behavior <p>By default, the toolkit applies updates automatically. If you prefer manual control:</p> <pre><code># Disable automatic updates\nsystemctl disable dnf-automatic.timer\n\n# Re-enable later if desired\nsystemctl enable --now dnf-automatic.timer\n</code></pre> <p>Manual updates require regular monitoring to ensure security patches are applied promptly.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-6-install-and-configure-lets-encrypt-ssl-certificates-optional-but-recommended","title":"Step 6 \u2013 Install and Configure Let's Encrypt SSL Certificates (Optional but Recommended)","text":"<p>The perfSONAR Toolkit web interface uses HTTPS with self-signed certificates by default. For production deployments, replacing these with Let's Encrypt certificates provides:</p> <ul> <li>Browser trust: No certificate warnings when accessing the web UI</li> <li>Security: Industry-standard encryption with automatic renewals</li> <li>Compliance: Meets security requirements for production infrastructure</li> </ul> <p>This step is optional but highly recommended for production sites.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-61-prerequisites-for-lets-encrypt","title":"Step 6.1 \u2013 Prerequisites for Let's Encrypt","text":"<p>Before obtaining Let's Encrypt certificates, ensure:</p> <ol> <li>DNS is configured correctly:</li> </ol> <p>Your hostname must have valid forward (A/AAAA) and reverse (PTR) DNS records that are publicly resolvable.    Verify this with the DNS checker:</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/check-perfsonar-dns.sh\n</code></pre> <ol> <li>Port 80 is accessible:</li> </ol> <p>Let's Encrypt uses HTTP-01 challenge which requires port 80 to be open from the internet. Update your firewall:</p> <pre><code># Add HTTP port to nftables (in addition to existing HTTPS)\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-install-nftables.sh --ports=80,443 --yes\n</code></pre> <ol> <li>Apache is not listening on port 80:</li> </ol> <p>The perfSONAR Toolkit Apache server should only listen on port 443 (HTTPS). Verify this:</p> <pre><code># Check Apache is only listening on 443, not 80\nss -tlnp | grep :80\nss -tlnp | grep :443\n\n# Should show port 443 with httpd, but port 80 should be free\n</code></pre> If Apache is listening on port 80 <p>The Toolkit's Apache configuration should not bind to port 80. If it is, check <code>/etc/httpd/conf/httpd.conf</code> and <code>/etc/httpd/conf.d/*.conf</code> for <code>Listen 80</code> directives and comment them out:</p> <pre><code># Find Listen directives\ngrep -r \"^Listen 80\" /etc/httpd/\n\n# Edit the file(s) and comment out or change to Listen 443\nvi /etc/httpd/conf/httpd.conf\n\n# Restart Apache\nsystemctl restart httpd\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-62-install-certbot","title":"Step 6.2 \u2013 Install Certbot","text":"<p>Install certbot using snapd (recommended method) or EPEL packages:</p> <p>Option A: Install via Snap (Recommended)</p> <pre><code># Install snapd\ndnf install -y snapd\nsystemctl enable --now snapd.socket\n\n# Wait for snapd to initialize\nsleep 10\n\n# Create symlink for classic snap support\nln -sf /var/lib/snapd/snap /snap\n\n# Install certbot\nsnap install --classic certbot\n\n# Create symlink for certbot command\nln -sf /snap/bin/certbot /usr/bin/certbot\n</code></pre> <p>Option B: Install via DNF (Alternative)</p> <pre><code># Install certbot from EPEL\ndnf install -y certbot\n\n# Verify installation\ncertbot --version\n</code></pre> Why use snap for certbot? <p>The Certbot developers recommend snap installation because: - Always provides the latest certbot version - Automatic updates via snap refresh - Consistent across distributions - Includes all necessary dependencies</p> <p>EPEL packages work but may lag behind upstream releases.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-63-obtain-lets-encrypt-certificate","title":"Step 6.3 \u2013 Obtain Let's Encrypt Certificate","text":"<p>Use certbot in standalone mode to obtain your certificate. Replace <code>&lt;your-fqdn&gt;</code> with your host's fully-qualified domain name and <code>&lt;admin-email&gt;</code> with your email address (used for renewal notifications).</p> <p>Obtain certificate (interactive):</p> <pre><code># Stop Apache temporarily to free port 80\nsystemctl stop httpd\n\n# Obtain certificate using standalone mode\ncertbot certonly --standalone \\\n    -d &lt;your-fqdn&gt; \\\n    -m &lt;admin-email&gt; \\\n    --agree-tos\n\n# Restart Apache\nsystemctl start httpd\n</code></pre> <p>Example:</p> <pre><code>certbot certonly --standalone \\\n    -d ps-toolkit.example.org \\\n    -m psadmin@example.org \\\n    --agree-tos\n</code></pre> <p>Non-interactive (for automation):</p> <pre><code>systemctl stop httpd\n\ncertbot certonly --standalone \\\n    -d &lt;your-fqdn&gt; \\\n    -m &lt;admin-email&gt; \\\n    --agree-tos \\\n    --non-interactive\n\nsystemctl start httpd\n</code></pre> Certificate file locations <p>After successful issuance, certificates are stored at:</p> <ul> <li>Full chain: <code>/etc/letsencrypt/live/&lt;your-fqdn&gt;/fullchain.pem</code></li> <li>Private key: <code>/etc/letsencrypt/live/&lt;your-fqdn&gt;/privkey.pem</code></li> <li>Chain only: <code>/etc/letsencrypt/live/&lt;your-fqdn&gt;/chain.pem</code></li> <li>Certificate only: <code>/etc/letsencrypt/live/&lt;your-fqdn&gt;/cert.pem</code></li> </ul> <p>The actual certificate files are in <code>/etc/letsencrypt/archive/&lt;your-fqdn&gt;/</code> and the <code>live/</code> directory contains symlinks to the latest versions.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-64-configure-apache-to-use-lets-encrypt-certificate","title":"Step 6.4 \u2013 Configure Apache to Use Let's Encrypt Certificate","text":"<p>Use the helper script to update Apache SSL configuration:</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/configure-toolkit-letsencrypt.sh &lt;your-fqdn&gt;\n</code></pre> <p>Example:</p> <pre><code>/opt/perfsonar-toolkit/tools_scripts/configure-toolkit-letsencrypt.sh ps-toolkit.example.org\n</code></pre> <p>This script:</p> <ul> <li>Backs up the original Apache SSL configuration</li> <li>Updates <code>SSLCertificateFile</code> to point to Let's Encrypt fullchain</li> <li>Updates <code>SSLCertificateKeyFile</code> to point to Let's Encrypt private key</li> <li>Adds or updates <code>SSLCertificateChainFile</code></li> </ul> <p>Verify Apache configuration syntax:</p> <pre><code>apachectl configtest\n</code></pre> <p>Reload Apache to apply changes:</p> <pre><code>systemctl reload httpd\n</code></pre> <p>Verify the certificate is in use:</p> <pre><code># Check certificate via OpenSSL\necho | openssl s_client -connect &lt;your-fqdn&gt;:443 -servername &lt;your-fqdn&gt; 2&gt;/dev/null | openssl x509 -noout -issuer -dates\n\n# Should show:\n# issuer=C=US, O=Let's Encrypt, CN=...\n# notBefore=...\n# notAfter=...\n</code></pre> <p>Test in browser:</p> <p>Navigate to <code>https://&lt;your-fqdn&gt;/toolkit</code> and verify:</p> <ul> <li>No certificate warnings</li> <li>Certificate is issued by \"Let's Encrypt\"</li> <li>Certificate is valid (green padlock icon)</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-65-configure-automatic-certificate-renewal","title":"Step 6.5 \u2013 Configure Automatic Certificate Renewal","text":"<p>Let's Encrypt certificates expire after 90 days. Configure automatic renewal to avoid expiration.</p> <p>Test renewal process (dry run):</p> <pre><code># Perform a test renewal without actually renewing\ncertbot renew --dry-run --pre-hook \"systemctl stop httpd\" --post-hook \"systemctl start httpd\"\n</code></pre> <p>If the dry run succeeds, configure automatic renewal:</p> <p>Option A: Using Certbot Timer (Recommended)</p> <p>Certbot automatically installs a systemd timer for renewals when installed via snap:</p> <pre><code># Check timer status\nsystemctl list-timers | grep certbot\n\n# If not present, enable it\nsystemctl enable --now snap.certbot.renew.timer\n\n# Or for EPEL installation:\nsystemctl enable --now certbot-renew.timer\n</code></pre> <p>Option B: Using Cron (Alternative)</p> <p>Add a cron job for automatic renewal:</p> <pre><code># Create renewal script\ncat &gt; /usr/local/bin/certbot-renew.sh &lt;&lt; 'EOF'\n#!/bin/bash\n# Renew Let's Encrypt certificates and reload Apache\n\ncertbot renew \\\n    --pre-hook \"systemctl stop httpd\" \\\n    --post-hook \"systemctl start httpd\" \\\n    --quiet\n\nexit 0\nEOF\n\nchmod 0755 /usr/local/bin/certbot-renew.sh\n\n# Add cron job (runs twice daily at 3:30 AM and 3:30 PM)\ncat &gt; /etc/cron.d/certbot-renew &lt;&lt; 'EOF'\n30 3,15 * * * root /usr/local/bin/certbot-renew.sh\nEOF\n</code></pre> Renewal frequency and timing <ul> <li>Certbot automatically checks certificates and only renews those expiring within 30 days</li> <li>Running renewal checks twice daily ensures timely renewal even if one attempt fails</li> <li>The <code>--quiet</code> flag suppresses output unless there's an error</li> <li>Pre/post hooks stop and start Apache to free port 80 for the standalone authenticator</li> </ul> <p>Verify automatic renewal is configured:</p> <pre><code># For snap installation\nsystemctl status snap.certbot.renew.timer\n\n# For EPEL installation\nsystemctl status certbot-renew.timer\n\n# For cron-based renewal\ncrontab -l | grep certbot\n# or\ncat /etc/cron.d/certbot-renew\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-66-monitor-certificate-expiration","title":"Step 6.6 \u2013 Monitor Certificate Expiration","text":"<p>Even with automatic renewal, monitor certificate expiration to catch renewal failures:</p> <p>Check certificate expiration date:</p> <pre><code># Check all certificates\ncertbot certificates\n\n# Check specific certificate via OpenSSL\necho | openssl s_client -connect &lt;your-fqdn&gt;:443 -servername &lt;your-fqdn&gt; 2&gt;/dev/null | openssl x509 -noout -dates\n</code></pre> <p>Set up expiration monitoring (optional):</p> Email alerts for expiration <p>Let's Encrypt sends expiration warning emails to the address provided during certificate issuance. Ensure this email address is monitored:</p> <pre><code># Check configured email\ngrep email /etc/letsencrypt/renewal/&lt;your-fqdn&gt;.conf\n</code></pre> <p>You can also set up local monitoring using nagios, icinga, or a simple script:</p> <pre><code>#!/bin/bash\n# check-cert-expiry.sh - Alert if certificate expires within 14 days\n\nDOMAIN=\"&lt;your-fqdn&gt;\"\nWARN_DAYS=14\n\nEXPIRY=$(echo | openssl s_client -connect ${DOMAIN}:443 -servername ${DOMAIN} 2&gt;/dev/null | \\\n         openssl x509 -noout -enddate | cut -d= -f2)\n\nEXPIRY_EPOCH=$(date -d \"${EXPIRY}\" +%s)\nNOW_EPOCH=$(date +%s)\nDAYS_LEFT=$(( (EXPIRY_EPOCH - NOW_EPOCH) / 86400 ))\n\nif [ $DAYS_LEFT -lt $WARN_DAYS ]; then\n    echo \"WARNING: Certificate for ${DOMAIN} expires in ${DAYS_LEFT} days!\"\n    # Send alert via email, Slack, etc.\nelse\n    echo \"OK: Certificate valid for ${DAYS_LEFT} more days\"\nfi\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#troubleshooting-lets-encrypt","title":"Troubleshooting Let's Encrypt","text":"Certificate issuance fails with 'Connection refused' <p>Symptoms: Certbot fails with \"Failed to authenticate\" or \"Connection refused\" errors during HTTP-01 challenge.</p> <p>Diagnostic steps:</p> <pre><code># Verify port 80 is open in firewall\nnft list ruleset | grep \"dport 80\"\n\n# Test port 80 accessibility from external host\ncurl -v http://&lt;your-fqdn&gt;/\n\n# Check nothing is listening on port 80\nss -tlnp | grep :80\n</code></pre> <p>Solutions:</p> <ul> <li>Add port 80 to nftables: <code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-install-nftables.sh --ports=80,443 --yes</code></li> <li>Ensure Apache is not listening on port 80 (should only listen on 443)</li> <li>Verify DNS resolves correctly from public internet</li> <li>Check network firewall/router allows inbound port 80</li> </ul> Certificate renewal fails <p>Symptoms: Certificate expires or renewal fails with errors in logs.</p> <p>Diagnostic steps:</p> <pre><code># Check renewal logs\njournalctl -u snap.certbot.renew.timer -n 50\n# or\ngrep certbot /var/log/syslog | tail -50\n\n# Test renewal manually\ncertbot renew --dry-run --pre-hook \"systemctl stop httpd\" --post-hook \"systemctl start httpd\" -vvv\n\n# Check certificate status\ncertbot certificates\n</code></pre> <p>Common causes:</p> <ul> <li>Port 80 blocked: Verify firewall allows HTTP during renewal</li> <li>Apache failed to stop/start: Check Apache service status</li> <li>DNS changes: Verify hostname still resolves correctly</li> <li>Rate limiting: Let's Encrypt has rate limits (5 renewals per 7 days per domain)</li> </ul> <p>Solutions:</p> <ul> <li>Fix firewall or DNS issues</li> <li>Manually renew: <code>certbot renew --force-renewal</code></li> <li>If rate limited, wait 7 days before retrying</li> </ul> Browser shows old certificate after renewal <p>Symptoms: Certificate renewed successfully but browser still shows old/expired certificate.</p> <p>Diagnostic steps:</p> <pre><code># Check certificate files are updated\nls -la /etc/letsencrypt/live/&lt;your-fqdn&gt;/\n\n# Verify Apache configuration points to correct files\ngrep SSLCertificate /etc/httpd/conf.d/ssl.conf\n\n# Check Apache loaded the new certificate\nsystemctl status httpd\n</code></pre> <p>Solutions:</p> <ul> <li>Reload or restart Apache: <code>systemctl restart httpd</code></li> <li>Clear browser cache and hard refresh (Ctrl+Shift+R)</li> <li>Verify SSL configuration: <code>apachectl -t -D DUMP_VHOSTS</code></li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-7-configure-and-enroll-in-psconfig","title":"Step 7 \u2013 Configure and Enroll in pSConfig","text":"<p>Enroll your toolkit host with the OSG/WLCG pSConfig service so tests are auto-configured. Use the \"auto URL\" for each FQDN you expose for perfSONAR (one or two depending on whether you split latency/throughput by hostname).</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#option-a-web-ui-configuration-recommended","title":"Option A: Web UI Configuration (Recommended)","text":"<p>The easiest way to configure pSConfig is via the web interface:</p> <ol> <li>Navigate to: <code>https://&lt;your-hostname&gt;/toolkit/admin?view=psconfig</code></li> <li>Click \"Add Remote Configuration\"</li> <li>Enter the auto URL: <code>https://psconfig.opensciencegrid.org/pub/auto/&lt;your-fqdn&gt;</code></li> <li>Enable \"Configure Archives\" to automatically set up result archiving</li> <li>Save and restart the pSConfig agent</li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#option-b-command-line-configuration","title":"Option B: Command Line Configuration","text":"<p>Basic enrollment via command line:</p> <pre><code># Add auto URLs (configures archives too) and show configured remotes\npsconfig remote --configure-archives add \\\n    \"https://psconfig.opensciencegrid.org/pub/auto/ps-lat-example.my.edu\"\n\npsconfig remote list\n</code></pre> <p>If there are any stale/old/incorrect entries, you can remove them:</p> <pre><code>psconfig remote delete \"&lt;old-url&gt;\"\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#option-c-automated-enrollment-script","title":"Option C: Automated Enrollment Script","text":"<p>Automation tip: derive FQDNs from your configured IPs (PTR lookup) and enroll automatically. Review the list before applying.</p> <p>For RPM Toolkit installs (non-container):</p> <pre><code># Dry run only (show planned URLs):\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-auto-enroll-psconfig.sh --local -n\n\n# Apply enrollment:\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-auto-enroll-psconfig.sh --local -v\n\n# Verify configured remotes\npsconfig remote list\n</code></pre> <p>For container-based installs:</p> <pre><code># Dry run only (show planned URLs):\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-auto-enroll-psconfig.sh -n\n\n# Apply enrollment:\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-auto-enroll-psconfig.sh -v\n\n# Verify configured remotes\npodman exec -it perfsonar-testpoint psconfig remote list\n</code></pre> The auto enroll psconfig script details <ul> <li>Parses IP lists from <code>/etc/perfSONAR-multi-nic-config.conf</code>  (<code>NIC_IPV4_ADDRS</code> / <code>NIC_IPV6_ADDRS</code>).</li> <li>Performs reverse DNS lookups (getent/dig) to derive FQDNs.</li> <li>Deduplicates while preserving discovery order.</li> <li>Adds each <code>https://psconfig.opensciencegrid.org/pub/auto/&lt;FQDN&gt;</code> with <code>--configure-archives</code>.</li> <li>Lists configured remotes and returns non-zero if any enrollment fails.</li> </ul> <p>Integrate into provisioning CI by running with <code>-n</code> (dry-run) for approval and then <code>-y</code> once approved.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-8-register-and-configure-with-wlcgosg","title":"Step 8 \u2013 Register and Configure with WLCG/OSG","text":"<ol> <li> <p>OSG/WLCG registration workflow:</p> Registration steps and portals <ul> <li>Register the host in OSG topology.</li> <li>Create or update a GGUS ticket announcing the new measurement point.</li> <li>In GOCDB, add the service endpoint         <code>org.opensciencegrid.crc.perfsonar-testpoint</code> bound to this host.</li> </ul> </li> <li> <p>Document memberships:</p> <p>Update your site wiki or change log with assigned mesh names, feed  URLs, and support contacts.</p> </li> <li> <p>Update Lookup Service registration:</p> <p>Option A: Web UI (Recommended)</p> <p>The easiest way to configure registration information is via the Toolkit web interface:</p> <ol> <li>Navigate to: <code>https://&lt;your-hostname&gt;/toolkit/admin?view=host</code></li> <li>Fill in administrative information:<ul> <li>Site name, organization, location (city, state, country, zip code)</li> <li>Latitude and longitude (for map display)</li> <li>Administrator name and email</li> <li>Projects (WLCG, OSG, etc.)</li> </ul> </li> <li>Save changes - the lsregistrationdaemon restarts automatically</li> </ol> <p>Option B: Command Line</p> <p>Edit <code>/etc/perfsonar/lsregistrationdaemon.conf</code> directly and restart the service:</p> <pre><code>vi /etc/perfsonar/lsregistrationdaemon.conf\n\n# After editing, restart the registration daemon\nsystemctl restart perfsonar-lsregistrationdaemon\n</code></pre> <p>Option C: Helper Script</p> <p>Use the helper script to update registration. For RPM Toolkit installs, use the <code>--local</code> flag:</p> <p>For RPM Toolkit installs (non-container): <pre><code># Preview changes only\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh update --local \\\n    --dry-run --site-name \"Acme Co.\" --project WLCG \\\n    --admin-email admin@example.org --admin-name \"pS Admin\"\n\n# Apply new settings and restart the daemon\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh create --local \\\n    --site-name \"Acme Co.\" --domain example.org --project WLCG --project OSG \\\n    --city Berkeley --region CA --country US --zip 94720 \\\n    --latitude 37.5 --longitude -121.7469 \\\n    --admin-name \"pS Admin\" --admin-email admin@example.org\n\n# Save current config (raw conf file)\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh save --output my-lsreg.conf --local\n\n# Or produce a self-contained executable restore script\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh extract --output /root/restore-lsreg.sh --local\n</code></pre></p> </li> <li> <p>Automatic updates</p> <p>The perfSONAR Toolkit uses <code>dnf-automatic</code> for automatic updates (already configured in Step 5).</p> </li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-9-selinux-troubleshooting-if-enabled","title":"Step 9 \u2013 SELinux Troubleshooting (If Enabled)","text":"<p>If you've enabled SELinux in enforcing mode, certain perfSONAR operations may generate audit log alerts. This section explains common issues and their fixes.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#selinux-basics-for-perfsonar","title":"SELinux Basics for perfSONAR","text":"<p>SELinux enforces mandatory access controls based on file labels and process contexts. perfSONAR services run under specific contexts (e.g., <code>lsregistrationdaemon_t</code>, <code>httpd_t</code>), and accessed files must have compatible labels.</p> <p>Check SELinux status:</p> <pre><code>sestatus\n# Expected output: \"SELinux status:  enabled\" and \"Current mode:  enforcing\"\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#common-selinux-issues-and-fixes","title":"Common SELinux Issues and Fixes","text":""},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#issue-1-etcperfsonarlsregistrationdaemonconf-has-wrong-label","title":"Issue 1: <code>/etc/perfsonar/lsregistrationdaemon.conf</code> Has Wrong Label","text":"<p>Symptom: Audit log shows: <pre><code>SELinux is preventing /usr/bin/perl from getattr access on the file /etc/perfsonar/lsregistrationdaemon.conf.\n</code></pre></p> <p>Root cause: The configuration file was created or modified (e.g., via restore or manual edit) and has an incorrect SELinux label. The file should be labeled <code>lsregistrationdaemon_etc_t</code> but may be labeled <code>admin_home_t</code> or have no label.</p> <p>Fix: Apply <code>restorecon</code> to relabel the file:</p> <pre><code># Restore the default SELinux context for the file\nsudo /sbin/restorecon -v /etc/perfsonar/lsregistrationdaemon.conf\n\n# Verify the label is now correct\nls -Z /etc/perfsonar/lsregistrationdaemon.conf\n# Expected: system_u:object_r:lsregistrationdaemon_etc_t:s0\n</code></pre> <p>Automatic fix during restore:</p> <p>Our <code>perfSONAR-update-lsregistration.sh</code> helper attempts to automatically apply <code>restorecon</code> after writing the configuration file. If <code>restorecon</code> is available on your system, it runs without user intervention:</p> <pre><code># Use the helper to restore config (with automatic restorecon attempt)\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh restore --local \\\n    --input ./my-lsreg.conf\n\n# Or extract and run a self-contained restore script\n/opt/perfsonar-toolkit/tools_scripts/perfSONAR-update-lsregistration.sh extract --local \\\n    --output ./restore-lsreg.sh\n./restore-lsreg.sh  # This script includes a restorecon attempt\n</code></pre> <p>Preventing the issue:</p> <ul> <li>Always use the helper script (<code>perfSONAR-update-lsregistration.sh</code>) for configuration changes, as it handles <code>restorecon</code> automatically.</li> <li>After any manual edits to <code>/etc/perfsonar/lsregistrationdaemon.conf</code>, explicitly run <code>restorecon</code>:   <pre><code>sudo vi /etc/perfsonar/lsregistrationdaemon.conf\nsudo /sbin/restorecon -v /etc/perfsonar/lsregistrationdaemon.conf  # Fix labels immediately\nsudo systemctl restart perfsonar-lsregistrationdaemon\n</code></pre></li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#issue-2-other-services-ethtool-df-python3-postgresql-collect2-generating-audit-alerts","title":"Issue 2: Other Services (ethtool, df, python3, postgresql, collect2) Generating Audit Alerts","text":"<p>Symptoms: Audit log shows alerts for various tools running in unexpected SELinux contexts: <pre><code>SELinux is preventing /usr/sbin/ethtool from setopt access on netlink_generic_socket labeled httpd_t.\nSELinux is preventing /usr/bin/df from getattr access on the directory /var/cache/openafs.\nSELinux is preventing /usr/bin/python3.9 from execute access on the file ldconfig.\nSELinux is preventing /usr/libexec/gcc/x86_64-redhat-linux/11/collect2 from search access on the directory snapd.\n</code></pre></p> <p>Root cause: These alerts typically stem from: - Tools invoked from web interfaces or services running in different SELinux contexts (e.g., <code>httpd_t</code>, <code>postgresql_t</code>) - Third-party or system utilities that lack complete SELinux policy coverage - Legitimate operations conflicting with default policy rules - Build/compilation tools invoked during package installation (usually transient)</p> <p>Assessment and diagnosis:</p> <ol> <li>Check if the alert is related to perfSONAR functionality:</li> </ol> <pre><code># View recent audit alerts\ntail -100 /var/log/audit/audit.log\n\n# Filter by command name to see context\ngrep \"ethtool\\|df\\|python\\|collect2\\|ldconfig\" /var/log/audit/audit.log | head -20\n\n# Count alert types to identify patterns\nausearch -m AVC | awk -F'avc:' '{print $2}' | sort | uniq -c | sort -rn | head -10\n</code></pre> <ol> <li> <p>Determine the source process and context:</p> </li> <li> <p>Alerts mentioning <code>httpd_t</code> usually indicate the web UI triggered the operation (typically safe to allow)</p> </li> <li>Alerts from <code>postgresql_t</code> indicate database tools being invoked (context boundary may not be required)</li> <li>Alerts from <code>lsregistrationdaemon_t</code> indicate the registration daemon needs access (fix labels first, not policies)</li> <li> <p>Alerts from <code>gcc/collect2</code> during package install are usually transient (monitor periodically)</p> </li> <li> <p>Create a local SELinux policy module (if operation is verified as safe)</p> </li> </ol> <pre><code># Generate policy module for a specific alert (example: ethtool)\nsudo ausearch -c 'ethtool' --raw | audit2allow -M my-ethtool\n\n# Review the generated module to ensure it's safe\ncat my-ethtool.te\n\n# Install the module (if approved and safe)\nsudo semodule -i my-ethtool.pp\n\n# Verify installation\nsemodule -l | grep my-ethtool\n</code></pre> <p>Specific service fixes:</p> <p>ethtool netlink access (from httpd_t or lsregistrationdaemon_t):    - Operation: Checking NIC link status, speed, duplex (safe)    - Source: Web UI health checks or daemon monitoring    - Fix: <pre><code>sudo ausearch -c 'ethtool' --raw | audit2allow -M my-ethtool\nsudo semodule -i my-ethtool.pp\n</code></pre></p> <p>df/stat on /var/cache/openafs (from lsregistrationdaemon_t):    - Operation: Checking available disk space (safe)    - Source: Registration daemon system health queries    - Fix: <pre><code>sudo ausearch -c 'df' --raw | audit2allow -M my-df\nsudo semodule -i my-df.pp\n</code></pre></p> <p>python3/postgresql context issues (collect2, ldconfig):    - Operation: Build tools, library checks during package installation (usually transient)    - Assessment: These are typically safe but may be ephemeral    - Fix (if persistent): <pre><code># For postgresql-related alerts\nsudo ausearch -c 'validate-config' --raw | audit2allow -M my-postgresql\nsudo semodule -i my-postgresql.pp\n</code></pre></p> <p>Audit log monitoring (prevents future surprises):</p> <pre><code># Check for recent AVC denials\nsudo ausearch -m AVC -ts recent | tail -50\n\n# Create a daily monitoring script\ncat &gt; /usr/local/bin/check-selinux-alerts.sh &lt;&lt; 'EOF'\n#!/bin/bash\n# Check for recent SELinux audit alerts\n\nRECENT_ALERTS=$(ausearch -m AVC -ts recent 2&gt;/dev/null | wc -l)\n\nif [ $RECENT_ALERTS -gt 0 ]; then\n    echo \"WARNING: Found $RECENT_ALERTS recent SELinux alerts:\"\n    ausearch -m AVC -ts recent | tail -20\nelse\n    echo \"OK: No recent SELinux audit alerts\"\nfi\nEOF\n\nchmod 0755 /usr/local/bin/check-selinux-alerts.sh\n\n# Add to cron (runs daily at 9 AM)\necho \"0 9 * * * root /usr/local/bin/check-selinux-alerts.sh\" | sudo tee /etc/cron.d/selinux-alert-check\n</code></pre> <p>Best practice for handling alerts:</p> <ol> <li>Log all alerts for 1-2 weeks to establish a baseline</li> <li>Review and categorize (safe vs. unsafe operations)</li> <li>Create local policy modules only for verified, safe operations</li> <li>Document each module in your change log</li> <li>Monitor weekly for new or unexpected alerts</li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#issue-3-audit-log-flooding","title":"Issue 3: Audit Log Flooding","text":"<p>Symptom: Audit log grows very large due to repeated identical alerts.</p> <p>Mitigation:</p> <pre><code># View count of each AVC alert type\nausearch -m AVC | awk -F'avc:' '{print $2}' | sort | uniq -c | sort -rn | head -20\n\n# Suppress specific alerts (if they are verified as safe):\n# Add rules to /etc/audit/audit.rules or /etc/audit/rules.d/\n# (requires audit service restart and SELinux expertise)\n</code></pre>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#best-practices-for-selinux-with-perfsonar","title":"Best Practices for SELinux with perfSONAR","text":"<ol> <li> <p>Use automated tools: Always use the helper scripts (<code>perfSONAR-update-lsregistration.sh</code>, <code>perfSONAR-install-nftables.sh</code>) which handle SELinux contexts automatically.</p> </li> <li> <p>Run <code>restorecon</code> after manual edits: If you manually edit any perfSONAR configuration file, immediately restore the SELinux context:    <pre><code>sudo /sbin/restorecon -v /path/to/file\n</code></pre></p> </li> <li> <p>Monitor audit logs regularly: Check <code>/var/log/audit/audit.log</code> weekly to catch new issues early.</p> </li> <li> <p>Document exceptions: If you create local SELinux policy modules, document them in your change log so future admins understand why they exist.</p> </li> <li> <p>Keep policies minimal: Only add local policy modules for operations that are verified as safe and necessary. Overly permissive policies increase security risk.</p> </li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#step-10-post-install-validation","title":"Step 10 \u2013 Post-Install Validation","text":"<p>Perform these checks before handing the host over to operations:</p> <ol> <li> <p>System services:</p> Verify perfSONAR services <pre><code># Check all perfSONAR services are running\nsystemctl status pscheduler-scheduler pscheduler-runner pscheduler-archiver pscheduler-ticker\nsystemctl status psconfig-pscheduler-agent owamp-server perfsonar-lsregistrationdaemon\n\n# Check web server (Apache)\nsystemctl status apache2 --no-pager\n\n# Check measurement archive services\nsystemctl status opensearch logstash\n</code></pre> <p>Ensure all services show <code>active (running)</code> status.</p> </li> <li> <p>Web interface access:</p> Verify web UI is accessible <pre><code># Test HTTPS connectivity to web UI\ncurl -k -s -o /dev/null -w \"%{http_code}\" https://localhost/toolkit\n# Should return 200\n\n# Check Apache error logs if issues\njournalctl -u apache2 -n 50\n</code></pre> <p>Access the web UI in a browser: <code>https://&lt;your-hostname&gt;/toolkit</code></p> <p>Verify the dashboard loads and shows measurement results (may take a few minutes after first tests run).</p> </li> <li> <p>Service logs:</p> Check perfSONAR service logs for errors <pre><code># Check pScheduler logs for errors\njournalctl -u pscheduler-scheduler -n 50 --no-pager\njournalctl -u pscheduler-runner -n 50 --no-pager\n\n# Check pSConfig agent logs\njournalctl -u psconfig-pscheduler-agent -n 50 --no-pager\n\n# Check registration daemon\njournalctl -u perfsonar-lsregistrationdaemon -n 20 --no-pager\n</code></pre> </li> <li> <p>Network path validation:</p> Test network connectivity and routing <p>Test throughput to a remote endpoint: <pre><code>pscheduler task throughput --dest &lt;remote-testpoint&gt;\n</code></pre></p> <p>Check routing from the host: <pre><code>tracepath -n &lt;remote-testpoint&gt;\nip route get &lt;remote-testpoint-ip&gt;\n</code></pre></p> <p>Confirm traffic uses the intended policy-based routes (check <code>ip route get &lt;dest&gt;</code>).</p> </li> <li> <p>Security posture:</p> Check firewall, fail2ban, and SELinux <pre><code># Check nftables firewall rules\nnft list ruleset | grep perfsonar\n\n# Check fail2ban status (automatically installed by toolkit)\nsystemctl status fail2ban\nfail2ban-client status\n\n# Check for recent SELinux denials\nif command -v ausearch &gt;/dev/null 2&gt;&amp;1; then\n    ausearch --message AVC --just-one\nelif [ -f /var/log/audit/audit.log ]; then\n    grep -i \"avc.*denied\" /var/log/audit/audit.log | tail -5\nelse\n    echo \"SELinux audit tools not available\"\nfi\n</code></pre> <p>Investigate any SELinux denials or repeated Fail2Ban bans.</p> </li> <li> <p>Certificate check (if using HTTPS):</p> Verify certificate validity <pre><code># Check certificate via HTTPS connection\necho | openssl s_client -connect &lt;your-hostname&gt;:443 -servername &lt;your-hostname&gt; 2&gt;/dev/null | openssl x509 -noout -dates -issuer\n</code></pre> <p>Ensure the certificate is valid and not expired.</p> </li> <li> <p>Measurement archive:</p> Verify local archive is collecting data <p>Check that OpenSearch is receiving measurement results:</p> <pre><code># Check OpenSearch cluster health\ncurl -k https://localhost:9200/_cluster/health?pretty\n\n# Check for measurement data (after tests have run)\ncurl -k https://localhost:9200/_cat/indices?v | grep pscheduler\n</code></pre> <p>Via web UI: Navigate to <code>https://&lt;your-hostname&gt;/toolkit/archive</code> to view stored measurements.</p> </li> <li> <p>Reporting:</p> Run perfSONAR diagnostic reports <p>Run the perfSONAR troubleshoot command and send outputs to operations: <pre><code>pscheduler troubleshoot\n</code></pre></p> </li> </ol>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#ongoing-maintenance","title":"Ongoing Maintenance","text":"<ul> <li> <p>Quarterly or as-needed: Re-validate routing policy and nftables rules after network changes or security audits.</p> </li> <li> <p>Monthly or during maintenance windows: Apply OS updates (<code>dnf update</code>) and reboot during scheduled downtime.</p> </li> <li> <p>Monitor psconfig feeds for changes in mesh participation and test configuration.</p> </li> <li> <p>Track certificate expiry with <code>certbot renew --dry-run</code> if you rely on Let's Encrypt (automatic renewal is configured but monitoring is recommended).</p> </li> <li> <p>Review container logs periodically for errors: <code>podman logs perfsonar-testpoint</code> and <code>podman logs certbot</code>.</p> </li> <li> <p>Verify auto-update timer is active: <code>systemctl list-timers perfsonar-auto-update.timer</code>.</p> </li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#troubleshooting","title":"Troubleshooting","text":""},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#networking-issues","title":"Networking Issues","text":"Policy-based routing not working correctly <p>Symptoms: Traffic not using expected interfaces, routing to wrong gateway.</p> <p>Diagnostic steps:</p> <pre><code># Check routing rules\nip rule show\n\n# Check routing tables\nip route show table all\n\n\n# Test specific route lookup\nip route get &lt;destination-ip&gt;\n\n# Check NetworkManager connections\nnmcli connection show\n\n# Review PBR script log\ntail -100 /var/log/perfSONAR-multi-nic-config.log\n</code></pre> <p>Solutions:</p> <ul> <li>Verify <code>/etc/perfSONAR-multi-nic-config.conf</code> has correct IPs and gateways</li> <li>Reapply configuration: <code>/opt/perfsonar-toolkit/tools_scripts/perfSONAR-pbr-nm.sh --yes</code></li> <li>Reboot if rules are not being applied correctly</li> <li>Check for conflicting NetworkManager or systemd-networkd rules</li> </ul> DNS resolution failing for test endpoints <p>Symptoms: perfSONAR tests fail with \"unknown host\" or DNS errors.</p> <p>Diagnostic steps:</p> <pre><code># Test DNS resolution from container\npodman exec -it perfsonar-testpoint dig &lt;remote-testpoint&gt;\n\n# Check container's resolv.conf\n\npodman exec -it perfsonar-testpoint cat /etc/resolv.conf\n\n# Verify forward and reverse DNS\n/opt/perfsonar-toolkit/tools_scripts/check-perfsonar-dns.sh\n</code></pre> <p>Solutions:</p> <ul> <li>Ensure DNS servers are correctly configured on host</li> <li>Fix missing PTR records in DNS zones</li> <li>Verify forward A/AAAA records match reverse PTR records</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#certificate-issues","title":"Certificate Issues","text":"Let's Encrypt certificate issuance fails <p>Symptoms: Certbot fails with \"Failed to authenticate\" or \"Connection refused\" errors.</p> <p>Diagnostic steps:</p> <pre><code># Check if port 80 is open\nnft list ruleset | grep \"80\"\n\n# Verify Apache is NOT listening on port 80 in container\npodman exec perfsonar-testpoint netstat -tlnp | grep :80\n\n# Test port 80 accessibility from external host\ncurl -v http://&lt;your-fqdn&gt;/\n\n# Run certbot in verbose mode\n\npodman run --rm --net=host \\\n    -v /etc/letsencrypt:/etc/letsencrypt:Z \\\n    -v /var/www/html:/var/www/html:Z \\\n    docker.io/certbot/certbot:latest certonly \\\n    --standalone -d &lt;SERVER_FQDN&gt; -m &lt;EMAIL&gt; --dry-run -vvv\n</code></pre> <p>Common causes:</p> <ul> <li>Port 80 blocked by firewall: Add with <code>perfSONAR-install-nftables.sh --ports=80,443</code></li> <li>Apache listening on port 80: Verify testpoint-entrypoint-wrapper.sh patched Apache correctly</li> <li>DNS not propagated: Wait for DNS changes to propagate globally</li> <li>Rate limiting: Let's Encrypt has rate limits; wait if you've hit them</li> </ul> Certificate not loaded after renewal <p>Symptoms: Old certificate still in use after automatic renewal.</p> <p>Diagnostic steps:</p> <pre><code># Check certificate files\nls -la /etc/letsencrypt/live/&lt;fqdn&gt;/\n\n# Verify deploy hook is configured\npodman logs certbot 2&gt;&amp;1 | grep \"deploy hook\"\n\n# Check if container restarted\npodman ps --format 'table {{.Names}}\\t{{.Status}}'\n\n# Manually restart testpoint\npodman restart perfsonar-testpoint\n</code></pre> <p>Solutions:</p> <ul> <li>Verify deploy hook script exists and is executable: <code>/opt/perfsonar-toolkit/tools_scripts/certbot-deploy-hook.sh</code></li> <li>Ensure deploy hook is mounted in container at: <code>/etc/letsencrypt/renewal-hooks/deploy/certbot-deploy-hook.sh</code></li> <li>Verify Podman socket is mounted in certbot container: <code>/run/podman/podman.sock</code></li> <li>Check deploy hook logs: <code>journalctl -u perfsonar-certbot.service | grep deploy</code></li> <li>Manually restart testpoint after renewals if deploy hook fails: <code>podman restart perfsonar-testpoint</code></li> </ul> <p>Note: Certbot automatically executes scripts in <code>/etc/letsencrypt/renewal-hooks/deploy/</code> when certificates are renewed. Do not use <code>--deploy-hook</code> parameter with full paths ending in <code>.sh</code> as certbot will append <code>-hook</code> to the filename.</p>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#perfsonar-service-issues","title":"perfSONAR Service Issues","text":"perfSONAR services not running <p>Symptoms: Web interface not accessible, tests not running.</p> <p>Diagnostic steps:</p> <pre><code># Check service status inside container\npodman exec perfsonar-testpoint systemctl status apache2\npodman exec perfsonar-testpoint systemctl status pscheduler-ticker\npodman exec perfsonar-testpoint systemctl status owamp-server\n\n# Check for errors in service logs\npodman exec perfsonar-testpoint journalctl -u apache2 -n 50\npodman exec perfsonar-testpoint journalctl -u pscheduler-ticker -n 50\n</code></pre> <p>Solutions:</p> <ul> <li>Restart services inside container: <code>podman exec perfsonar-testpoint systemctl restart apache2</code></li> <li>Check Apache SSL configuration was patched correctly</li> <li>Verify certificates are in place: <code>ls -la /etc/letsencrypt/live/</code></li> <li>Restart container: <code>podman restart perfsonar-testpoint</code></li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#auto-update-issues","title":"Auto-Update Issues","text":"Auto-update not working <p>Symptoms: Containers not updating despite new images available.</p> <p>Diagnostic steps:</p> <pre><code># Check timer status\nsystemctl status perfsonar-auto-update.timer\nsystemctl list-timers perfsonar-auto-update.timer\n\n# Check service logs\njournalctl -u perfsonar-auto-update.service -n 100\n\n# Check update log\ntail -50 /var/log/perfsonar-auto-update.log\n\n# Manually test update\nsystemctl start perfsonar-auto-update.service\n</code></pre> <p>Solutions:</p> <ul> <li>Enable timer if not active: <code>systemctl enable --now perfsonar-auto-update.timer</code></li> <li>Verify script exists and is executable: <code>ls -la /usr/local/bin/perfsonar-auto-update.sh</code></li> <li>Check podman-compose is installed and working</li> <li>Review script for errors and update if needed</li> </ul>"},{"location":"personas/quick-deploy/install-perfsonar-toolkit/#general-debugging-tips","title":"General Debugging Tips","text":"Useful debugging commands <p>Container management:</p> <pre><code># View all containers (running and stopped)\npodman ps -a\n\n# View container resource usage\npodman stats\n\n# Enter container for interactive debugging\npodman exec -it perfsonar-testpoint /bin/bash\n\n# View compose configuration\ncd /opt/perfsonar-toolkit &amp;&amp; podman-compose config\n</code></pre> <p>Networking:</p> <pre><code># Check which process is listening on a port\nss -tlnp | grep &lt;port&gt;\n\n# Test connectivity to remote testpoint\nping &lt;remote-ip&gt;\ntraceroute &lt;remote-ip&gt;\n\n# Check nftables rules\nnft list ruleset\n</code></pre> <p>Logs:</p> <pre><code># System journal for container runtime\njournalctl -u podman -n 100\n\n# All logs from a container\npodman logs perfsonar-testpoint --tail=100\n\n# Follow logs in real-time\npodman logs -f perfsonar-testpoint\n</code></pre>"},{"location":"personas/quick-deploy/intro/","title":"Quick Deploy \u2014 Overview","text":"<p>This folder contains short, tested procedures to get a working service online quickly. Each quickstart focuses on the minimum steps required for a working deployment, along with a short verification checklist and links to optional hardening or automation pages.</p> <p>Start here if you need a working service in a single admin session. For perfSONAR testpoints the canonical quickstart is <code>quickstart-perfsonar-testpoint-v2.md</code> (recommended).</p> <p>What you will find:</p> <ul> <li> <p>Minimal quickstarts (install + verify)</p> </li> <li> <p>Brief verification steps and smoke tests</p> </li> <li> <p>Links to automation and optional hardening (firewall, selinux, nftables)</p> </li> </ul> <p>If you are doing repeat deployments, use the automated setup examples in <code>automated-setup/</code> instead of the manual quickstarts.</p>","tags":["quickstart","quick-deploy"]},{"location":"personas/quick-deploy/landing/","title":"\ud83d\ude80 Quick Deploy \u2014 perfSONAR for OSG/WLCG","text":"<p>Get perfSONAR running on OSG/WLCG in 1-2 hours with guided installation.</p> <ul> <li>Not sure which to use?</li> <li>Why perfSONAR in OSG/WLCG? \u2014 motivation and context</li> <li>Deployment Models &amp; Options \u2014 hardware, container vs RPM, and more</li> </ul>","tags":["quickstart","perfSONAR","testpoint","toolkit","deployment"]},{"location":"personas/quick-deploy/landing/#choose-your-deployment-type","title":"Choose Your Deployment Type","text":"Option Best For Guide \ud83d\udc33 Testpoint (Container) Lightweight, central archiving, minimal local resources Install perfSONAR Testpoint \ud83d\udce6 Toolkit (RPM) Full-featured, local web UI, on-site data Install perfSONAR Toolkit","tags":["quickstart","perfSONAR","testpoint","toolkit","deployment"]},{"location":"personas/quick-deploy/landing/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Each install guide covers:</li> <li>Post-install validation &amp; troubleshooting</li> <li>Security hardening</li> <li>Registration &amp; mesh enrollment</li> <li> <p>Multi-NIC and advanced network setup</p> </li> <li> <p>For automation, multi-host, or CI/CD: see Automated Setup Examples</p> </li> <li>For advanced/legacy/manual steps: see Manual Steps (legacy)</li> </ul>","tags":["quickstart","perfSONAR","testpoint","toolkit","deployment"]},{"location":"personas/quick-deploy/landing/#post-deploy-configuration","title":"Post-Deploy Configuration","text":"","tags":["quickstart","perfSONAR","testpoint","toolkit","deployment"]},{"location":"personas/quick-deploy/landing/#host-tuning-optional-but-recommended","title":"Host Tuning (Optional but Recommended)","text":"<p>Optimize kernel and NIC settings for network throughput:</p> <ul> <li>Fasterdata Tuning \u2014 ESnet recommendations for high-performance hosts</li> <li>Tool: <code>fasterdata-tuning.sh</code> (audit and apply modes, ~15 minutes)</li> </ul>","tags":["quickstart","perfSONAR","testpoint","toolkit","deployment"]},{"location":"personas/quick-deploy/landing/#support-resources","title":"Support &amp; Resources","text":"<ul> <li>perfSONAR FAQ</li> <li>Troubleshooter Guide</li> <li>Quick Triage Checklist</li> <li>Network Troubleshooting Guide</li> <li>Tools &amp; Scripts Reference</li> <li>perfSONAR User Mailing List</li> <li>OSG GOC support</li> <li>WLCG GGUS ticket \u2192 \"WLCG perfSONAR support\"</li> </ul>","tags":["quickstart","perfSONAR","testpoint","toolkit","deployment"]},{"location":"personas/quick-deploy/automated-setup/","title":"Automated setup \u2014 perfSONAR Testpoint","text":"<p>This directory contains examples and pointers for automated deployment:</p> <ul> <li> <p><code>ansible/</code> \u2014 playbooks and roles (example skeleton)</p> </li> <li> <p><code>docker/</code> \u2014 example container-based testpoint deployments (optional)</p> </li> </ul> <p>Use these examples as a starting point and adapt to site policies.</p>","tags":["automation","ansible"]},{"location":"personas/research/architecture/","title":"Architecture of OSG perfSONAR monitoring","text":"","tags":["architecture","data-pipeline"]},{"location":"personas/research/architecture/#architecture-summary","title":"Architecture (summary)","text":"<ul> <li> <p>perfSONAR agents (testpoints)</p> </li> <li> <p>central collectors (ELK/Elasticsearch ingestion)</p> </li> <li> <p>configuration services (psconfig / psetf)</p> </li> </ul> <p>Diagram and detailed flow to follow.</p>","tags":["architecture","data-pipeline"]},{"location":"personas/research/intro/","title":"Research \u2014 Overview","text":"<p>This area is for readers who want to understand system architecture, data pipelines, and development notes. See <code>architecture.md</code> for component diagrams and <code>data-pipeline.md</code> for ingestion/processing details.</p>","tags":["architecture","research"]},{"location":"personas/research/landing/","title":"\ud83d\udd2d Researcher \u2014 Architecture &amp; Analytics","text":"<p>Understand the OSG/WLCG network monitoring system, access data, and explore insights.</p>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#system-architecture","title":"System Architecture","text":"","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#high-level-overview","title":"High-Level Overview","text":"<p>Architecture Overview \u2014 components, responsibilities, and system design</p> <p>The perfSONAR network consists of:</p> <ul> <li> <p>Testpoints \u2014 distributed measurement agents at OSG/WLCG sites</p> </li> <li> <p>Collection Pipeline \u2014 HTTP-Archiver ingestion and Logstash processing</p> </li> <li> <p>Data Storage \u2014 Central Elasticsearch instances (distributed for</p> </li> </ul> <p>resilience)</p> <ul> <li> <p>Configuration Services \u2014 pSConfig for centralized test mesh management</p> </li> <li> <p>Monitoring \u2014 PSETF for infrastructure health and visibility</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#data-flow-from-measurement-to-insight","title":"Data Flow: From Measurement to Insight","text":"<ol> <li> <p>Measurement (2-minute intervals)</p> </li> <li> <p>perfSONAR testpoints run periodic latency, bandwidth, traceroute tests</p> </li> <li> <p>Tests configured by central mesh at <code>psconfig.opensciencegrid.org</code></p> </li> <li> <p>Collection (near real-time)</p> </li> <li> <p>Results sent to central Elasticsearch via HTTP-Archiver</p> </li> <li> <p>Logstash processes and enriches measurement metadata</p> </li> <li> <p>Storage (permanent)</p> </li> <li> <p>OSG Network Datastore \u2014 distributed Elasticsearch storage</p> </li> <li> <p>JSON API for direct programmatic access</p> </li> <li> <p>Analysis (on-demand)</p> </li> <li> <p>OSG Analytics Platform \u2014 Kibana dashboards + Jupyter notebooks</p> </li> <li> <p>Custom queries, time-series analysis, anomaly detection</p> </li> <li> <p>Visualization (real-time dashboards)</p> </li> <li> <p>WLCG Dashboards \u2014 performance monitoring</p> </li> <li> <p>Site-to-site path performance, latency trends, bandwidth utilization</p> </li> </ol>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#accessing-analyzing-data","title":"Accessing &amp; Analyzing Data","text":"","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#real-time-dashboards","title":"Real-Time Dashboards","text":"<p>WLCG Grafana Dashboards</p> <ul> <li> <p>Network performance by site and path</p> </li> <li> <p>Latency, bandwidth, packet loss trends</p> </li> <li> <p>Time-series filtering and drill-down</p> </li> </ul> <p>OSG PSETF Monitoring</p> <ul> <li> <p>perfSONAR infrastructure health</p> </li> <li> <p>Testpoint availability and service status</p> </li> <li> <p>Test execution success rates</p> </li> </ul> <p>OSG Analytics Platform</p> <ul> <li> <p>Custom Kibana queries</p> </li> <li> <p>Ad-hoc measurement exploration</p> </li> <li> <p>Jupyter notebooks for advanced analysis</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#programmatic-access","title":"Programmatic Access","text":"","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#elasticsearch-api","title":"Elasticsearch API","text":"<ul> <li> <p>OSG Network Datastore \u2014 detailed API documentation</p> </li> <li> <p>JSON endpoints for direct queries</p> </li> <li> <p>Available at: University of Chicago and University of Nebraska instances</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#example-query","title":"Example query","text":"<pre><code>curl -X GET \"elasticsearch-server:9200/perfsonar-testpoint/_search\" \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"query\": {\"match\": {\"agent\": \"testpoint.example.com\"}}}'\n</code></pre>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li> <p>Available on analytics platform</p> </li> <li> <p>Pre-built examples for common analysis tasks</p> </li> <li> <p>Python libraries: pandas, numpy, matplotlib for data science workflows</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#psconfig-api","title":"pSConfig API","text":"<ul> <li> <p>Central Mesh Configuration \u2014 test definitions and schedule</p> </li> <li> <p>JSON endpoints for retrieving test configurations</p> </li> <li> <p>Dynamic mesh membership and test parameters</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#analyzing-network-measurements","title":"Analyzing Network Measurements","text":"","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#questions-you-can-answer","title":"Questions You Can Answer","text":"<p>Performance Characterization:</p> <ul> <li> <p>Which network paths have persistent latency issues?</p> </li> <li> <p>What's the peak and sustained bandwidth between sites?</p> </li> <li> <p>How has network performance trended over the past month/year?</p> </li> <li> <p>Are there time-of-day or day-of-week patterns?</p> </li> </ul> <p>Infrastructure Health:</p> <ul> <li> <p>Which perfSONAR testpoints are most active?</p> </li> <li> <p>What's the geographic distribution of measurement agents?</p> </li> <li> <p>Are there coverage gaps (missing paths)?</p> </li> </ul> <p>Root Cause Analysis:</p> <ul> <li> <p>Did network performance degrade after a specific event?</p> </li> <li> <p>Correlate measurements with known network changes</p> </li> <li> <p>Identify bottlenecks in multi-hop paths</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#tools-resources","title":"Tools &amp; Resources","text":"<p>Data Analysis:</p> <ul> <li> <p>Kibana \u2014 query, filter, and visualize Elasticsearch data</p> </li> <li> <p>Jupyter \u2014 Python/pandas for advanced statistical analysis</p> </li> <li> <p>Grafana \u2014 time-series visualization and alerting</p> </li> </ul> <p>Measurement Understanding:</p> <ul> <li> <p>perfSONAR Documentation \u2014 test definitions, data formats</p> </li> <li> <p>ESnet Network Tools \u2014 methodology and best practices</p> </li> <li> <p>Measurement Archive \u2014 historical data storage and retrieval</p> </li> </ul> <p>Community Resources:</p> <ul> <li> <p>perfSONAR Mailing List \u2014 research collaborations</p> </li> <li> <p>WLCG Network WG \u2014 mesh governance</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#contributing-development","title":"Contributing &amp; Development","text":"","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#adding-new-measurements-or-tests","title":"Adding New Measurements or Tests","text":"<p>Propose new tests to the WLCG Mesh Configuration:</p> <ul> <li> <p>Define measurement parameters and schedule</p> </li> <li> <p>Request inclusion in production mesh</p> </li> <li> <p>Community review and approval</p> </li> </ul> <p>Or deploy local tests:</p> <ul> <li> <p>Add custom tests via your testpoint's pSConfig web interface</p> </li> <li> <p>Share configurations with the community</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#improving-the-infrastructure","title":"Improving the Infrastructure","text":"<p>Source code and development:</p> <ul> <li> <p>GitHub: osg-htc/networking \u2014 documentation, scripts, and automation</p> </li> <li> <p>GitHub: perfsonar/perfsonar \u2014 core perfSONAR software</p> </li> <li> <p>Issues &amp; Discussions \u2014 feature requests and bug reports</p> </li> </ul> <p>Contributing:</p> <ul> <li> <p>Submit pull requests for improvements</p> </li> <li> <p>Report issues and propose enhancements</p> </li> <li> <p>Email: networking-team@osg-htc.org</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#architecture-documentation","title":"Architecture &amp; Documentation","text":"<p>Want to contribute diagrams, data pipeline notes, or architecture updates?</p> <ul> <li> <p>Add diagrams or notes to <code>personas/research/</code></p> </li> <li> <p>Submit via GitHub PR or email networking-team@osg-htc.org</p> </li> <li> <p>All contributions welcome and attributed</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#related-topics","title":"Related Topics","text":"","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#infrastructure-services","title":"Infrastructure &amp; Services","text":"<ul> <li> <p>Network Services &amp; Data \u2014 datastore architecture and details</p> </li> <li> <p>Network Analytics \u2014 analytics platform overview</p> </li> <li> <p>perfSONAR Infrastructure Monitoring \u2014 PSETF system and health checks</p> </li> <li> <p>pSConfig Web Admin \u2014 centralized test configuration</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#foundational-concepts","title":"Foundational Concepts","text":"<ul> <li> <p>perfSONAR in OSG/WLCG \u2014 motivation and importance</p> </li> <li> <p>Deployment Models \u2014 testpoint architecture</p> </li> <li> <p>Installation Guide \u2014 for setting up your own measurement agent</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/research/landing/#tools-technical-details","title":"Tools &amp; Technical Details","text":"<ul> <li> <p>Tools &amp; Scripts \u2014 orchestration and management tools</p> </li> <li> <p>Host Tuning \u2014 performance optimization for measurement hosts</p> </li> <li> <p>perfSONAR FAQ \u2014 technical questions answered</p> </li> </ul>","tags":["architecture","research","analytics","data-pipeline"]},{"location":"personas/troubleshoot/intro/","title":"Troubleshooter \u2014 Overview","text":"<p>This area contains short triage checklists, playbooks, and example commands to gather the right information fast. Start with <code>triage-checklist-v2.md</code> to collect the host and network facts, then follow a specific playbook for the observed symptom.</p> <p>Key items:</p> <ul> <li> <p><code>triage-checklist-v2.md</code> \u2014 minimal data collection and verification steps</p> </li> <li> <p><code>playbooks/</code> \u2014 scenario-based runbooks (e.g., multi-NIC routing, firewall blocks, container issues)</p> </li> </ul>","tags":["troubleshoot","playbook"]},{"location":"personas/troubleshoot/landing/","title":"Troubleshoot Issues","text":"<p>for network troubleshooting in OSG/WLCG.\" persona: troubleshoot owners: [\"networking-team@osg-htc.org\"] status: active</p>"},{"location":"personas/troubleshoot/landing/#tags-troubleshoot-playbook-diagnostics","title":"tags: [troubleshoot, playbook, diagnostics]","text":""},{"location":"personas/troubleshoot/landing/#troubleshooter-diagnose-fix-network-issues","title":"\ud83d\udd27 Troubleshooter \u2014 Diagnose &amp; Fix Network Issues","text":"<p>Systematic approach to identifying and resolving network and perfSONAR problems.</p>"},{"location":"personas/troubleshoot/landing/#quick-start-5-minutes","title":"Quick Start (5 Minutes)","text":""},{"location":"personas/troubleshoot/landing/#is-it-a-network-problem","title":"Is it a Network Problem?","text":"<ol> <li>Gather facts: Run the Quick Triage Checklist \u2014</li> </ol> <p>collects system info, connectivity, services, logs</p> <ol> <li>Basic diagnostics: Follow **[Network Troubleshooting Guide](</li> </ol> <p>../../network-troubleshooting.md)** \u2014 contact procedures, support escalation</p> <ol> <li>Learn more: **[ESnet Troubleshooting Guide](</li> </ol> <p>https://fasterdata.es.net/performance-testing/troubleshooting/)** \u2014 detailed network investigation</p>"},{"location":"personas/troubleshoot/landing/#is-it-a-perfsonar-problem","title":"Is it a perfSONAR Problem?","text":"<ul> <li> <p>perfSONAR FAQ \u2014 quick answers to common issues</p> </li> <li> <p>OSG Debugging Guide \u2014 investigation steps</p> </li> <li> <p>perfSONAR Official Docs \u2014 comprehensive reference</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#diagnostic-tools-guides","title":"Diagnostic Tools &amp; Guides","text":""},{"location":"personas/troubleshoot/landing/#on-the-perfsonar-host","title":"On the perfSONAR Host","text":"<p>Check system status:</p> <ul> <li> <p>Systemd services: <code>systemctl status perfsonar-*</code></p> </li> <li> <p>Container status: <code>podman ps -a</code> or <code>docker ps -a</code></p> </li> <li> <p>Container logs: <code>podman logs perfsonar-testpoint</code> or <code>docker logs</code></p> </li> </ul> <p>Verify network configuration:</p> <ul> <li> <p>Triage Checklist \u2014 step-by-step verification</p> </li> <li> <p>Multiple NIC Setup \u2014 for multi-interface issues</p> </li> <li> <p>Host Tuning \u2014 audit kernel and NIC settings</p> </li> </ul> <p>Check firewall &amp; security:</p> <ul> <li> <p>Security &amp; Firewall Guide \u2014 required ports and rules</p> </li> <li> <p>nftables rules: <code>nft list ruleset</code></p> </li> <li> <p>Port status: <code>ss -ltnp</code></p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#network-path-analysis","title":"Network Path Analysis","text":"<p>ESnet tools: ESnet Troubleshooting Guide</p> <p>perfSONAR tools:</p> <ul> <li> <p>pScheduler: pScheduler documentation</p> </li> <li> <p>Test API: Query test meshes and historical results</p> </li> <li> <p>Measurement archive: Access stored results via web interface</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#common-scenarios-playbooks","title":"Common Scenarios &amp; Playbooks","text":""},{"location":"personas/troubleshoot/landing/#container-wont-start","title":"Container Won't Start","text":"<p>Playbook: Container Startup Issues (in progress)</p> <p>Quick checks:</p> <ul> <li> <p>Image available: <code>podman images | grep perfsonar</code></p> </li> <li> <p>Volumes mounted: <code>podman volume ls</code></p> </li> <li> <p>Ports available: <code>ss -ltnp | grep -E '(443|5001|9000|8080)'</code></p> </li> <li> <p>Logs: <code>podman logs perfsonar-testpoint</code></p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#tests-not-running","title":"Tests Not Running","text":"<p>Playbook: Tests Not Running (in progress)</p> <p>Quick checks:</p> <ul> <li> <p>pSConfig enrolled: <code>psconfig remote list</code></p> </li> <li> <p>Mesh connectivity: Can reach <code>psconfig.opensciencegrid.org</code>?</p> </li> <li> <p>pScheduler agent: <code>systemctl status perfsonar-pscheduler-agent</code></p> </li> <li> <p>Log errors: <code>podman logs perfsonar-testpoint | grep -i error</code></p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#high-latency-slow-tests","title":"High Latency / Slow Tests","text":"<p>Playbook: Performance Issues (in progress)</p> <p>Quick checks:</p> <ul> <li> <p>Host tuning: Run <code>fasterdata-tuning.sh</code> audit mode</p> </li> <li> <p>NIC settings: Check MTU, GRO, GSO, ring buffers</p> </li> <li> <p>Network load: Peak bandwidth during test time?</p> </li> <li> <p>Competing tests: Multiple tests running simultaneously?</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#firewall-blocking-tests","title":"Firewall Blocking Tests","text":"<p>Playbook: Firewall &amp; Network Access (in progress)</p> <p>Quick checks:</p> <ul> <li> <p>Required ports: Security &amp; Firewall Guide</p> </li> <li> <p>Test connectivity: Can reach remote perfSONAR instances?</p> </li> <li> <p>Firewall logs: Check local and campus firewall rules</p> </li> <li> <p>DNS resolution: Can resolve perfSONAR hosts?</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#escalation-support","title":"Escalation &amp; Support","text":"<p>When to contact support:</p>"},{"location":"personas/troubleshoot/landing/#level-1-self-service-diagnostics","title":"Level 1: Self-Service Diagnostics","text":"<ul> <li> <p>Run Triage Checklist</p> </li> <li> <p>Consult perfSONAR FAQ</p> </li> <li> <p>Review OSG Debugging Document</p> </li> <li> <p>Search perfSONAR Mailing List Archives</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#level-2-site-specific-support","title":"Level 2: Site-Specific Support","text":"<ul> <li> <p>Contact your site's network administrator</p> </li> <li> <p>Check local firewall, VLAN, NIC configuration</p> </li> <li> <p>Verify DNS, IP routing, upstream connectivity</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#level-3-osgwlcg-support","title":"Level 3: OSG/WLCG Support","text":"<ul> <li> <p>OSG sites: GOC Support Ticket</p> </li> <li> <p>Include: hostname, triage checklist results, error messages, logs</p> </li> <li> <p>WLCG sites: GGUS Ticket \u2192 \"WLCG Network Throughput\" or \"WLCG perfSONAR support\"</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#level-4-perfsonar-community","title":"Level 4: perfSONAR Community","text":"<ul> <li> <p>perfSONAR Community \u2014 active support</p> </li> <li> <p>perfSONAR Documentation \u2014 comprehensive reference</p> </li> <li> <p>GitHub Issues \u2014 report bugs</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#related-resources","title":"Related Resources","text":""},{"location":"personas/troubleshoot/landing/#setup-installation","title":"Setup &amp; Installation","text":"<ul> <li> <p>Quick Deploy Guide \u2014 initial installation help</p> </li> <li> <p>Installation Guide \u2014 detailed setup steps</p> </li> <li> <p>Deployment Models \u2014 choosing the right setup</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#configuration-optimization","title":"Configuration &amp; Optimization","text":"<ul> <li> <p>Host Tuning \u2014 performance optimization</p> </li> <li> <p>Multiple NIC Setup \u2014 multi-interface configuration</p> </li> <li> <p>fail2ban, nftables, SELinux</p> </li> </ul>"},{"location":"personas/troubleshoot/landing/#understanding-the-system","title":"Understanding the System","text":"<ul> <li> <p>perfSONAR in OSG/WLCG \u2014 why perfSONAR matters</p> </li> <li> <p>Architecture Overview \u2014 system design and data flow</p> </li> </ul>"},{"location":"personas/troubleshoot/triage-checklist/","title":"Triage checklist (minimal)","text":"","tags":["troubleshoot","checklist"]},{"location":"personas/troubleshoot/triage-checklist/#quick-triage-checklist","title":"Quick triage checklist","text":"<ol> <li>Gather host information</li> </ol> <pre><code>hostnamectl\ncat /etc/os-release\nuname -a\nip -c a\n</code></pre> <ol> <li>Check basic connectivity</li> </ol> <pre><code>ping -c 4 &lt;remote-ip-or-host&gt;\ntraceroute -n &lt;remote-ip-or-host&gt;\n</code></pre> <ol> <li>Verify perfSONAR services and containers</li> </ol> <pre><code>systemctl status perfsonar-*\nps aux | grep perfsonar\npodman ps || docker ps\n</code></pre> <ol> <li>Check firewall and ports</li> </ol> <pre><code>nft list ruleset\nss -ltnp\n</code></pre> <ol> <li> <p>Collect logs and measurements</p> </li> <li> <p>Container logs: <code>podman logs perfsonar-testpoint</code></p> </li> <li> <p>perfSONAR checks: <code>pscheduler tasks --host localhost</code></p> </li> </ol> <p>Use the scenario playbooks in <code>playbooks/</code> for step-by-step remediation instructions.</p>","tags":["troubleshoot","checklist"]},{"location":"personas/troubleshoot/playbooks/container-startup/","title":"Playbook: Container Won't Start","text":"","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#playbook-container-wont-start","title":"Playbook: Container Won't Start","text":"<p>!!! info \"Status\" This playbook is a placeholder for the troubleshooter persona. Detailed step-by-step diagnostics coming soon.</p>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#quick-diagnosis","title":"Quick Diagnosis","text":"<p>When to use this playbook: The perfSONAR container fails to start or immediately exits.</p>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#step-1-check-container-status","title":"Step 1: Check Container Status","text":"<pre><code># For Podman\n\npodman ps -a | grep perfsonar\n\n# For Docker\n\ndocker ps -a | grep perfsonar\n</code></pre> <p>Look for:</p> <ul> <li> <p>Exit code (non-zero = failure)</p> </li> <li> <p>Last restart time</p> </li> <li> <p>Error messages</p> </li> </ul>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#step-2-view-container-logs","title":"Step 2: View Container Logs","text":"<pre><code># For Podman\n\npodman logs perfsonar-testpoint\n\n# For Docker\n\ndocker logs perfsonar-testpoint\n</code></pre> <p>Common errors:</p> <ul> <li> <p><code>OCI runtime error</code> \u2014 host kernel/runtime issue</p> </li> <li> <p><code>Failed to bind port</code> \u2014 port already in use</p> </li> <li> <p><code>No such file or directory</code> \u2014 missing volume/mount</p> </li> <li> <p><code>permission denied</code> \u2014 volume permission issue</p> </li> </ul>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#step-3-check-prerequisites","title":"Step 3: Check Prerequisites","text":"<ul> <li> <p>Image available: <code>podman images | grep perfsonar</code></p> </li> <li> <p>Volumes exist: <code>podman volume ls | grep perfsonar</code></p> </li> <li> <p>Ports available: <code>ss -ltnp | grep -E '(443|5001|9000|8080)'</code></p> </li> <li> <p>Disk space: <code>df -h /var/lib/podman</code> or <code>/var/lib/docker</code></p> </li> </ul>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#step-4-escalate","title":"Step 4: Escalate","text":"<p>If the above doesn't resolve the issue, collect:</p> <ul> <li> <p>Container logs: <code>podman logs perfsonar-testpoint &gt; /tmp/logs.txt</code></p> </li> <li> <p>Systemd logs: <code>journalctl -u perfsonar-testpoint -n 50 &gt; /tmp/systemd.txt</code></p> </li> <li> <p>Host info: <code>uname -a</code>, <code>cat /etc/os-release</code></p> </li> </ul> <p>Then contact:</p> <ul> <li> <p>OSG GOC (OSG sites)</p> </li> <li> <p>WLCG GGUS (WLCG sites)</p> </li> <li> <p>perfSONAR Mailing List</p> </li> </ul>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#common-solutions","title":"Common Solutions","text":"","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#port-already-in-use","title":"Port Already in Use","text":"<p>Problem: Container fails with \"Address already in use\"</p> <p>Solution:</p> <pre><code># Find what's using the port (e.g., 443)\n\nss -ltnp | grep 443\n\n# Kill the process or change container port mapping\n\npodman stop conflicting-container\npodman rm conflicting-container\n</code></pre>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#volume-permission-denied","title":"Volume Permission Denied","text":"<p>Problem: Container fails with \"permission denied\" on volume</p> <p>Solution:</p> <pre><code># Check volume ownership\n\nls -la /var/lib/podman/volumes/perfsonar_data/\n\n# Fix permissions (adjust UID/GID as needed)\n\nsudo chown -R 65534:65534 /path/to/volume\n</code></pre>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#out-of-disk-space","title":"Out of Disk Space","text":"<p>Problem: Container fails with \"no space left\"</p> <p>Solution:</p> <pre><code># Check disk usage\n\ndf -h\n\n# Clean old images/containers\n\npodman system prune -a\n\n# Or increase disk allocation\n\n# (varies by host setup)\n</code></pre>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/container-startup/#see-also","title":"See Also","text":"<ul> <li> <p>Installation Guide</p> </li> <li> <p>Troubleshooter Landing</p> </li> <li> <p>Quick Triage Checklist</p> </li> </ul>","tags":["troubleshoot","container","docker","podman"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/","title":"Playbook: Firewall & Network Access Issues","text":"","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#playbook-firewall-network-access-issues","title":"Playbook: Firewall &amp; Network Access Issues","text":"<p>!!! info \"Status\" This playbook is a placeholder for the troubleshooter persona. Detailed step-by-step diagnostics coming soon.</p>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#quick-diagnosis","title":"Quick Diagnosis","text":"<p>When to use this playbook: Tests fail to execute, firewall errors in logs, or remote testpoints can't connect.</p>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#step-1-check-required-ports","title":"Step 1: Check Required Ports","text":"<pre><code># perfSONAR required ports\n\nPORTS=\"443 5001 8080 9000\"\n\n# Check if listening\n\nss -ltnp | grep -E '(443|5001|8080|9000)'\n\n# Test remote connectivity\n\nfor port in $PORTS; do\n  nc -zv &lt;remote_testpoint&gt; $port\ndone\n</code></pre> <p>Expected results:</p> <ul> <li> <p>443 (pScheduler) \u2014 REQUIRED, must be open</p> </li> <li> <p>5001 (iperf) \u2014 bandwidth tests only</p> </li> <li> <p>8080 (pSConfig) \u2014 configuration/discovery</p> </li> <li> <p>9000 (logging) \u2014 optional</p> </li> </ul>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#step-2-check-local-firewall","title":"Step 2: Check Local Firewall","text":"<pre><code># List current rules\n\nnft list ruleset\n\n# Check filter table specifically\n\nnft list table filter\n\n# Look for DROP/REJECT rules on required ports\n\nnft list table filter | grep -E '(443|5001|8080|9000|DROP|REJECT)'\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#step-3-test-connectivity-to-remote","title":"Step 3: Test Connectivity to Remote","text":"<pre><code># DNS resolution\n\ndig +short &lt;remote_testpoint_hostname&gt;\nnslookup &lt;remote_testpoint_hostname&gt;\n\n# Basic ping\n\nping -c 3 &lt;remote_testpoint_ip&gt;\n\n# Traceroute to identify hops\n\ntraceroute -n &lt;remote_testpoint_ip&gt;\n\n# Test port connectivity\n\ncurl -v https://&lt;remote_testpoint_hostname&gt;:443/\n\n# Test from remote back (may need to ask admin)\n\nssh &lt;remote_admin&gt; \"curl -v https://&lt;YOUR_TESTPOINT&gt;:443/\"\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#step-4-check-campusupstream-firewall","title":"Step 4: Check Campus/Upstream Firewall","text":"<pre><code># Ask your network team to check for:\n\n# 1. Outbound HTTPS (443) to perfSONAR hosts\n\n# 2. Inbound HTTPS (443) from perfSONAR hosts\n\n# 3. Outbound ephemeral ports (5000-6000 range)\n\n# 4. Inbound from trusted perfSONAR subnets\n\n# Provide them:\n\n- Your testpoint IP\n- Remote testpoint IPs you need to reach\n- Required ports (443 primary, 5001/8080 secondary)\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#step-5-review-container-host-logs","title":"Step 5: Review Container &amp; Host Logs","text":"<pre><code># Container firewall errors\n\npodman logs perfsonar-testpoint | grep -i \"firewall\\|iptables\\|nftables\\|refused\\|unreachable\"\n\n# System logs\n\njournalctl -n 100 | grep -i \"firewall\\|dropped\\|rejected\"\n\n# nftables audit logs (if enabled)\n\ndmesg | grep -i nft\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#step-6-escalate","title":"Step 6: Escalate","text":"<p>If still blocked, collect:</p> <ul> <li> <p>Firewall rules: <code>nft list ruleset &gt; /tmp/firewall.txt</code></p> </li> <li> <p>Listening ports: <code>ss -ltnp &gt; /tmp/ports.txt</code></p> </li> <li> <p>Test connectivity results</p> </li> <li> <p>Container logs: <code>podman logs perfsonar-testpoint &gt; /tmp/logs.txt</code></p> </li> <li> <p>Traceroute output: <code>traceroute -n &lt;remote&gt; &gt; /tmp/traceroute.txt</code></p> </li> </ul> <p>Then contact:</p> <ul> <li> <p>Local network team \u2014 for campus firewall checks</p> </li> <li> <p>OSG GOC (OSG sites)</p> </li> <li> <p>WLCG GGUS (WLCG sites)</p> </li> </ul>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#common-solutions","title":"Common Solutions","text":"","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#port-not-open-on-testpoint","title":"Port Not Open on Testpoint","text":"<p>Problem: <code>ss -ltnp | grep 443</code> shows nothing</p> <p>Solution:</p> <pre><code># Restart container\n\nsystemctl restart perfsonar-testpoint\n\n# Verify it started\n\nsystemctl status perfsonar-testpoint\n\n# Check logs\n\npodman logs perfsonar-testpoint | tail -20\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#local-firewall-blocking-nftables","title":"Local Firewall Blocking (nftables)","text":"<p>Problem: <code>nft list ruleset</code> shows DROP/REJECT on 443</p> <p>Solution:</p> <pre><code># View current rules\n\nnft list ruleset\n\n# Check if rule exists\n\nnft list table filter | grep \"dport 443\"\n\n# If missing, add rule (before DROP rule)\n\nsudo nft add rule inet filter input tcp dport 443 accept\n\n# Verify\n\nnft list table filter\n\n# Make persistent (varies by host setup)\n\n# Usually in /etc/nftables.conf or similar\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#campus-firewall-blocking","title":"Campus Firewall Blocking","text":"<p>Problem: Can ping remote, but can't reach port 443</p> <p>Solution:</p> <ol> <li> <p>Contact your campus network team with:</p> </li> <li> <p>Your testpoint IP address</p> </li> <li> <p>List of remote perfSONAR testpoints you need to reach</p> </li> <li> <p>Required ports: 443 (primary), 5001/8080 (secondary)</p> </li> <li> <p>Provide them: OSG/WLCG mesh documentation</p> </li> <li> <p>List available at: <code>psconfig.opensciencegrid.org</code></p> </li> <li> <p>Verify after firewall changes:</p> </li> </ol> <pre><code>curl -v https://&lt;remote_testpoint&gt;:443/\npscheduler tasks --host localhost\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#dns-resolution-failing","title":"DNS Resolution Failing","text":"<p>Problem: <code>nslookup &lt;remote_testpoint&gt;</code> fails</p> <p>Solution:</p> <pre><code># Check local resolver\n\ncat /etc/resolv.conf\n\n# Test with Google's DNS\n\nping 8.8.8.8\ndig @8.8.8.8 &lt;remote_testpoint_hostname&gt;\n\n# Verify DNS server is reachable\n\nnc -zv &lt;dns_server&gt; 53\n</code></pre>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#reference-required-ports","title":"Reference: Required Ports","text":"<p>| Port | Protocol | Purpose | Required | |------|----------|---------|----------| | 443 | HTTPS | pScheduler (test scheduling) | YES | | 5001 | TCP/UDP | iperf (bandwidth tests) | No (bandwidth only) | | 8080 | HTTP | pSConfig (configuration) | No (can use 443) | | 9000 | TCP | Logging (optional) | No |</p>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/firewall-issues/#see-also","title":"See Also","text":"<ul> <li> <p>Security &amp; Firewall Guide</p> </li> <li> <p>Network Troubleshooting Guide</p> </li> <li> <p>Troubleshooter Landing</p> </li> <li> <p>Quick Triage Checklist</p> </li> </ul>","tags":["troubleshoot","firewall","networking","nftables"]},{"location":"personas/troubleshoot/playbooks/performance-issues/","title":"Playbook: Performance Issues","text":"","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#playbook-performance-issues","title":"Playbook: Performance Issues","text":"<p>!!! info \"Status\" This playbook is a placeholder for the troubleshooter persona. Detailed step-by-step diagnostics coming soon.</p>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#quick-diagnosis","title":"Quick Diagnosis","text":"<p>When to use this playbook: Tests run but show high latency, low bandwidth, or slow execution times.</p>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#step-1-audit-host-tuning","title":"Step 1: Audit Host Tuning","text":"<pre><code># Download and run audit\n\ncurl -fsSL https://raw.githubusercontent.com/osg-htc/networking/main/docs/perfsonar/tools_scripts/fasterdata-tuning.sh \\\n  -o /tmp/fasterdata-tuning.sh\nchmod 0755 /tmp/fasterdata-tuning.sh\n\n# Run audit (no changes)\n\n/tmp/fasterdata-tuning.sh audit\n\n# Look for yellow (warnings) and red (critical) items\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#step-2-check-nic-settings","title":"Step 2: Check NIC Settings","text":"<pre><code># View interface statistics\n\nip -s link show\n\n# Check ring buffers\n\nethtool -g eth0\nethtool -g eth1\n\n# Check offload settings (should be ON)\n\nethtool -k eth0 | grep -E 'gro|gso|tso|rx-offload'\n\n# Check MTU (should be 1500 or higher if supported)\n\nip link show | grep mtu\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#step-3-monitor-during-test","title":"Step 3: Monitor During Test","text":"<pre><code># Watch CPU and memory\n\nwatch -n 1 'top -b -n 1 | head -20'\n\n# Monitor network traffic\n\niftop -i eth0\n\n# Check for packet loss/errors\n\nwatch -n 1 'ip -s link show eth0'\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#step-4-identify-bottlenecks","title":"Step 4: Identify Bottlenecks","text":"<p>Is it the testpoint?</p> <pre><code># Local bandwidth test\n\niperf3 -s\n\n# (from another host)\n\niperf3 -c &lt;testpoint_ip&gt;\n\n# Should reach near link speed (1Gbps, 10Gbps, 100Gbps)\n</code></pre> <p>Is it the network path?</p> <pre><code># Traceroute to remote testpoint\n\ntraceroute -n &lt;remote_testpoint&gt;\n\n# MTU path discovery\n\nping -M do -s 1472 &lt;remote_testpoint&gt;\n\n# Check for ECMP load balancing\n\nmtr -r -c 100 &lt;remote_testpoint&gt;\n</code></pre> <p>Is it contention?</p> <pre><code># Check for competing traffic\n\niftop -i eth0\n\n# Check qdisc\n\ntc qdisc show\n\n# Look at interface queue depth\n\nwatch -n 1 'cat /proc/net/dev | head -5'\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#step-5-escalate","title":"Step 5: Escalate","text":"<p>If still slow, collect:</p> <ul> <li> <p>Tuning audit: <code>/tmp/fasterdata-tuning.sh audit &gt; /tmp/audit.txt</code></p> </li> <li> <p>NIC stats: <code>ethtool -i eth0</code> and <code>ethtool -g eth0</code></p> </li> <li> <p>Local iperf results</p> </li> <li> <p>Test results from measurement archive</p> </li> <li> <p>perfSONAR logs: <code>podman logs perfsonar-testpoint | tail -100 &gt; /tmp/logs.txt</code></p> </li> </ul> <p>Then contact:</p> <ul> <li> <p>OSG GOC (OSG sites)</p> </li> <li> <p>WLCG GGUS (WLCG sites)</p> </li> </ul>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#common-solutions","title":"Common Solutions","text":"","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#kernel-buffers-too-small","title":"Kernel Buffers Too Small","text":"<p>Problem: <code>fasterdata-tuning.sh audit</code> shows red for <code>net.core.rmem_max</code> or <code>net.core.wmem_max</code></p> <p>Solution:</p> <pre><code># Apply tuning\n\nsudo /tmp/fasterdata-tuning.sh apply\n\n# Reboot to apply qdisc changes\n\nsudo reboot\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#nic-ring-buffers-too-small","title":"NIC Ring Buffers Too Small","text":"<p>Problem: Packet drops during bandwidth tests</p> <p>Solution:</p> <pre><code># Check current\n\nethtool -g eth0\n\n# Increase (example for 1Gbps)\n\nsudo ethtool -G eth0 rx 4096 tx 4096\n\n# Verify\n\nethtool -g eth0\n\n# Make persistent (add to network config or startup script)\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#mtu-mismatch","title":"MTU Mismatch","text":"<p>Problem: Path MTU discovery failing or fragmentation</p> <p>Solution:</p> <pre><code># Check MTU on all hops\n\nping -M do -s 1472 &lt;remote_testpoint&gt;\n\n# Adjust testpoint MTU if needed\n\nsudo ip link set dev eth0 mtu 9000\n\n# Update interface config for persistence\n\n# (varies by OS/network manager)\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#competing-tests","title":"Competing Tests","text":"<p>Problem: Multiple tests running simultaneously, causing slowdown</p> <p>Solution:</p> <pre><code># Check scheduled tests\n\npscheduler tasks --host localhost\n\n# View test execution schedule\n\npscheduler tasks --host localhost --format json\n\n# Space out high-bandwidth tests via pSConfig\n\n# (contact mesh administrators)\n</code></pre>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/performance-issues/#see-also","title":"See Also","text":"<ul> <li> <p>Host Tuning Guide</p> </li> <li> <p>Multiple NIC Setup</p> </li> <li> <p>Installation Guide</p> </li> <li> <p>Troubleshooter Landing</p> </li> </ul>","tags":["troubleshoot","performance","tuning"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/","title":"Playbook: Tests Not Running","text":"","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#playbook-tests-not-running","title":"Playbook: Tests Not Running","text":"<p>!!! info \"Status\" This playbook is a placeholder for the troubleshooter persona. Detailed step-by-step diagnostics coming soon.</p>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#quick-diagnosis","title":"Quick Diagnosis","text":"<p>When to use this playbook: Tests configured in pSConfig don't execute, or no results appear in the measurement archive.</p>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#step-1-verify-psconfig-enrollment","title":"Step 1: Verify pSConfig Enrollment","text":"<pre><code># Check if enrolled in meshes\n\npsconfig remote list\n\n# Expected output: should show OSG/WLCG mesh URLs\n\n# Example:\n\n# https://psconfig.opensciencegrid.org/pub/auto/&lt;YOUR_HOSTNAME&gt;\n</code></pre> <p>If empty or missing:</p> <ul> <li> <p>Testpoint not enrolled in mesh</p> </li> <li> <p>Run: `/opt/perfsonar-tp/tools_scripts/</p> </li> </ul> <p>perfSONAR-auto-enroll-psconfig.sh`</p>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#step-2-check-pscheduler-status","title":"Step 2: Check pScheduler Status","text":"<pre><code># View scheduled tests\n\npscheduler tasks --host localhost\n\n# Check agent is running\n\nsystemctl status perfsonar-pscheduler-agent\n\n# View agent logs\n\npodman logs perfsonar-testpoint | grep -i scheduler\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#step-3-verify-network-connectivity","title":"Step 3: Verify Network Connectivity","text":"<pre><code># Can reach pSConfig server?\n\nping psconfig.opensciencegrid.org\ncurl -I https://psconfig.opensciencegrid.org/pub/auto\n\n# Can reach remote perfSONAR instances?\n\nping &lt;remote_testpoint_hostname&gt;\ntelnet &lt;remote_testpoint_hostname&gt; 443\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#step-4-check-firewall-ports","title":"Step 4: Check Firewall &amp; Ports","text":"<pre><code># Verify required ports are open\n\nss -ltnp | grep -E '(443|5001|8080|9000)'\n\n# Check firewall rules\n\nnft list ruleset | grep -E '(443|5001)'\n\n# Can connect to remote port 443?\n\nnc -zv &lt;remote_testpoint_hostname&gt; 443\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#step-5-review-container-logs","title":"Step 5: Review Container Logs","text":"<pre><code># Look for pscheduler errors\n\npodman logs perfsonar-testpoint | grep -i error | tail -20\n\n# Look for HTTP errors\n\npodman logs perfsonar-testpoint | grep -i \"http\\|connection\\|refused\"\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#step-6-escalate","title":"Step 6: Escalate","text":"<p>If still not running, collect:</p> <ul> <li> <p>pSConfig status: <code>psconfig remote list</code></p> </li> <li> <p>pScheduler tasks: <code>pscheduler tasks --host localhost &gt; /tmp/tasks.txt</code></p> </li> <li> <p>Container logs: <code>podman logs perfsonar-testpoint &gt; /tmp/logs.txt</code></p> </li> <li> <p>Firewall rules: <code>nft list ruleset &gt; /tmp/firewall.txt</code></p> </li> </ul> <p>Then contact:</p> <ul> <li> <p>OSG GOC (OSG sites)</p> </li> <li> <p>WLCG GGUS (WLCG sites)</p> </li> <li> <p>perfSONAR Mailing List</p> </li> </ul>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#common-solutions","title":"Common Solutions","text":"","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#not-enrolled-in-mesh","title":"Not Enrolled in Mesh","text":"<p>Problem: <code>psconfig remote list</code> is empty</p> <p>Solution:</p> <pre><code># Enroll automatically\n\n/opt/perfsonar-tp/tools_scripts/perfSONAR-auto-enroll-psconfig.sh \\\n  --fqdn &lt;YOUR_HOSTNAME&gt; \\\n  --profile latency\n\n# Verify enrollment\n\npsconfig remote list\npscheduler tasks --host localhost\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#firewall-blocking-remote-tests","title":"Firewall Blocking Remote Tests","text":"<p>Problem: Tests fail to connect to remote testpoints</p> <p>Solution:</p> <pre><code># Test connectivity\n\ncurl -v https://&lt;remote_testpoint&gt;:443/\n\n# Check if 443 is open\n\nnft add rule inet filter input tcp dport 443 accept\n\n# Or add to firewall rules permanently\n\n# (varies by site configuration)\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#wrong-fqdn-in-psconfig","title":"Wrong FQDN in pSConfig","text":"<p>Problem: Hostname mismatch between pSConfig and local config</p> <p>Solution:</p> <pre><code># Check local FQDN\n\nhostname -f\n\n# Re-enroll with correct hostname\n\n/opt/perfsonar-tp/tools_scripts/perfSONAR-auto-enroll-psconfig.sh \\\n  --fqdn $(hostname -f) \\\n  --profile latency\n</code></pre>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"personas/troubleshoot/playbooks/tests-not-running/#see-also","title":"See Also","text":"<ul> <li> <p>Installation Guide</p> </li> <li> <p>pSConfig Documentation</p> </li> <li> <p>Troubleshooter Landing</p> </li> <li> <p>Quick Triage Checklist</p> </li> </ul>","tags":["troubleshoot","pscheduler","psconfig","tests"]},{"location":"release-notes/quick-deploy-1.0.0/","title":"Quick Deploy \u2014 v1.0.0 (Quick Deploy docs)","text":"<p>Release date: 2025-11-08</p>"},{"location":"release-notes/quick-deploy-1.0.0/#summary","title":"Summary","text":"<p>This release packages the Quick Deploy documentation for perfSONAR Testpoint into a formal v1.0.0 release. It contains the following user-facing changes and quality-of-life improvements:</p> <ul> <li> <p>Update container image references to use the OSG registry:   -- <code>hub.opensciencegrid.org/osg-htc/perfsonar-testpoint:production</code> (replaced earlier ghcr.io references)</p> </li> <li> <p>Add <code>seed_testpoint_host_dirs.sh</code> helper to seed host directories from temporary containers</p> </li> <li> <p>Installed at <code>docs/perfsonar/tools_scripts/seed_testpoint_host_dirs.sh</code> (also referenced in the Quick Deploy doc)</p> </li> <li> <p>Replace the inline host-directory seeding snippet in Step 5 (Option B) with a reference and usage examples for the helper script.</p> </li> <li> <p>Add site versions metadata (<code>docs/versions.json</code>) and make this release available as <code>1.00</code> in the version selector.</p> </li> <li> <p>Minor doc fixes and mkdocs rebuild to ensure consistent rendering.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.0.0/#testing-and-verification","title":"Testing and verification","text":"<ul> <li> <p>The site was rebuilt locally with <code>mkdocs build</code> and previewed to verify the Quick Deploy page renders and the helper script is linked.</p> </li> <li> <p>The helper script was tested for basic execution path (no destructive operations) by ensuring it can be created and marked executable.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.0.0/#notes-for-reviewers","title":"Notes for reviewers","text":"<ul> <li> <p>The release touches documentation only; there are no code or runtime behavior changes beyond helper scripts intended for operator convenience.</p> </li> <li> <p>Reviewers should confirm the image reference change and the helper script usage are correct for site deployment processes.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.0.0/#how-to-roll-back","title":"How to roll back","text":"<p>If you need to revert to the prior doc state, checkout the <code>master</code> commit prior to this branch and reapply changes selectively. The previous documented build is available as version <code>0.9</code> in <code>docs/versions.json</code>.</p>"},{"location":"release-notes/quick-deploy-1.0.1/","title":"Quick Deploy documentation \u2014 v1.0.1","text":"<p>Release date: 2025-11-08</p> <p>This release contains small but important documentation fixes and housekeeping for the Quick Deploy guide.</p> <p>Highlights</p> <ul> <li> <p>Marked the current site build as version 1.0.1 (<code>docs/versions.json</code>).</p> </li> <li> <p>Cleaned <code>docs/versions.json</code> formatting so the versions list is valid JSON.</p> </li> <li> <p>Several doc improvements and clarifications were retained from the v1.00 work: the <code>seed_testpoint_host_dirs.sh</code> helper is referenced in the Quick Deploy Step 5 instructions, container image references were updated to <code>hub.opensciencegrid.org/osg-htc/perfsonar-testpoint:production</code>, and Step 6 enrollment guidance was consolidated and clarified.</p> </li> </ul> <p>Notes for operators</p> <ul> <li> <p>The helper script <code>docs/perfsonar/tools_scripts/seed_testpoint_host_dirs.sh</code> remains available in the docs tree and is recommended when seeding host directories during deployment.</p> </li> <li> <p>The psconfig auto-enroll helper (<code>perfSONAR-auto-enroll-psconfig.sh</code>) skips RFC1918 addresses and logs discovered FQDNs \u2014 see Step 6 for sample usage.</p> </li> </ul> <p>If you need an actual release artifact (tag or site snapshot) created, tell me whether to create a tag (git tag) and/or prepare a static site snapshot for upload.</p>"},{"location":"release-notes/quick-deploy-1.1.0/","title":"Release 1.1.0 \u2014 2025-11-08","text":"<p>This release collects small documentation/tooling updates to the Quick Deploy guide and the perfSONAR tools bundle.</p>"},{"location":"release-notes/quick-deploy-1.1.0/#highlights","title":"Highlights","text":"<ul> <li> <p>Deprecation: <code>perfSONAR-extract-lsregistration.sh</code> has been deprecated and removed from active maintenance. A short DEPRECATION.md explains the rationale and points users to <code>perfSONAR-update- lsregistration.sh</code> for restores.</p> </li> <li> <p>New helper: <code>seed_testpoint_host_dirs.sh</code> (added previously) is the recommended helper for seeding host directories from a short-lived perfSONAR testpoint container and certbot container.</p> </li> <li> <p>Versioning: the docs site <code>docs/versions.json</code> has been updated and the   current site version is <code>1.1.0</code>.</p> </li> <li> <p>Installer update: <code>install_tools_scripts.sh</code> was updated to avoid fetching the deprecated extractor and includes a comment pointing to the deprecation doc.</p> </li> <li> <p>Container image references in the Quick Deploy examples continue to point   at <code>hub.opensciencegrid.org/osg-htc/perfsonar-testpoint:production</code>.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.1.0/#notes-for-maintainers","title":"Notes for maintainers","text":"<ul> <li> <p>The repository now contains a <code>DEPRECATION.md</code> alongside the deprecated script (which is replaced by a stub that emits the warning). If you want to preserve the full extractor implementation for archival reasons, consider moving it to an <code>archive/</code> or <code>legacy/</code> folder and referencing it from the deprecation doc.</p> </li> <li> <p>If you want a signed snapshot of the generated site attached to the GitHub release, create the site build (<code>mkdocs build</code>), tar/gzip the <code>site/</code> directory, create a SHA256 (and optionally a GPG detached signature), and upload the assets to the release.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.1.0/#how-to-publish-this-release-suggested-sequence","title":"How to publish this release (suggested sequence)","text":"<ol> <li>Review the unstaged changes locally. If you accept them, commit them:</li> </ol> <p>git add docs/versions.json docs/release-notes/quick-deploy-1.1.0.md docs/perfsonar/tools_scripts/DEPRECATION.md docs/perfsonar/tools_scripts/perfSONAR-extract-lsregistration.sh docs/perfsonar/tools_scripts/install_tools_scripts.sh CHANGELOG.md git commit -m \"chore(release): prepare Quick Deploy v1.1.0; deprecate extractor\"</p> <ol> <li>Create an annotated tag for the release (replace DATE and notes as desired):</li> </ol> <p>git tag -a v1.1.0 -m \"Quick Deploy v1.1.0 \u2014 2025-11-08\"</p> <ol> <li>Push commits and tags:</li> </ol> <p>git push origin master git push origin v1.1.0</p> <ol> <li>Build the site and create a snapshot (optional but recommended):</li> </ol> <p>mkdocs build tar -czf site-quick-deploy-v1.1.0-$(date -u +%Y%m%dT%H%M%SZ).tar.gz site/ sha256sum site-quick- deploy-v1.1.0-*.tar.gz &gt; site-quick-deploy-v1.1.0.sha256</p> <ol> <li>Create a GitHub release for <code>v1.1.0</code> and upload the snapshot and checksum.</li> </ol> <p>If you want, I can perform the tagging and release creation for you \u2014 but you said you'll commit and push manually, so I'm leaving those final steps to you.</p>"},{"location":"release-notes/quick-deploy-1.3.0/","title":"Release Notes - Quick Deploy Guide v1.3.0","text":"<p>Release Date: November 9, 2025</p>"},{"location":"release-notes/quick-deploy-1.3.0/#overview","title":"Overview","text":"<p>Version 1.3.0 represents a major documentation and usability improvement release. All 20 recommendations from the comprehensive documentation review have been implemented, significantly enhancing clarity, correctness, and consistency across the entire perfSONAR testpoint deployment guide.</p>"},{"location":"release-notes/quick-deploy-1.3.0/#high-priority-fixes","title":"High-Priority Fixes","text":""},{"location":"release-notes/quick-deploy-1.3.0/#correctness-improvements","title":"Correctness Improvements","text":"<ol> <li> <p>Fixed documentation typo - Corrected \"configuraiton\" \u2192 \"configuration\" in Step 3</p> </li> <li> <p>Added bind-utils requirement - DNS validation package now included in Step 1 installation</p> </li> <li> <p>Clarified command execution context - All pscheduler commands now show proper container execution syntax</p> </li> <li> <p>Documented certbot flags - Added comprehensive explanation of all certbot command options</p> </li> </ol>"},{"location":"release-notes/quick-deploy-1.3.0/#consistency-improvements","title":"Consistency Improvements","text":""},{"location":"release-notes/quick-deploy-1.3.0/#script-versioning","title":"Script Versioning","text":"<p>All helper scripts now include version headers (v1.0.0):</p> <ul> <li> <p><code>check-deps.sh</code></p> </li> <li> <p><code>check-perfsonar-dns.sh</code></p> </li> <li> <p><code>seed_testpoint_host_dirs.sh</code></p> </li> <li> <p><code>perfSONAR-auto-enroll-psconfig.sh</code></p> </li> <li> <p><code>install_tools_scripts.sh</code></p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#enhanced-script-usability","title":"Enhanced Script Usability","text":"<p>All scripts now support:</p> <ul> <li> <p><code>--version</code> flag for version information</p> </li> <li> <p><code>--help</code> / <code>-h</code> flag with comprehensive usage documentation</p> </li> <li> <p>Documented exit codes for automation</p> </li> <li> <p>Consistent command-line interface</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#documentation-additions","title":"Documentation Additions","text":"<ol> <li> <p>SELinux volume labels explained - Clear documentation of <code>:Z</code> vs <code>:z</code> flag usage</p> </li> <li> <p>Bootstrap verification - Step 2 now includes verification commands</p> </li> <li> <p>Auto-update testing - Step 6.5 includes testing instructions</p> </li> <li> <p>Maintenance flexibility - Ongoing maintenance section now context-aware</p> </li> </ol>"},{"location":"release-notes/quick-deploy-1.3.0/#new-sections","title":"New Sections","text":""},{"location":"release-notes/quick-deploy-1.3.0/#comprehensive-troubleshooting-guide","title":"Comprehensive Troubleshooting Guide","text":"<p>Added extensive troubleshooting appendix covering:</p> <ul> <li> <p>Container Issues</p> </li> <li> <p>Container won't start or exits immediately</p> </li> <li> <p>SELinux denials blocking operations</p> </li> <li> <p>Networking Issues</p> </li> <li> <p>Policy-based routing not working</p> </li> <li> <p>DNS resolution failures</p> </li> <li> <p>Certificate Issues</p> </li> <li> <p>Let's Encrypt issuance failures</p> </li> <li> <p>Certificate not loaded after renewal</p> </li> <li> <p>perfSONAR Service Issues</p> </li> <li> <p>Services not running</p> </li> <li> <p>Apache/pScheduler/OWAMP errors</p> </li> <li> <p>Auto-Update Issues</p> </li> <li> <p>Timer not running</p> </li> <li> <p>Images not updating</p> </li> <li> <p>General Debugging Tips</p> </li> <li> <p>Container management commands</p> </li> <li> <p>Networking diagnostic commands</p> </li> <li> <p>Log inspection commands</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#recovery-instructions","title":"Recovery Instructions","text":"<ol> <li> <p>PBR network recovery - Step 3 now includes detailed recovery procedures for SSH disconnections</p> </li> <li> <p>Console access guidance - BMC/iLO/iDRAC usage documented</p> </li> <li> <p>Backup restoration - How to restore from NetworkManager connection backups</p> </li> </ol>"},{"location":"release-notes/quick-deploy-1.3.0/#technical-improvements","title":"Technical Improvements","text":""},{"location":"release-notes/quick-deploy-1.3.0/#package-management","title":"Package Management","text":"<ul> <li> <p><code>bind-utils</code> (EL) / <code>dnsutils</code> (Debian) added to Step 1 installation</p> </li> <li> <p>Dependencies clearly documented for each step</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#command-clarity","title":"Command Clarity","text":"<p>All examples now show:</p> <ul> <li> <p>Proper container execution context (<code>podman exec -it perfsonar-testpoint ...</code>)</p> </li> <li> <p>Host vs container command distinction</p> </li> <li> <p>Full command paths for clarity</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#exit-code-documentation","title":"Exit Code Documentation","text":"<p>All scripts now document their exit codes:</p> <ul> <li> <p><code>0</code> - Success</p> </li> <li> <p><code>1</code> - Usage/argument errors</p> </li> <li> <p><code>2</code> - Missing prerequisites</p> </li> <li> <p><code>3</code> - Operation failures</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#breaking-changes","title":"Breaking Changes","text":"<p>None. All changes are backward compatible.</p>"},{"location":"release-notes/quick-deploy-1.3.0/#upgrade-path","title":"Upgrade Path","text":""},{"location":"release-notes/quick-deploy-1.3.0/#from-v120","title":"From v1.2.0","text":"<ol> <li>Pull latest changes:</li> </ol> <pre><code>cd /opt/perfsonar-tp/tools_scripts\ncurl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh | bash -s -- /opt/perfsonar-tp\n</code></pre> <ol> <li> <p>Review new troubleshooting guide for common issues</p> </li> <li> <p>Test new script features:</p> </li> </ol> <pre><code>/opt/perfsonar-tp/tools_scripts/check-deps.sh --version\n/opt/perfsonar-tp/tools_scripts/check-deps.sh --help\n</code></pre>"},{"location":"release-notes/quick-deploy-1.3.0/#validation","title":"Validation","text":"<p>All recommendations from the documentation review have been implemented and verified:</p> <ul> <li> <p>\u2705 Typo corrections</p> </li> <li> <p>\u2705 Package requirements updated</p> </li> <li> <p>\u2705 Script version headers added</p> </li> <li> <p>\u2705 --version flags implemented</p> </li> <li> <p>\u2705 Help documentation enhanced</p> </li> <li> <p>\u2705 SELinux documentation added</p> </li> <li> <p>\u2705 Verification steps added</p> </li> <li> <p>\u2705 Testing instructions included</p> </li> <li> <p>\u2705 Troubleshooting guide created</p> </li> <li> <p>\u2705 Recovery procedures documented</p> </li> <li> <p>\u2705 Certbot flags explained</p> </li> <li> <p>\u2705 Maintenance schedules contextualized</p> </li> <li> <p>\u2705 Command execution clarified</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#git-tags","title":"Git Tags","text":"<ul> <li> <p>Previous: <code>v1.2.0</code></p> </li> <li> <p>Current: <code>v1.3.0</code></p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#commits","title":"Commits","text":"<ul> <li> <p><code>132492f</code> - docs: Major documentation improvements for v1.3.0</p> </li> <li> <p><code>8d6b10b</code> - feat: Add --version flag and improved help to all scripts</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#contributors","title":"Contributors","text":"<ul> <li> <p>Automated code review and implementation</p> </li> <li> <p>Based on comprehensive 20-point documentation review</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.3.0/#next-steps","title":"Next Steps","text":"<p>Future improvements may include:</p> <ul> <li> <p>Cross-references between main guide and detailed script READMEs</p> </li> <li> <p>Centralized logging strategy documentation</p> </li> <li> <p>Additional automation examples</p> </li> <li> <p>CI/CD integration examples</p> </li> </ul> <p>For the complete installation guide, see Installing a perfSONAR Testpoint for WLCG/OSG.</p>"},{"location":"release-notes/quick-deploy-1.4.0/","title":"Release Notes - Quick Deploy Guide v1.4.0","text":"<p>Release Date: November 10, 2025</p>"},{"location":"release-notes/quick-deploy-1.4.0/#overview","title":"Overview","text":"<p>Version 1.4.0 focuses on reliability, guided automation, and simplifying the earliest steps of a perfSONAR testpoint deployment. It introduces a new orchestrator, improves policy-based routing safety, and deprecates an older dependency- checker script in favor of a unified package install workflow.</p>"},{"location":"release-notes/quick-deploy-1.4.0/#highlights","title":"Highlights","text":""},{"location":"release-notes/quick-deploy-1.4.0/#guided-orchestrator-new","title":"Guided Orchestrator (New)","text":"<p><code>perfSONAR-orchestrator.sh</code> provides a step-by-step (or non-interactive) path through prerequisites, bootstrap, network configuration (PBR), security hardening, container deployment (Option A/B), certificate issuance, and pSConfig enrollment.</p> <p>Key features:</p> <ol> <li> <p>Interactive confirm/skip flow or <code>--non-interactive</code> batch mode.</p> </li> <li> <p>Supports selection between deployment options (testpoint only vs Let's Encrypt).</p> </li> <li> <p>Automatic DNS and certificate validation checkpoints.</p> </li> <li> <p>Exit codes suitable for automation.</p> </li> </ol>"},{"location":"release-notes/quick-deploy-1.4.0/#non-disruptive-pbr-mode-improved","title":"Non-Disruptive PBR Mode (Improved)","text":"<p><code>perfSONAR-pbr-nm.sh</code> now defaults to an in-place apply mode that preserves existing NetworkManager connections and minimizes SSH disruption. A full rebuild can still be invoked with <code>--rebuild-all</code> when a clean slate is needed.</p>"},{"location":"release-notes/quick-deploy-1.4.0/#unified-prerequisite-installation-simplified","title":"Unified Prerequisite Installation (Simplified)","text":"<p>The guide now uses a single one-shot <code>dnf</code> command (on EL9 derivatives) early in Step 1 to install all required packages (including DNS tools). This replaces iterative dependency checks and removes friction on minimal hosts.</p>"},{"location":"release-notes/quick-deploy-1.4.0/#deprecation-check-depssh","title":"Deprecation: <code>check-deps.sh</code>","text":"<p>The legacy dependency checking script is deprecated. Its responsibilities are replaced by the unified package install and orchestrator validation steps. See <code>DEPRECATION.md</code> for migration guidance and the planned removal timeline.</p>"},{"location":"release-notes/quick-deploy-1.4.0/#detailed-changes","title":"Detailed Changes","text":"<ul> <li> <p>Added orchestrator script with interactive and batch operation modes.</p> </li> <li> <p>Updated Quick Deploy Steps 1\u20133 to surface orchestrator early and remove <code>sudo</code> prefixes.</p> </li> <li> <p>Improved documentation spacing and markdownlint compliance.</p> </li> <li> <p>Introduced non-disruptive default mode for PBR script, reducing need for console access.</p> </li> <li> <p>Auto-gen: <code>perfSONAR-pbr-nm.sh</code> generator now skips NICs with no IPv4/IPv6 gateway (management-only NICs) to avoid producing non-functional NetworkManager profiles; <code>DEFAULT_ROUTE_NIC</code> is preserved (see PR #28).</p> </li> <li> <p>Added dedicated deprecation tracking file <code>DEPRECATION.md</code>.</p> </li> <li> <p>Updated CHANGELOG with new version entry and deprecation notice.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.0/#upgrade-path","title":"Upgrade Path","text":"<p>From v1.3.0:</p> <ol> <li>Refresh helper scripts:</li> </ol> <pre><code>curl -fsSL https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install_tools_scripts.sh | bash -s -- /opt/perfsonar-tp\n</code></pre> <ol> <li> <p>Review <code>DEPRECATION.md</code> and remove any automation references to <code>check-deps.sh</code>.</p> </li> <li> <p>Use orchestrator for new hosts or when revalidating configuration:</p> </li> </ol> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-orchestrator.sh --option A --fqdn &lt;FQDN&gt; --email &lt;EMAIL&gt;\n</code></pre> <ol> <li>(Optional) Reapply PBR in non-disruptive mode:</li> </ol> <pre><code>/opt/perfsonar-tp/tools_scripts/perfSONAR-pbr-nm.sh --apply-inplace\n</code></pre>"},{"location":"release-notes/quick-deploy-1.4.0/#validation","title":"Validation","text":"<ul> <li> <p>MkDocs site builds without structural errors after additions.</p> </li> <li> <p>Orchestrator help/usage renders correctly in documentation references.</p> </li> <li> <p>PBR script tested for idempotent route/rule replacement.</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.0/#deprecations","title":"Deprecations","text":"<ul> <li><code>check-deps.sh</code> (planned removal: 2026-03-01) \u2014 replace with unified install + orchestrator.</li> </ul>"},{"location":"release-notes/quick-deploy-1.4.0/#breaking-changes","title":"Breaking Changes","text":"<p>None. All enhancements are backward compatible; deprecated script is still present for transition.</p>"},{"location":"release-notes/quick-deploy-1.4.0/#commits","title":"Commits","text":"<ul> <li> <p>docs/orchestrator: add guided install script and integrate into quick-deploy</p> </li> <li> <p>docs/pbr: add in-place mode and update guide references</p> </li> <li> <p>docs: remove <code>sudo</code> prefixes and fix markdown spacing</p> </li> <li> <p>chore: add DEPRECATION.md and update CHANGELOG for v1.4.0</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.0/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Remove deprecated dependency checker after transition window.</p> </li> <li> <p>Add automated test harness for orchestrator flows.</p> </li> <li> <p>Expand CI to include link checking heuristics under controlled ignore patterns.</p> </li> </ul> <p>For the full installation guide, see Installing a perfSONAR Testpoint for WLCG/OSG.</p>"},{"location":"release-notes/quick-deploy-1.4.1/","title":"Release Notes - Quick Deploy Guide v1.4.1","text":"<p>Release Date: December 3, 2025</p>"},{"location":"release-notes/quick-deploy-1.4.1/#overview","title":"Overview","text":"<p>Version 1.4.1 is a bug-fix release that resolves critical container lifecycle issues with the certbot systemd service, ensuring both perfSONAR testpoint and certbot containers survive system reboots and maintain proper systemd integration.</p>"},{"location":"release-notes/quick-deploy-1.4.1/#highlights","title":"Highlights","text":""},{"location":"release-notes/quick-deploy-1.4.1/#fixed-certbot-service-restart-after-reboot","title":"Fixed Certbot Service Restart After Reboot","text":"<p>The <code>perfsonar-certbot.service</code> systemd unit has been fixed to properly manage the certbot container lifecycle. Previously, the certbot service would fail immediately after starting with exit code 2 and the error: <code>certbot: error: Unable to open config file</code>.</p> <p>Root Cause: The certbot container image has a built-in entrypoint that expects direct certbot commands. When the systemd unit attempted to run a shell loop for certificate renewal, the entrypoint incorrectly interpreted the shell command as a config file path.</p> <p>Solution: Three critical fixes were applied to the <code>install-systemd-units.sh</code> script:</p> <ol> <li> <p>Added <code>--entrypoint=/bin/sh</code> to override the container's built-in entrypoint, allowing shell commands to execute properly</p> </li> <li> <p>Added <code>--systemd=always</code> to ensure proper systemd integration and automatic container restart after host reboots</p> </li> <li> <p>Improved command syntax from <code>/bin/sh -c '...'</code> to <code>-c \"...\"</code> with proper signal handling</p> </li> <li> <p>Removed <code>--deploy-hook</code> parameter from certbot renew command - certbot automatically discovers and executes hooks in <code>/etc/letsencrypt/renewal-hooks/deploy/</code> (using <code>--deploy-hook</code> with paths ending in <code>.sh</code> causes certbot to append <code>-hook</code> to the filename, breaking hook execution)</p> </li> </ol>"},{"location":"release-notes/quick-deploy-1.4.1/#detailed-changes","title":"Detailed Changes","text":""},{"location":"release-notes/quick-deploy-1.4.1/#files-modified","title":"Files Modified","text":"<ul> <li> <p><code>docs/perfsonar/tools_scripts/install-systemd-units.sh</code></p> </li> <li> <p>Added <code>--systemd=always</code> flag to certbot service</p> </li> <li> <p>Added <code>--entrypoint=/bin/sh</code> to override certbot container entrypoint</p> </li> <li> <p>Changed trap handling from <code>trap exit TERM</code> to <code>trap 'exit 0' TERM</code> for cleaner shutdown</p> </li> <li> <p>Fixed command syntax to use <code>-c</code> flag properly with entrypoint override</p> </li> <li> <p>Removed <code>--deploy-hook</code> parameter from <code>certbot renew</code> command to use automatic hook discovery</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.1/#documentation-updates","title":"Documentation Updates","text":"<ul> <li> <p><code>docs/personas/quick-deploy/install-perfsonar-testpoint.md</code></p> </li> <li> <p>Added new troubleshooting section: \"Certbot service fails with 'Unable to open config file' error\"</p> </li> <li> <p>Includes diagnostic steps, root cause explanation, and solution</p> </li> <li> <p>Provides verification commands to confirm the fix</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.1/#impact","title":"Impact","text":""},{"location":"release-notes/quick-deploy-1.4.1/#before-fix","title":"Before Fix","text":"<ul> <li> <p>\u274c Certbot service would crash immediately after starting</p> </li> <li> <p>\u274c Certificate renewal automation would fail</p> </li> <li> <p>\u274c Service would not survive system reboots</p> </li> <li> <p>\u274c Logs showed: <code>certbot: error: Unable to open config file</code></p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.1/#after-fix","title":"After Fix","text":"<ul> <li> <p>\u2705 Certbot service starts and runs continuously</p> </li> <li> <p>\u2705 Certificate renewal loop operates correctly (checks every 12 hours)</p> </li> <li> <p>\u2705 Both testpoint and certbot containers survive system reboots</p> </li> <li> <p>\u2705 Proper systemd integration with <code>--systemd=always</code> flag</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.1/#upgrade-path","title":"Upgrade Path","text":""},{"location":"release-notes/quick-deploy-1.4.1/#for-new-installations","title":"For New Installations","text":"<p>No action needed - the fixed script is automatically used when running:</p> <pre><code>/opt/perfsonar-tp/tools_scripts/install-systemd-units.sh --with-certbot\n</code></pre>"},{"location":"release-notes/quick-deploy-1.4.1/#for-existing-deployments-with-failing-certbot-service","title":"For Existing Deployments with Failing Certbot Service","text":"<p>If you're experiencing the certbot service failure, update to the fixed version:</p> <pre><code># Stop the failing service\nsystemctl stop perfsonar-certbot.service\n\n# Download the updated script\ncurl -fsSL \\\n    https://raw.githubusercontent.com/osg-htc/networking/master/docs/perfsonar/tools_scripts/install-systemd-units.sh \\\n    -o /tmp/install-systemd-units.sh\nchmod 0755 /tmp/install-systemd-units.sh\n\n# Reinstall with the fix\n/tmp/install-systemd-units.sh --install-dir /opt/perfsonar-tp --with-certbot\n\n# Reload and start\nsystemctl daemon-reload\nsystemctl start perfsonar-certbot.service\n\n# Verify success\nsystemctl status perfsonar-certbot.service\npodman ps | grep certbot\n</code></pre>"},{"location":"release-notes/quick-deploy-1.4.1/#verification","title":"Verification","text":"<p>After applying the fix, verify both services are working correctly:</p> <pre><code># Check service status\nsystemctl status perfsonar-testpoint.service perfsonar-certbot.service\n\n# Verify containers are running\npodman ps --format 'table {{.Names}}\\t{{.Status}}'\n\n# Test HTTPS endpoint\ncurl -kI https://127.0.0.1/\n\n# Check certbot logs for renewal messages\njournalctl -u perfsonar-certbot.service -n 20\n</code></pre> <p>Expected Results:</p> <ul> <li> <p>Both services show <code>active (running)</code> status</p> </li> <li> <p>Both containers show \"Up\" status (not \"Exited\")</p> </li> <li> <p>HTTPS endpoint returns <code>HTTP/1.1 200 OK</code></p> </li> <li> <p>No error messages in certbot logs</p> </li> </ul>"},{"location":"release-notes/quick-deploy-1.4.1/#reboot-persistence-test","title":"Reboot Persistence Test","text":"<p>To confirm the fix resolves the reboot persistence issue:</p> <pre><code># Verify services are enabled\nsystemctl is-enabled perfsonar-testpoint.service perfsonar-certbot.service\n\n# Reboot the system\nreboot\n\n# After reboot, verify everything started automatically\nsystemctl status perfsonar-testpoint.service perfsonar-certbot.service\npodman ps\ncurl -kI https://127.0.0.1/\n</code></pre>"},{"location":"release-notes/quick-deploy-1.4.1/#breaking-changes","title":"Breaking Changes","text":"<p>None. This is a backward-compatible bug fix.</p>"},{"location":"release-notes/quick-deploy-1.4.1/#known-issues","title":"Known Issues","text":"<p>None at this time.</p>"},{"location":"release-notes/quick-deploy-1.4.1/#commits","title":"Commits","text":"<ul> <li>fix: Add --systemd=always and --entrypoint to certbot service for reboot persistence (956b053)</li> </ul>"},{"location":"release-notes/quick-deploy-1.4.1/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Monitor deployments to ensure certbot renewal automation works correctly</p> </li> <li> <p>Consider adding automated tests for post-reboot container state</p> </li> <li> <p>Evaluate whether additional health checks should be added to systemd units</p> </li> </ul> <p>For the full installation guide, see Installing a perfSONAR Testpoint for WLCG/OSG.</p> <p>For troubleshooting certificate issues, see the Certificate Issues section.</p>"},{"location":"tools/","title":"Link-check tools","text":"<p>This folder contains utilities to check and optionally clean up broken links in the <code>docs/</code> tree.</p> <p>find_and_remove_broken_links.py</p> <ul> <li> <p>Scans Markdown files under <code>docs/</code> for broken local links (relative links whose targets are missing).</p> </li> <li> <p>Writes a human-readable report to <code>docs/BROKEN_LINKS_REPORT.md</code>.</p> </li> <li> <p>By default it does not modify files. Use <code>--remove</code> to back up and patch files in-place.</p> </li> <li> <p>Example (dry run, just report):</p> </li> </ul> <pre><code>python docs/tools/find_and_remove_broken_links.py\n</code></pre> <ul> <li>Example (backup and remove broken links):</li> </ul> <pre><code>python docs/tools/find_and_remove_broken_links.py --remove --backup-dir docs/.link_check_backups\n</code></pre> <p>Notes:</p> <ul> <li> <p>External (http/https/mailto) links are not checked by default. To enable external HTTP checks add <code>--check-externals</code>. This may be slow and requires network access.</p> </li> <li> <p>Backups are created per-file with timestamped <code>*.bak.YYYYMMDDTHHMMSSZ</code> suffix under the backup directory.</p> </li> </ul> <p>After running, inspect <code>docs/BROKEN_LINKS_REPORT.md</code> for the list of broken links and suggested fixes.</p>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/","title":"Title: docs: fix broken links using automated mapping","text":""},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#body","title":"Body","text":""},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#summary","title":"Summary","text":"<p>Applied <code>docs/tools/link_mapping.json</code> to replace BROKEN-LINK markers with updated targets. Backups were created under <code>docs/.link_check_backups/</code>. This branch contains changes to documentation where broken links were replaced with mapped targets or annotated when the resource requires authentication.</p>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#files-changed","title":"Files changed","text":"<ul> <li> <p><code>docs/index.md</code></p> </li> <li> <p><code>docs/network-troubleshooting/osg-debugging-document.md</code></p> </li> <li> <p><code>docs/perfsonar/install-testpoint.md</code></p> </li> <li> <p><code>docs/perfsonar/psetf.md</code></p> </li> <li> <p><code>docs/perfsonar/tools_scripts/README.md</code></p> </li> <li> <p><code>docs/perfsonar/tools_scripts/perfSONAR-pbr-nm.sh</code></p> </li> </ul>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#details","title":"Details","text":"<ul> <li> <p>Replaced internal placeholders and gated links with public or stable alternatives where possible using the mapping file <code>docs/tools/link_mapping.json</code>.</p> </li> <li> <p>For external links that returned 401/403 the script replaced them with plain text indicating \"link requires authentication\" rather than removing them.</p> </li> <li> <p>Mailto entries were handled according to the mapping; the applier was run with <code>--remove-mailto</code> earlier to remove leftover mailto links, and mapping can re-add them if desired.</p> </li> </ul>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#backups","title":"Backups","text":"<p>Each modified file has a timestamped backup under <code>docs/.link_check_backups/</code> with suffix <code>.bak.YYYYMMDDTHHMMSSZ</code>.</p>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#testing","title":"Testing","text":"<ol> <li> <p>Preview the site locally (mkdocs) to ensure no rendering regressions.</p> </li> <li> <p>Run the docs link-check tool in dry-run to find remaining issues:</p> </li> </ol> <pre><code>python docs/tools/find_and_remove_broken_links.py --check-externals\n</code></pre>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#notes-next-steps","title":"Notes / Next steps","text":"<ul> <li> <p>Please review replaced URLs for accuracy; some mappings point to public equivalents (e.g., kernel docs) that may not be the preferred internal references.</p> </li> <li> <p>If any replacements are incorrect, they can be changed by editing <code>docs/tools/link_mapping.json</code> and re-running the applier.</p> </li> <li> <p>Consider adding a GitHub Actions workflow to run the link-checker on PRs to prevent regressions.</p> </li> </ul>"},{"location":"tools/PR_BROKEN_LINKS_DRAFT/#reviewer-suggestions","title":"Reviewer suggestions","text":"<ul> <li> <p>Confirm the chosen public replacements for gated resources (Red Hat docs, internal dashboards).</p> </li> <li> <p>Verify that the perfSONAR psetf and ETF references point to appropriate public pages or internal artifacts depending on access requirements.</p> </li> </ul>"}]}